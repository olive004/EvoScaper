{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run VAE models systematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from evoscaper.scripts.cvae_scan import main as cvae_scan\n",
    "from evoscaper.utils.preprocess import make_datetime_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table of all VAE model training settings\n",
    "\n",
    "Parameters for:\n",
    "- Biological dataset generation\n",
    "- Training data\n",
    "    - Input\n",
    "    - Output \n",
    "- Model architecture\n",
    "- Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "\n",
    "hpos_architecture = {\n",
    "    'seed_arch': 1,\n",
    "    'hidden_size': 32,\n",
    "    'enc_layers': [64, 64, 64],\n",
    "    'dec_layers': [64, 64, 64],\n",
    "    'model': 'CVAE',\n",
    "    'use_sigmoid_decoder': False,\n",
    "    'enc_init': 'HeNormal',\n",
    "    'dec_init': 'HeNormal',\n",
    "    'init_model_with_random': True,\n",
    "    'activation': 'leaky_relu',\n",
    "}\n",
    "\n",
    "\n",
    "hpos_training = {\n",
    "    'seed_train': 1,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 500,\n",
    "    'patience': 1000,\n",
    "    'learning_rate': 1e-1,\n",
    "    'loss_func': 'mse',\n",
    "    'use_dropout': False,\n",
    "    'dropout_rate': 0.1,\n",
    "    'use_l2_reg': False,\n",
    "    'l2_reg_alpha': 0.01,\n",
    "    'use_kl_div': True,\n",
    "    'kl_weight': 0.00025,  # inspired by https://github.com/elttaes/VAE-MNIST-Haiku-Jax/blob/main/cVAE_mnist.ipynb\n",
    "}\n",
    "hpos_training['print_every'] = hpos_training['epochs'] // 100\n",
    "\n",
    "hpos_optimization = {\n",
    "    'seed_opt': 1,\n",
    "    'opt_method': 'adam',\n",
    "    'opt_min_lr': 1e-6,\n",
    "    'opt_min_delta': 1e-4,\n",
    "    'learning_rate_sched': 'cosine_decay',\n",
    "    'use_warmup': True,\n",
    "    'warmup_epochs': 20,\n",
    "}\n",
    "\n",
    "hpos_dataset = {\n",
    "    'seed_dataset': 1,\n",
    "    'include_diffs': False,\n",
    "    'objective_col': 'Log sensitivity',\n",
    "    'output_species': ['RNA_2'],\n",
    "    'signal_species': ['RNA_0'],\n",
    "    'filenames_train_config': [f'{data_dir}/raw/summarise_simulation/2024_12_05_210221/ensemble_config.json'], \n",
    "    'filenames_train_table': [f'{data_dir}/raw/summarise_simulation/2024_12_05_210221/tabulated_mutation_info.csv'],\n",
    "    'filenames_verify_config': [f'{data_dir}/raw/summarise_simulation/2024_11_21_160955/ensemble_config.json'], \n",
    "    'filenames_verify_table': [f'{data_dir}/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv'],\n",
    "    'use_test_data': False,\n",
    "    # 'total_ds': None,   # TO BE RECORDED\n",
    "    'total_ds_max': 3e6,\n",
    "    'train_split': 0.8,\n",
    "    'x_type': 'energies',\n",
    "    # XY filtering:\n",
    "    'filt_x_nans': True,\n",
    "    'filt_y_nans': True,\n",
    "    'filt_sensitivity_nans': True,\n",
    "    'filt_precision_nans': True,\n",
    "    'filt_n_same_x_max': 100,\n",
    "    'filt_n_same_x_max_bins': 500,\n",
    "    # XY preprocessing:\n",
    "    'prep_x_standardise': False,\n",
    "    'prep_y_standardise': False,\n",
    "    'prep_x_min_max': False,\n",
    "    'prep_y_min_max': False,\n",
    "    'prep_x_robust_scaling': True,\n",
    "    'prep_y_robust_scaling': True,\n",
    "    'prep_x_logscale': False,\n",
    "    'prep_y_logscale': False,\n",
    "    'prep_x_categorical': False,\n",
    "    'prep_y_categorical': True,\n",
    "    'prep_x_categorical_onehot': False,\n",
    "    'prep_y_categorical_onehot': True,\n",
    "    'prep_x_categorical_n_bins': 10,\n",
    "    'prep_y_categorical_n_bins': 10,\n",
    "    'prep_x_categorical_method': 'quantile',\n",
    "    'prep_y_categorical_method': 'quantile',\n",
    "    'prep_x_negative': True,\n",
    "    'prep_y_negative': False\n",
    "}\n",
    "\n",
    "hpos_biological = {\n",
    "    'n_species': 3,\n",
    "    'sequence_length': 20,\n",
    "    'signal_function': 'step_function',\n",
    "    'signal_target': 2,\n",
    "    'starting_copynumbers_input': [200],\n",
    "    'starting_copynumbers_output': [200],\n",
    "    'starting_copynumbers_other': [200],\n",
    "    'association_binding_rate': 1000000,\n",
    "    'include_prod_deg': False,\n",
    "}\n",
    "\n",
    "hpos_eval = {\n",
    "    'eval_n_to_sample': 1e5\n",
    "}\n",
    "\n",
    "info_to_be_recorded = {\n",
    "    'filename_saved_model': 'TO_BE_RECORDED',\n",
    "    'total_ds': 'TO_BE_RECORDED',\n",
    "    'n_batches': 'TO_BE_RECORDED',\n",
    "    'R2_train': 'TO_BE_RECORDED',\n",
    "    'R2_test': 'TO_BE_RECORDED',\n",
    "    'mutual_information_conditionality': 'TO_BE_RECORDED',\n",
    "    'n_layers_enc': 'TO_BE_RECORDED',\n",
    "    'n_layers_dec': 'TO_BE_RECORDED',\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_arch</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>enc_layers</th>\n",
       "      <th>dec_layers</th>\n",
       "      <th>model</th>\n",
       "      <th>use_sigmoid_decoder</th>\n",
       "      <th>enc_init</th>\n",
       "      <th>dec_init</th>\n",
       "      <th>init_model_with_random</th>\n",
       "      <th>activation</th>\n",
       "      <th>...</th>\n",
       "      <th>prep_y_categorical</th>\n",
       "      <th>prep_x_categorical_onehot</th>\n",
       "      <th>prep_y_categorical_onehot</th>\n",
       "      <th>prep_x_categorical_n_bins</th>\n",
       "      <th>prep_y_categorical_n_bins</th>\n",
       "      <th>prep_x_categorical_method</th>\n",
       "      <th>prep_y_categorical_method</th>\n",
       "      <th>prep_x_negative</th>\n",
       "      <th>prep_y_negative</th>\n",
       "      <th>eval_n_to_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  seed_arch hidden_size    enc_layers    dec_layers model use_sigmoid_decoder  \\\n",
       "0         1          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "\n",
       "   enc_init  dec_init init_model_with_random  activation  ...  \\\n",
       "0  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "\n",
       "  prep_y_categorical prep_x_categorical_onehot prep_y_categorical_onehot  \\\n",
       "0               True                     False                      True   \n",
       "\n",
       "  prep_x_categorical_n_bins prep_y_categorical_n_bins  \\\n",
       "0                        10                        10   \n",
       "\n",
       "  prep_x_categorical_method prep_y_categorical_method prep_x_negative  \\\n",
       "0                  quantile                  quantile            True   \n",
       "\n",
       "  prep_y_negative eval_n_to_sample  \n",
       "0           False         100000.0  \n",
       "\n",
       "[1 rows x 68 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hpos = pd.concat([pd.DataFrame.from_dict(hpos, orient='index').T for hpos in [hpos_architecture, hpos_training, hpos_optimization, hpos_dataset, hpos_eval]], axis=1)\n",
    "assert df_hpos.columns.duplicated().sum() == 0, 'Change some column names, there are duplicates'\n",
    "basic_setting = df_hpos.copy()\n",
    "df_hpos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpos_to_vary_from_og = {\n",
    "    # 'total_ds_max': [1e4, 5e4, 1e5, 5e5, 1e6, 5e6],\n",
    "    'seed_arch': [1, 2, 3, 4, 5],\n",
    "}\n",
    "hpos_to_vary_together = {\n",
    "    'hidden_size': [16, 32, 64, 128, 256, 512],\n",
    "    'objective_col': ['adaptability', 'sensitivity_wrt_species-6'],\n",
    "    'x_type': ['energies', 'binding_rates_dissociation'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "}\n",
    "\n",
    "df_hpos.loc[df_hpos['objective_col'] == 'sensitivity_wrt_species-6', 'prep_y_logscale'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_arch</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>enc_layers</th>\n",
       "      <th>dec_layers</th>\n",
       "      <th>model</th>\n",
       "      <th>use_sigmoid_decoder</th>\n",
       "      <th>enc_init</th>\n",
       "      <th>dec_init</th>\n",
       "      <th>init_model_with_random</th>\n",
       "      <th>activation</th>\n",
       "      <th>...</th>\n",
       "      <th>prep_y_categorical</th>\n",
       "      <th>prep_x_categorical_onehot</th>\n",
       "      <th>prep_y_categorical_onehot</th>\n",
       "      <th>prep_x_categorical_n_bins</th>\n",
       "      <th>prep_y_categorical_n_bins</th>\n",
       "      <th>prep_x_categorical_method</th>\n",
       "      <th>prep_y_categorical_method</th>\n",
       "      <th>prep_x_negative</th>\n",
       "      <th>prep_y_negative</th>\n",
       "      <th>eval_n_to_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>HeNormal</td>\n",
       "      <td>True</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>quantile</td>\n",
       "      <td>quantile</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  seed_arch hidden_size    enc_layers    dec_layers model use_sigmoid_decoder  \\\n",
       "0         1          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "1         1          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "2         2          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "3         3          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "4         4          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "5         5          32  [64, 64, 64]  [64, 64, 64]  CVAE               False   \n",
       "\n",
       "   enc_init  dec_init init_model_with_random  activation  ...  \\\n",
       "0  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "1  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "2  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "3  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "4  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "5  HeNormal  HeNormal                   True  leaky_relu  ...   \n",
       "\n",
       "  prep_y_categorical prep_x_categorical_onehot prep_y_categorical_onehot  \\\n",
       "0               True                     False                      True   \n",
       "1               True                     False                      True   \n",
       "2               True                     False                      True   \n",
       "3               True                     False                      True   \n",
       "4               True                     False                      True   \n",
       "5               True                     False                      True   \n",
       "\n",
       "  prep_x_categorical_n_bins prep_y_categorical_n_bins  \\\n",
       "0                        10                        10   \n",
       "1                        10                        10   \n",
       "2                        10                        10   \n",
       "3                        10                        10   \n",
       "4                        10                        10   \n",
       "5                        10                        10   \n",
       "\n",
       "  prep_x_categorical_method prep_y_categorical_method prep_x_negative  \\\n",
       "0                  quantile                  quantile            True   \n",
       "1                  quantile                  quantile            True   \n",
       "2                  quantile                  quantile            True   \n",
       "3                  quantile                  quantile            True   \n",
       "4                  quantile                  quantile            True   \n",
       "5                  quantile                  quantile            True   \n",
       "\n",
       "  prep_y_negative eval_n_to_sample  \n",
       "0           False         100000.0  \n",
       "1           False         100000.0  \n",
       "2           False         100000.0  \n",
       "3           False         100000.0  \n",
       "4           False         100000.0  \n",
       "5           False         100000.0  \n",
       "\n",
       "[6 rows x 68 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for h, v in hpos_to_vary_from_og.items():\n",
    "    df_hpos = pd.concat([df_hpos] + [basic_setting.assign(**{h: vv}) for vv in v], ignore_index=True)\n",
    "df_hpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good if these are equal:  78 78\n"
     ]
    }
   ],
   "source": [
    "keys_vary_together = sorted(hpos_to_vary_together.keys())\n",
    "for v in itertools.product(*[hpos_to_vary_together[h] for h in keys_vary_together]):\n",
    "    curr = basic_setting.assign(**{h: vv for h, vv in zip(keys_vary_together, v)})\n",
    "    df_hpos = pd.concat([df_hpos, curr], ignore_index=True)\n",
    "print('All good if these are equal: ', len(df_hpos), len(list(itertools.product(*[hpos_to_vary_together[h] for h in keys_vary_together]))) + np.sum([len(v) for v in hpos_to_vary_from_og.values()]) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use table to create dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpos = df_hpos.reset_index().iloc[0].to_dict()\n",
    "\n",
    "fn = '../data/raw/summarise_simulation/2024_11_21_144918/tabulated_mutation_info.csv'\n",
    "# fn = '../data/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv'\n",
    "# fn = '../data/raw/summarise_simulation/2024_12_05_210221/tabulated_mutation_info.csv'\n",
    "data = pd.read_csv(fn).iloc[:100]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpos = df_hpos.reset_index().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.py:train():109: Epoch 0 / 500 -\t\t Train loss: 0.6882670521736145\tVal loss: 0.915489912033081\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 5 / 500 -\t\t Train loss: 0.33758753538131714\tVal loss: 0.7543137073516846\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 10 / 500 -\t\t Train loss: 0.31823816895484924\tVal loss: 0.672414243221283\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 15 / 500 -\t\t Train loss: 0.3645678758621216\tVal loss: 0.5750048756599426\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 20 / 500 -\t\t Train loss: 0.32148808240890503\tVal loss: 0.6948093771934509\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 25 / 500 -\t\t Train loss: 0.3325931429862976\tVal loss: 0.5213280916213989\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 30 / 500 -\t\t Train loss: 0.3010670840740204\tVal loss: 0.4804573953151703\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 35 / 500 -\t\t Train loss: 0.2838444411754608\tVal loss: 0.46031898260116577\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 40 / 500 -\t\t Train loss: 0.3024349808692932\tVal loss: 0.44258588552474976\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 45 / 500 -\t\t Train loss: 0.2716399133205414\tVal loss: 0.4426683187484741\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 50 / 500 -\t\t Train loss: 0.254062294960022\tVal loss: 0.43250489234924316\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 55 / 500 -\t\t Train loss: 0.2502577006816864\tVal loss: 0.3979595899581909\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 60 / 500 -\t\t Train loss: 0.2507280707359314\tVal loss: 0.40787625312805176\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 65 / 500 -\t\t Train loss: 0.2351050227880478\tVal loss: 0.39560791850090027\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 70 / 500 -\t\t Train loss: 0.2268238365650177\tVal loss: 0.3933612108230591\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 75 / 500 -\t\t Train loss: 0.22328411042690277\tVal loss: 0.37903863191604614\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 80 / 500 -\t\t Train loss: 0.213154137134552\tVal loss: 0.39241430163383484\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 85 / 500 -\t\t Train loss: 0.1906023770570755\tVal loss: 0.39757734537124634\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 90 / 500 -\t\t Train loss: 0.1925838589668274\tVal loss: 0.3823198676109314\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 95 / 500 -\t\t Train loss: 0.19241571426391602\tVal loss: 0.4135337173938751\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 100 / 500 -\t\t Train loss: 0.1896679848432541\tVal loss: 0.3590150773525238\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 105 / 500 -\t\t Train loss: 0.16193179786205292\tVal loss: 0.3188897371292114\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 110 / 500 -\t\t Train loss: 0.12837453186511993\tVal loss: 0.3563686013221741\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 115 / 500 -\t\t Train loss: 0.11902250349521637\tVal loss: 0.3117474615573883\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 120 / 500 -\t\t Train loss: 0.10345830023288727\tVal loss: 0.34050044417381287\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 125 / 500 -\t\t Train loss: 0.0942905992269516\tVal loss: 0.3517724871635437\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 130 / 500 -\t\t Train loss: 0.08527274429798126\tVal loss: 0.3622399866580963\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 135 / 500 -\t\t Train loss: 0.0844217836856842\tVal loss: 0.3584041893482208\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 140 / 500 -\t\t Train loss: 0.08119495213031769\tVal loss: 0.3698327839374542\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 145 / 500 -\t\t Train loss: 0.08061419427394867\tVal loss: 0.37700897455215454\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 150 / 500 -\t\t Train loss: 0.08033022284507751\tVal loss: 0.383238822221756\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 155 / 500 -\t\t Train loss: 0.08247960358858109\tVal loss: 0.3801976442337036\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 160 / 500 -\t\t Train loss: 0.08070944249629974\tVal loss: 0.3945903778076172\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 165 / 500 -\t\t Train loss: 0.07981537282466888\tVal loss: 0.4010780453681946\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 170 / 500 -\t\t Train loss: 0.07967744022607803\tVal loss: 0.40666908025741577\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 175 / 500 -\t\t Train loss: 0.07965666800737381\tVal loss: 0.40866267681121826\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 180 / 500 -\t\t Train loss: 0.07984907925128937\tVal loss: 0.40951359272003174\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 185 / 500 -\t\t Train loss: 0.07946161925792694\tVal loss: 0.4148653447628021\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 190 / 500 -\t\t Train loss: 0.07938434928655624\tVal loss: 0.42082929611206055\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 195 / 500 -\t\t Train loss: 0.079347163438797\tVal loss: 0.42552417516708374\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 200 / 500 -\t\t Train loss: 0.07930266112089157\tVal loss: 0.4297432005405426\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 205 / 500 -\t\t Train loss: 0.07927798479795456\tVal loss: 0.4336324632167816\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 210 / 500 -\t\t Train loss: 0.08073031902313232\tVal loss: 0.42636409401893616\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 215 / 500 -\t\t Train loss: 0.0806141346693039\tVal loss: 0.41928616166114807\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 220 / 500 -\t\t Train loss: 0.07926927506923676\tVal loss: 0.42988523840904236\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 225 / 500 -\t\t Train loss: 0.07918666303157806\tVal loss: 0.4353833496570587\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 230 / 500 -\t\t Train loss: 0.07914799451828003\tVal loss: 0.43980222940444946\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 235 / 500 -\t\t Train loss: 0.0791221335530281\tVal loss: 0.4446336627006531\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 240 / 500 -\t\t Train loss: 0.07911817729473114\tVal loss: 0.44662344455718994\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 245 / 500 -\t\t Train loss: 0.07907673716545105\tVal loss: 0.4511452317237854\tVal accuracy: 0.0052083334885537624 INFO\n",
      "train.py:train():109: Epoch 250 / 500 -\t\t Train loss: 0.07905241847038269\tVal loss: 0.45476073026657104\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 255 / 500 -\t\t Train loss: 0.07903537154197693\tVal loss: 0.4581768810749054\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 260 / 500 -\t\t Train loss: 0.0790160521864891\tVal loss: 0.46127650141716003\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 265 / 500 -\t\t Train loss: 0.07899963855743408\tVal loss: 0.46440231800079346\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 270 / 500 -\t\t Train loss: 0.07898427546024323\tVal loss: 0.46757516264915466\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 275 / 500 -\t\t Train loss: 0.08009421080350876\tVal loss: 0.45742619037628174\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 280 / 500 -\t\t Train loss: 0.07903698831796646\tVal loss: 0.4688608646392822\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 285 / 500 -\t\t Train loss: 0.07896339893341064\tVal loss: 0.47208666801452637\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 290 / 500 -\t\t Train loss: 0.07894568145275116\tVal loss: 0.47490817308425903\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 295 / 500 -\t\t Train loss: 0.07893218100070953\tVal loss: 0.4779036343097687\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 300 / 500 -\t\t Train loss: 0.0789204016327858\tVal loss: 0.48060768842697144\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 305 / 500 -\t\t Train loss: 0.07890787720680237\tVal loss: 0.4837121367454529\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 310 / 500 -\t\t Train loss: 0.07890553772449493\tVal loss: 0.4858337640762329\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 315 / 500 -\t\t Train loss: 0.07888966798782349\tVal loss: 0.48888927698135376\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 320 / 500 -\t\t Train loss: 0.07888014614582062\tVal loss: 0.49160951375961304\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 325 / 500 -\t\t Train loss: 0.07887246459722519\tVal loss: 0.4940182566642761\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 330 / 500 -\t\t Train loss: 0.07886326313018799\tVal loss: 0.4968549311161041\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 335 / 500 -\t\t Train loss: 0.07885555177927017\tVal loss: 0.49945491552352905\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 340 / 500 -\t\t Train loss: 0.07884810119867325\tVal loss: 0.5020203590393066\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 345 / 500 -\t\t Train loss: 0.07884056121110916\tVal loss: 0.5047285556793213\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 350 / 500 -\t\t Train loss: 0.0788336843252182\tVal loss: 0.5072302222251892\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 355 / 500 -\t\t Train loss: 0.0788273811340332\tVal loss: 0.5096403360366821\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 360 / 500 -\t\t Train loss: 0.07982806116342545\tVal loss: 0.4960026443004608\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 365 / 500 -\t\t Train loss: 0.07852134108543396\tVal loss: 0.5031620860099792\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 370 / 500 -\t\t Train loss: 0.0783919021487236\tVal loss: 0.5075553059577942\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 375 / 500 -\t\t Train loss: 0.0783519446849823\tVal loss: 0.5113407373428345\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 380 / 500 -\t\t Train loss: 0.07835352420806885\tVal loss: 0.5140805840492249\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 385 / 500 -\t\t Train loss: 0.07831719517707825\tVal loss: 0.5168581604957581\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 390 / 500 -\t\t Train loss: 0.07830742001533508\tVal loss: 0.5194091796875\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 395 / 500 -\t\t Train loss: 0.07830019295215607\tVal loss: 0.5218057036399841\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 400 / 500 -\t\t Train loss: 0.07829250395298004\tVal loss: 0.5240319967269897\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 405 / 500 -\t\t Train loss: 0.0782875269651413\tVal loss: 0.5260926485061646\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 410 / 500 -\t\t Train loss: 0.07828257977962494\tVal loss: 0.5280510783195496\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 415 / 500 -\t\t Train loss: 0.07827835530042648\tVal loss: 0.5298243761062622\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 420 / 500 -\t\t Train loss: 0.07827433943748474\tVal loss: 0.5316414833068848\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 425 / 500 -\t\t Train loss: 0.07827063649892807\tVal loss: 0.5332967638969421\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 430 / 500 -\t\t Train loss: 0.07826748490333557\tVal loss: 0.5349286198616028\tVal accuracy: 0.00390625 INFO\n",
      "train.py:train():109: Epoch 435 / 500 -\t\t Train loss: 0.07825936377048492\tVal loss: 0.5364840030670166\tVal accuracy: 0.0026041667442768812 INFO\n",
      "train.py:train():109: Epoch 440 / 500 -\t\t Train loss: 0.07824712991714478\tVal loss: 0.5381325483322144\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 445 / 500 -\t\t Train loss: 0.07824170589447021\tVal loss: 0.5396440029144287\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 450 / 500 -\t\t Train loss: 0.07823808491230011\tVal loss: 0.5409941077232361\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 455 / 500 -\t\t Train loss: 0.07823485136032104\tVal loss: 0.542414665222168\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 460 / 500 -\t\t Train loss: 0.07823246717453003\tVal loss: 0.543616533279419\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 465 / 500 -\t\t Train loss: 0.0782647579908371\tVal loss: 0.544275164604187\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 470 / 500 -\t\t Train loss: 0.0782303437590599\tVal loss: 0.545172929763794\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 475 / 500 -\t\t Train loss: 0.07822683453559875\tVal loss: 0.5454656481742859\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 480 / 500 -\t\t Train loss: 0.07822372764348984\tVal loss: 0.5467262864112854\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 485 / 500 -\t\t Train loss: 0.07822155952453613\tVal loss: 0.5478533506393433\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 490 / 500 -\t\t Train loss: 0.0782197117805481\tVal loss: 0.5489723682403564\tVal accuracy: 0.0013020833721384406 INFO\n",
      "train.py:train():109: Epoch 495 / 500 -\t\t Train loss: 0.07821840792894363\tVal loss: 0.5498123168945312\tVal accuracy: 0.0026041667442768812 INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete: 0:11:36.656185\n",
      "Warning: not using the test data for evaluation, but the training data instead of ['../data/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv']\n",
      "The R2 score is  -0.35415375232696533\n",
      "The R2 score with weighted variance is  -0.35415375232696533\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10000 and the array at index 1 has size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevoscaper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcvae_scan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m cvae_scan\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# REMINDER: Total ds is 3 and the test data is not being used\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcvae_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhpos\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workdir/src/evoscaper/scripts/cvae_scan.py:239\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hpos, top_dir)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning: not using the test data for evaluation, but the training data instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dataset\u001b[38;5;241m.\u001b[39mfilenames_verify_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    236\u001b[0m     data_test \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    238\u001b[0m r2_test, mi \u001b[38;5;241m=\u001b[39m test(model, params, rng, decoder, saves, data_test,\n\u001b[0;32m--> 239\u001b[0m                    config_dataset, config_norm_y, config_model,\n\u001b[1;32m    240\u001b[0m                    x_cols, config_filter, top_write_dir,\n\u001b[1;32m    241\u001b[0m                    x_datanormaliser, x_methods_preprocessing,\n\u001b[1;32m    242\u001b[0m                    y_datanormaliser, y_methods_preprocessing)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Save stats\u001b[39;00m\n\u001b[1;32m    245\u001b[0m hpos \u001b[38;5;241m=\u001b[39m save_stats(hpos, save_path, total_ds, n_batches, r2_train, r2_test, mi, \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    246\u001b[0m     config_model\u001b[38;5;241m.\u001b[39menc_layers), \u001b[38;5;28mlen\u001b[39m(config_model\u001b[38;5;241m.\u001b[39mdec_layers))\n",
      "File \u001b[0;32m/workdir/src/evoscaper/scripts/cvae_scan.py:177\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, params, rng, decoder, saves, data_test, config_dataset, config_norm_y, config_model, x_cols, config_filter, top_write_dir, x_datanormaliser, x_methods_preprocessing, y_datanormaliser, y_methods_preprocessing)\u001b[0m\n\u001b[1;32m    172\u001b[0m r2_test \u001b[38;5;241m=\u001b[39m r2_score(x\u001b[38;5;241m.\u001b[39mflatten(), pred_y\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m    174\u001b[0m vis(saves, x, pred_y, top_write_dir)\n\u001b[1;32m    176\u001b[0m mi \u001b[38;5;241m=\u001b[39m test_conditionality(params, rng, decoder, df, x_cols,\n\u001b[0;32m--> 177\u001b[0m                          config_dataset, config_norm_y, config_model, top_write_dir,\n\u001b[1;32m    178\u001b[0m                          x_datanormaliser, x_methods_preprocessing,\n\u001b[1;32m    179\u001b[0m                          y_datanormaliser, y_methods_preprocessing, cond)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r2_test, mi\n",
      "File \u001b[0;32m/workdir/src/evoscaper/scripts/cvae_scan.py:141\u001b[0m, in \u001b[0;36mtest_conditionality\u001b[0;34m(params, rng, decoder, df, x_cols, config_dataset, config_norm_y, config_model, top_write_dir, x_datanormaliser, x_methods_preprocessing, y_datanormaliser, y_methods_preprocessing, cond)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_conditionality\u001b[39m(params, rng, decoder, df, x_cols,\n\u001b[1;32m    136\u001b[0m                         config_dataset: DatasetConfig, config_norm_y, config_model, top_write_dir,\n\u001b[1;32m    137\u001b[0m                         x_datanormaliser, x_methods_preprocessing,\n\u001b[1;32m    138\u001b[0m                         y_datanormaliser, y_methods_preprocessing, cond):\n\u001b[1;32m    139\u001b[0m     n_categories \u001b[38;5;241m=\u001b[39m config_norm_y\u001b[38;5;241m.\u001b[39mcategorical_n_bins\n\u001b[1;32m    140\u001b[0m     fake_circuits, z, sampled_cond \u001b[38;5;241m=\u001b[39m sample_reconstructions(params, rng, decoder,\n\u001b[0;32m--> 141\u001b[0m                                                             n_categories\u001b[38;5;241m=\u001b[39mn_categories, n_to_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39mconfig_model\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    142\u001b[0m                                                             x_datanormaliser\u001b[38;5;241m=\u001b[39mx_datanormaliser, x_methods_preprocessing\u001b[38;5;241m=\u001b[39mx_methods_preprocessing,\n\u001b[1;32m    143\u001b[0m                                                             use_binned_sampling\u001b[38;5;241m=\u001b[39mconfig_norm_y\u001b[38;5;241m.\u001b[39mcategorical, use_onehot\u001b[38;5;241m=\u001b[39mconfig_norm_y\u001b[38;5;241m.\u001b[39mcategorical_onehot,\n\u001b[1;32m    144\u001b[0m                                                             cond_min\u001b[38;5;241m=\u001b[39mcond\u001b[38;5;241m.\u001b[39mmin(), cond_max\u001b[38;5;241m=\u001b[39mcond\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    146\u001b[0m     mi \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(partial(estimate_mutual_information_knn, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))(z, sampled_cond)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# mi = estimate_mutual_information_knn(z.reshape(np.prod(\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m#     z.shape[:-1]), z.shape[-1]), sampled_cond.reshape(np.prod(sampled_cond.shape[:-1]), sampled_cond.shape[-1]), k=5)\u001b[39;00m\n",
      "File \u001b[0;32m/workdir/src/evoscaper/model/sampling.py:32\u001b[0m, in \u001b[0;36msample_reconstructions\u001b[0;34m(params, rng, decoder, n_categories, n_to_sample, hidden_size, x_datanormaliser, x_methods_preprocessing, use_binned_sampling, use_onehot, cond_min, cond_max)\u001b[0m\n\u001b[1;32m     30\u001b[0m z \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(rng, (n_to_sample, hidden_size))\n\u001b[1;32m     31\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(z[\u001b[38;5;28;01mNone\u001b[39;00m, :], repeats\u001b[38;5;241m=\u001b[39mn_categories, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_cond\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m x_fake \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(partial(decoder, params\u001b[38;5;241m=\u001b[39mparams, rng\u001b[38;5;241m=\u001b[39mrng))(inputs\u001b[38;5;241m=\u001b[39mz)\n\u001b[1;32m     35\u001b[0m x_fake \u001b[38;5;241m=\u001b[39m x_datanormaliser\u001b[38;5;241m.\u001b[39mcreate_chain_preprocessor_inverse(\n\u001b[1;32m     36\u001b[0m     x_methods_preprocessing)(x_fake)\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10000 and the array at index 1 has size 1000"
     ]
    }
   ],
   "source": [
    "# REMINDER: Total ds is 3 and the test data is not being used\n",
    "cvae_scan(hpos, top_dir=os.path.join('data', make_datetime_str()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = partial(VAE_fn, enc_layers=enc_layers, dec_layers=dec_layers, decoder_head=x.shape[-1], \n",
    "                   HIDDEN_SIZE=HIDDEN_SIZE, decoder_activation_final=jax.nn.sigmoid if USE_SIGMOID_DECODER else jax.nn.leaky_relu, \n",
    "                   enc_init=ENC_INIT, dec_init=DEC_INIT, activation=get_activation_fn(ACTIVATION))\n",
    "model_t = hk.multi_transform(model_fn)\n",
    "dummy_x = jax.random.normal(PRNG, x.shape)\n",
    "dummy_cond = jax.random.normal(PRNG, cond.shape)\n",
    "params = model_t.init(PRNG, dummy_x, dummy_cond, deterministic=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
