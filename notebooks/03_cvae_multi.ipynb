{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run VAE models systematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "from evoscaper.scripts.cvae_scan import main as cvae_scan\n",
    "from evoscaper.utils.preprocess import make_datetime_str\n",
    "from synbio_morpher.utils.data.data_format_tools.common import write_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table of all VAE model training settings\n",
    "\n",
    "Parameters for:\n",
    "- Biological dataset generation\n",
    "- Training data\n",
    "    - Input\n",
    "    - Output \n",
    "- Model architecture\n",
    "- Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "\n",
    "hpos_architecture = {\n",
    "    'seed_arch': 1,\n",
    "    'hidden_size': 16,\n",
    "    'enc_ls': 32,\n",
    "    'dec_ls': 32,\n",
    "    'num_enc_layers': 2,\n",
    "    'num_dec_layers': 2,\n",
    "    'factor_expanding_ls': 1,\n",
    "    'factor_contracting_ls': 1,\n",
    "    'model': 'CVAE',\n",
    "    'use_sigmoid_decoder': False,\n",
    "    'enc_init': 'HeNormal',\n",
    "    'dec_init': 'HeNormal',\n",
    "    'init_model_with_random': True,\n",
    "    'activation': 'leaky_relu',\n",
    "}\n",
    "\n",
    "hpos_training = {\n",
    "    'seed_train': 1,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 3000,\n",
    "    'patience': 500,\n",
    "    'learning_rate': 1e-2,\n",
    "    'loss_func': 'mse',\n",
    "    'accuracy_func': 'accuracy_regression',\n",
    "    'use_dropout': False,\n",
    "    'dropout_rate': 0.1,\n",
    "    'use_l2_reg': False,\n",
    "    'l2_reg_alpha': 5e-2,\n",
    "    'use_kl_div': True,\n",
    "    # inspired by https://github.com/elttaes/VAE-MNIST-Haiku-Jax/blob/main/cVAE_mnist.ipynb\n",
    "    'kl_weight': 2.5e-4,\n",
    "    'use_grad_clipping': False\n",
    "}\n",
    "hpos_training['print_every'] = hpos_training['epochs'] // 50\n",
    "\n",
    "hpos_optimization = {\n",
    "    'seed_opt': 1,\n",
    "    'opt_method': 'adam',\n",
    "    'opt_min_lr': 1e-6,\n",
    "    'opt_min_delta': 1e-4,\n",
    "    'learning_rate_sched': 'cosine_decay',\n",
    "    'use_warmup': True,\n",
    "    'warmup_epochs': 20,\n",
    "}\n",
    "\n",
    "hpos_dataset = {\n",
    "    'seed_dataset': 1,\n",
    "    'include_diffs': False,\n",
    "    'objective_col': ('Log sensitivity',),\n",
    "    'output_species': ('RNA_2',),\n",
    "    'signal_species': ('RNA_0',),\n",
    "    'filenames_train_config': f'{data_dir}/raw/summarise_simulation/2024_12_05_210221/ensemble_config.json',\n",
    "    'filenames_train_table': f'{data_dir}/raw/summarise_simulation/2024_12_05_210221/tabulated_mutation_info.csv',\n",
    "    'filenames_verify_config': f'{data_dir}/raw/summarise_simulation/2024_11_21_160955/ensemble_config.json',\n",
    "    'filenames_verify_table': f'{data_dir}/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv',\n",
    "    'use_test_data': False,\n",
    "    # 'total_ds': None,   # TO BE RECORDED\n",
    "    'total_ds_max': 5e6,\n",
    "    'train_split': 0.8,\n",
    "    'x_type': 'energies',\n",
    "    # XY filtering:\n",
    "    'filt_x_nans': True,\n",
    "    'filt_y_nans': True,\n",
    "    'filt_sensitivity_nans': True,\n",
    "    'filt_precision_nans': True,\n",
    "    'filt_n_same_x_max': 1,\n",
    "    'filt_n_same_x_max_bins': 50,\n",
    "    # XY preprocessing:\n",
    "    'prep_x_standardise': False,\n",
    "    'prep_y_standardise': False,\n",
    "    'prep_x_min_max': True,\n",
    "    'prep_y_min_max': True,\n",
    "    'prep_x_robust_scaling': True,\n",
    "    'prep_y_robust_scaling': True,\n",
    "    'prep_x_logscale': False,\n",
    "    'prep_y_logscale': False,\n",
    "    'prep_x_categorical': False,\n",
    "    'prep_y_categorical': True,\n",
    "    'prep_x_categorical_onehot': False,\n",
    "    'prep_y_categorical_onehot': True,\n",
    "    'prep_x_categorical_n_bins': 10,\n",
    "    'prep_y_categorical_n_bins': 10,\n",
    "    'prep_x_categorical_method': 'quantile',\n",
    "    'prep_y_categorical_method': 'quantile',\n",
    "    'prep_x_negative': True,\n",
    "    'prep_y_negative': False\n",
    "}\n",
    "\n",
    "hpos_biological = {\n",
    "    'n_species': 3,\n",
    "    'sequence_length': 20,\n",
    "    'signal_function': 'step_function',\n",
    "    'signal_target': 2,\n",
    "    'starting_copynumbers_input': 200,\n",
    "    'starting_copynumbers_output': 200,\n",
    "    'starting_copynumbers_other': 200,\n",
    "    'association_binding_rate': 1000000,\n",
    "    'include_prod_deg': False,\n",
    "}\n",
    "\n",
    "hpos_eval = {\n",
    "    'eval_n_to_sample': 1e3\n",
    "}\n",
    "\n",
    "info_to_be_recorded = {\n",
    "    'filename_saved_model': 'TO_BE_RECORDED',\n",
    "    'total_ds': 'TO_BE_RECORDED',\n",
    "    'n_batches': 'TO_BE_RECORDED',\n",
    "    'R2_train': 'TO_BE_RECORDED',\n",
    "    'R2_test': 'TO_BE_RECORDED',\n",
    "    'mutual_information_conditionality': 'TO_BE_RECORDED',\n",
    "    'n_layers_enc': 'TO_BE_RECORDED',\n",
    "    'n_layers_dec': 'TO_BE_RECORDED',\n",
    "    'run_successful': 'TO_BE_RECORDED',\n",
    "    'info_early_stop': 'TO_BE_RECORDED',\n",
    "    'error_msg': 'TO_BE_RECORDED',\n",
    "}\n",
    "\n",
    "hpos_all = {}\n",
    "for d in [hpos_architecture, hpos_training, hpos_optimization, hpos_dataset, hpos_eval, info_to_be_recorded]:\n",
    "    hpos_all.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_arch</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>enc_ls</th>\n",
       "      <th>dec_ls</th>\n",
       "      <th>num_enc_layers</th>\n",
       "      <th>num_dec_layers</th>\n",
       "      <th>factor_expanding_ls</th>\n",
       "      <th>factor_contracting_ls</th>\n",
       "      <th>model</th>\n",
       "      <th>use_sigmoid_decoder</th>\n",
       "      <th>...</th>\n",
       "      <th>total_ds</th>\n",
       "      <th>n_batches</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>mutual_information_conditionality</th>\n",
       "      <th>n_layers_enc</th>\n",
       "      <th>n_layers_dec</th>\n",
       "      <th>run_successful</th>\n",
       "      <th>info_early_stop</th>\n",
       "      <th>error_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "      <td>TO_BE_RECORDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  seed_arch hidden_size enc_ls dec_ls num_enc_layers num_dec_layers  \\\n",
       "0         1          16     32     32              2              2   \n",
       "\n",
       "  factor_expanding_ls factor_contracting_ls model use_sigmoid_decoder  ...  \\\n",
       "0                   1                     1  CVAE               False  ...   \n",
       "\n",
       "         total_ds       n_batches        R2_train         R2_test  \\\n",
       "0  TO_BE_RECORDED  TO_BE_RECORDED  TO_BE_RECORDED  TO_BE_RECORDED   \n",
       "\n",
       "  mutual_information_conditionality    n_layers_enc    n_layers_dec  \\\n",
       "0                    TO_BE_RECORDED  TO_BE_RECORDED  TO_BE_RECORDED   \n",
       "\n",
       "   run_successful info_early_stop       error_msg  \n",
       "0  TO_BE_RECORDED  TO_BE_RECORDED  TO_BE_RECORDED  \n",
       "\n",
       "[1 rows x 85 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hpos = pd.DataFrame.from_dict(hpos_all, orient='index').T\n",
    "assert df_hpos.columns.duplicated().sum() == 0, 'Change some column names, there are duplicates'\n",
    "basic_setting = df_hpos.copy()\n",
    "df_hpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ../data/raw/summarise_simulation/2024_12_05_21...\n",
       "Name: filenames_train_config, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k, v in hpos_all.items():\n",
    "#     if type(v) == tuple:\n",
    "#         print(k, v)\n",
    "#         df_hpos[k] = df_hpos[k].apply(lambda x: tuple(x))\n",
    "df_hpos['filenames_train_config']        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpos_to_vary_from_og = {\n",
    "    'seed_arch': [1, 2, 3, 4, 5],\n",
    "    'num_enc_layers': [4, 5],\n",
    "}\n",
    "hpos_to_vary_from_og2 = {\n",
    "    'objective_col': [('adaptation',), ('Log sensitivity',), ('Log sensitivity', 'Log precision')],\n",
    "}\n",
    "hpos_to_vary_from_og3 = {\n",
    "    'hidden_size': list(range(1, 33)),\n",
    "}\n",
    "hpos_to_vary_together = {\n",
    "    'hidden_size': [1, 2, 4, 8, 16, 32, 64, 128],\n",
    "    'enc_ls': [4, 8, 16, 32, 64],\n",
    "    'num_enc_layers': [1, 2, 3],\n",
    "    'factor_expanding_ls': [1, 2, 3],\n",
    "}\n",
    "hpos_to_vary_together2 = {\n",
    "    'hidden_size': [16, 32, 64],\n",
    "    'objective_col': [('adaptation',), ('Log sensitivity',)],\n",
    "    'x_type': ['energies', 'binding_rates_dissociation'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'use_l2_reg': [True],\n",
    "    'l2_reg_alpha': [0, 1e-2, 1e-3, 1e-4],\n",
    "    'kl_weight': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "}\n",
    "\n",
    "df_hpos.loc[df_hpos['objective_col'] == 'sensitivity_wrt_species-6', 'prep_y_logscale'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def keep_equal(df):\n",
    "    pairs = {\n",
    "        'enc_ls': 'dec_ls',\n",
    "        'num_enc_layers': 'num_dec_layers',\n",
    "        'factor_expanding_ls': 'factor_contracting_ls',\n",
    "    }\n",
    "    for k1, k2 in pairs.items():\n",
    "        if k1 in df.columns and k2 in df.columns:\n",
    "            df[k2] = df[k1]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_combinatorial_keys(df_hpos, hpos_to_vary_together, basic_setting):\n",
    "    keys_vary_together = sorted(hpos_to_vary_together.keys())\n",
    "    for v in itertools.product(*[hpos_to_vary_together[h] for h in keys_vary_together]):\n",
    "        curr = basic_setting.assign(\n",
    "            **{h: vv for h, vv in zip(keys_vary_together, v)})\n",
    "        df_hpos = pd.concat([df_hpos, curr], ignore_index=True)\n",
    "    return df_hpos\n",
    "\n",
    "\n",
    "def add_single_hpos(df_hpos, hpos_to_vary_from_og, basic_setting):\n",
    "    for h, v in hpos_to_vary_from_og.items():\n",
    "        try:\n",
    "            df_hpos = pd.concat(\n",
    "                [df_hpos] + [basic_setting.assign(**{h: vv}) for vv in v], ignore_index=True)\n",
    "        except ValueError:\n",
    "            for vv in v:\n",
    "                b = basic_setting.copy()\n",
    "                b.loc[0, h] = vv\n",
    "                df_hpos = pd.concat([df_hpos, b], ignore_index=True)\n",
    "    return df_hpos\n",
    "\n",
    "\n",
    "def postproc(df_hpos):\n",
    "    df_hpos = keep_equal(df_hpos)\n",
    "    df_hpos.loc[df_hpos['x_type'] ==\n",
    "                'binding_rates_dissociation', 'prep_x_negative'] = False\n",
    "    df_hpos = df_hpos.drop_duplicates().reset_index(drop=True)\n",
    "    return df_hpos\n",
    "\n",
    "\n",
    "df_hpos = add_single_hpos(df_hpos, hpos_to_vary_from_og3, basic_setting)\n",
    "df_hpos = add_single_hpos(df_hpos, hpos_to_vary_from_og2, basic_setting)\n",
    "df_hpos = add_combinatorial_keys(df_hpos, hpos_to_vary_together, basic_setting)\n",
    "df_hpos = add_combinatorial_keys(\n",
    "    df_hpos, hpos_to_vary_together2, basic_setting)\n",
    "df_hpos = add_single_hpos(df_hpos, hpos_to_vary_from_og, basic_setting)\n",
    "df_hpos = postproc(df_hpos)\n",
    "\n",
    "# Reorder columns\n",
    "cols_priority = list(set(list(hpos_to_vary_from_og.keys(\n",
    ")) + list(hpos_to_vary_together.keys()) + list(hpos_to_vary_together2.keys())))\n",
    "df_hpos = df_hpos[cols_priority +\n",
    "                  [c for c in df_hpos.columns if c not in cols_priority]]\n",
    "\n",
    "df_hpos.reset_index(drop=True)\n",
    "len(df_hpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use table to create dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = '../data/raw/summarise_simulation/2024_11_21_144918/tabulated_mutation_info.csv'\n",
    "# # fn = '../data/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv'\n",
    "# # fn = '../data/raw/summarise_simulation/2024_12_05_210221/tabulated_mutation_info.csv'\n",
    "# data = pd.read_csv(fn).iloc[:100]\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xla_bridge.py:backends():900: Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA INFO\n",
      "xla_bridge.py:backends():900: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory INFO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.py:train():160: Epoch 0 / 3000 -\t\t Train loss: 0.0850759968161583\tVal loss: 0.0551363080739975\tVal accuracy: 0.29670390486717224 INFO\n",
      "train.py:train():160: Epoch 10 / 3000 -\t\t Train loss: 0.030824551358819008\tVal loss: 0.03185836225748062\tVal accuracy: 0.48193779587745667 INFO\n",
      "train.py:train():160: Epoch 20 / 3000 -\t\t Train loss: 0.027099063619971275\tVal loss: 0.02640525996685028\tVal accuracy: 0.5190346240997314 INFO\n",
      "train.py:train():160: Epoch 30 / 3000 -\t\t Train loss: 0.027168527245521545\tVal loss: 0.026873325929045677\tVal accuracy: 0.5115935802459717 INFO\n",
      "train.py:train():160: Epoch 40 / 3000 -\t\t Train loss: 0.024896027520298958\tVal loss: 0.02483229525387287\tVal accuracy: 0.5371261239051819 INFO\n",
      "train.py:train():160: Epoch 50 / 3000 -\t\t Train loss: 0.023048004135489464\tVal loss: 0.02374468557536602\tVal accuracy: 0.5354818105697632 INFO\n",
      "train.py:train():160: Epoch 60 / 3000 -\t\t Train loss: 0.02287459373474121\tVal loss: 0.022974994033575058\tVal accuracy: 0.5591738820075989 INFO\n",
      "train.py:train():160: Epoch 70 / 3000 -\t\t Train loss: 0.022471172735095024\tVal loss: 0.022347992286086082\tVal accuracy: 0.5596371293067932 INFO\n",
      "train.py:train():160: Epoch 80 / 3000 -\t\t Train loss: 0.022000161930918694\tVal loss: 0.021839499473571777\tVal accuracy: 0.5544663667678833 INFO\n",
      "train.py:train():160: Epoch 90 / 3000 -\t\t Train loss: 0.021534474566578865\tVal loss: 0.02140284702181816\tVal accuracy: 0.5607555508613586 INFO\n",
      "train.py:train():160: Epoch 100 / 3000 -\t\t Train loss: 0.021066641435027122\tVal loss: 0.020928042009472847\tVal accuracy: 0.5551883578300476 INFO\n",
      "train.py:train():160: Epoch 110 / 3000 -\t\t Train loss: 0.021222254261374474\tVal loss: 0.0209229476749897\tVal accuracy: 0.5624624490737915 INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.py:train():160: Epoch 120 / 3000 -\t\t Train loss: 0.02116556093096733\tVal loss: 0.0206432081758976\tVal accuracy: 0.5662518739700317 INFO\n",
      "train.py:train():160: Epoch 130 / 3000 -\t\t Train loss: 0.020534861832857132\tVal loss: 0.020283784717321396\tVal accuracy: 0.5772486329078674 INFO\n",
      "train.py:train():160: Epoch 140 / 3000 -\t\t Train loss: 0.021957850083708763\tVal loss: 0.02206609956920147\tVal accuracy: 0.5719943642616272 INFO\n",
      "train.py:train():160: Epoch 150 / 3000 -\t\t Train loss: 0.02367209456861019\tVal loss: 0.022873874753713608\tVal accuracy: 0.5479475855827332 INFO\n",
      "train.py:train():160: Epoch 160 / 3000 -\t\t Train loss: 0.02248680219054222\tVal loss: 0.02300904132425785\tVal accuracy: 0.5405356884002686 INFO\n",
      "train.py:train():160: Epoch 170 / 3000 -\t\t Train loss: 0.0221996009349823\tVal loss: 0.022671859711408615\tVal accuracy: 0.55126953125 INFO\n",
      "train.py:train():160: Epoch 180 / 3000 -\t\t Train loss: 0.02576666884124279\tVal loss: 0.025834882631897926\tVal accuracy: 0.5406650900840759 INFO\n",
      "train.py:train():160: Epoch 190 / 3000 -\t\t Train loss: 0.02427845075726509\tVal loss: 0.02453066222369671\tVal accuracy: 0.5399848222732544 INFO\n",
      "train.py:train():160: Epoch 200 / 3000 -\t\t Train loss: 0.02353961579501629\tVal loss: 0.023312421515583992\tVal accuracy: 0.558518648147583 INFO\n",
      "train.py:train():160: Epoch 210 / 3000 -\t\t Train loss: 0.023679610341787338\tVal loss: 0.022829659283161163\tVal accuracy: 0.5542243123054504 INFO\n",
      "train.py:train():160: Epoch 220 / 3000 -\t\t Train loss: 0.022940002381801605\tVal loss: 0.02288524992763996\tVal accuracy: 0.5489032864570618 INFO\n",
      "train.py:train():160: Epoch 230 / 3000 -\t\t Train loss: 0.02379947155714035\tVal loss: 0.02350171096622944\tVal accuracy: 0.5736511945724487 INFO\n",
      "train.py:train():160: Epoch 240 / 3000 -\t\t Train loss: 0.02230128087103367\tVal loss: 0.0229337178170681\tVal accuracy: 0.5395257472991943 INFO\n",
      "train.py:train():160: Epoch 250 / 3000 -\t\t Train loss: 1.4664030075073242\tVal loss: 0.024250537157058716\tVal accuracy: 0.5394840240478516 INFO\n",
      "train.py:train():160: Epoch 260 / 3000 -\t\t Train loss: 0.023357093334197998\tVal loss: 0.023431571200489998\tVal accuracy: 0.5786717534065247 INFO\n",
      "train.py:train():160: Epoch 270 / 3000 -\t\t Train loss: 0.02283266745507717\tVal loss: 0.023540537804365158\tVal accuracy: 0.5493540167808533 INFO\n",
      "train.py:train():160: Epoch 280 / 3000 -\t\t Train loss: 0.022227412089705467\tVal loss: 0.02193508669734001\tVal accuracy: 0.5783044695854187 INFO\n",
      "train.py:train():160: Epoch 290 / 3000 -\t\t Train loss: 0.02200336754322052\tVal loss: 0.02162066660821438\tVal accuracy: 0.5632596015930176 INFO\n",
      "train.py:train():160: Epoch 300 / 3000 -\t\t Train loss: 0.021655147895216942\tVal loss: 0.022455351427197456\tVal accuracy: 0.5764515399932861 INFO\n",
      "train.py:train():160: Epoch 310 / 3000 -\t\t Train loss: 0.022766444832086563\tVal loss: 0.022923685610294342\tVal accuracy: 0.5516409873962402 INFO\n",
      "train.py:train():160: Epoch 320 / 3000 -\t\t Train loss: 0.021920770406723022\tVal loss: 0.021796833723783493\tVal accuracy: 0.5667985677719116 INFO\n",
      "train.py:train():160: Epoch 330 / 3000 -\t\t Train loss: 0.02220349758863449\tVal loss: 0.022072697058320045\tVal accuracy: 0.5766226053237915 INFO\n",
      "train.py:train():160: Epoch 340 / 3000 -\t\t Train loss: 0.022891785949468613\tVal loss: 0.022713882848620415\tVal accuracy: 0.5733799338340759 INFO\n",
      "train.py:train():160: Epoch 350 / 3000 -\t\t Train loss: 0.036932893097400665\tVal loss: 0.02275647036731243\tVal accuracy: 0.5710511803627014 INFO\n",
      "train.py:train():160: Epoch 360 / 3000 -\t\t Train loss: 0.024126680567860603\tVal loss: 0.02490878291428089\tVal accuracy: 0.5498464703559875 INFO\n",
      "train.py:train():160: Epoch 370 / 3000 -\t\t Train loss: 0.023236218839883804\tVal loss: 0.024013036862015724\tVal accuracy: 0.5825905203819275 INFO\n",
      "train.py:train():160: Epoch 380 / 3000 -\t\t Train loss: 0.02345477044582367\tVal loss: 0.023663364350795746\tVal accuracy: 0.5681382417678833 INFO\n",
      "train.py:train():160: Epoch 390 / 3000 -\t\t Train loss: 0.02431073784828186\tVal loss: 0.02404734306037426\tVal accuracy: 0.5681757926940918 INFO\n",
      "train.py:train():160: Epoch 400 / 3000 -\t\t Train loss: 0.024868134409189224\tVal loss: 0.02461368776857853\tVal accuracy: 0.5367212891578674 INFO\n",
      "train.py:train():160: Epoch 410 / 3000 -\t\t Train loss: 0.022958559915423393\tVal loss: 0.02349264547228813\tVal accuracy: 0.5776158571243286 INFO\n",
      "train.py:train():160: Epoch 420 / 3000 -\t\t Train loss: 0.022322524338960648\tVal loss: 0.022246181964874268\tVal accuracy: 0.5497504472732544 INFO\n",
      "train.py:train():160: Epoch 430 / 3000 -\t\t Train loss: 0.02377188391983509\tVal loss: 0.02200860157608986\tVal accuracy: 0.5558685660362244 INFO\n",
      "train.py:train():160: Epoch 440 / 3000 -\t\t Train loss: 0.02190367877483368\tVal loss: 0.02192881889641285\tVal accuracy: 0.5650165677070618 INFO\n",
      "train.py:train():160: Epoch 450 / 3000 -\t\t Train loss: 0.021771810948848724\tVal loss: 0.021898677572607994\tVal accuracy: 0.5695446133613586 INFO\n",
      "train.py:train():160: Epoch 460 / 3000 -\t\t Train loss: 0.02182103507220745\tVal loss: 0.02180827409029007\tVal accuracy: 0.5659555792808533 INFO\n",
      "train.py:train():160: Epoch 470 / 3000 -\t\t Train loss: 0.02194700948894024\tVal loss: 0.02215585857629776\tVal accuracy: 0.5649121999740601 INFO\n",
      "train.py:train():160: Epoch 480 / 3000 -\t\t Train loss: 0.021930616348981857\tVal loss: 0.02244727313518524\tVal accuracy: 0.5419671535491943 INFO\n",
      "train.py:train():160: Epoch 490 / 3000 -\t\t Train loss: 0.021388819441199303\tVal loss: 0.021770549938082695\tVal accuracy: 0.5816389918327332 INFO\n",
      "train.py:train():160: Epoch 500 / 3000 -\t\t Train loss: 0.021775733679533005\tVal loss: 0.022255295887589455\tVal accuracy: 0.5616069436073303 INFO\n",
      "train.py:train():160: Epoch 510 / 3000 -\t\t Train loss: 0.31005722284317017\tVal loss: 0.021990371868014336\tVal accuracy: 0.5589401721954346 INFO\n",
      "train.py:train():160: Epoch 520 / 3000 -\t\t Train loss: 0.02114107459783554\tVal loss: 0.021247392520308495\tVal accuracy: 0.5678085088729858 INFO\n",
      "train.py:train():160: Epoch 530 / 3000 -\t\t Train loss: 0.02100025676190853\tVal loss: 0.020796651020646095\tVal accuracy: 0.5791725516319275 INFO\n",
      "train.py:train():160: Epoch 540 / 3000 -\t\t Train loss: 0.02102145180106163\tVal loss: 0.020599106326699257\tVal accuracy: 0.5726621150970459 INFO\n",
      "train.py:train():160: Epoch 550 / 3000 -\t\t Train loss: 0.020631205290555954\tVal loss: 0.02069149911403656\tVal accuracy: 0.582398533821106 INFO\n",
      "train.py:train():160: Epoch 560 / 3000 -\t\t Train loss: 0.02077816054224968\tVal loss: 0.020039845257997513\tVal accuracy: 0.5850611329078674 INFO\n",
      "train.py:train():160: Epoch 570 / 3000 -\t\t Train loss: 0.020610859617590904\tVal loss: 0.020931238308548927\tVal accuracy: 0.579468846321106 INFO\n",
      "train.py:train():160: Epoch 580 / 3000 -\t\t Train loss: 0.020582957193255424\tVal loss: 0.02094489336013794\tVal accuracy: 0.5839343070983887 INFO\n",
      "train.py:train():160: Epoch 590 / 3000 -\t\t Train loss: 0.02024006098508835\tVal loss: 0.0217436533421278\tVal accuracy: 0.5558351874351501 INFO\n",
      "train.py:train():160: Epoch 600 / 3000 -\t\t Train loss: 0.02008090540766716\tVal loss: 0.020252026617527008\tVal accuracy: 0.5731796026229858 INFO\n",
      "train.py:train():160: Epoch 610 / 3000 -\t\t Train loss: 0.2090466171503067\tVal loss: 0.02123440057039261\tVal accuracy: 0.5820855498313904 INFO\n",
      "train.py:train():160: Epoch 620 / 3000 -\t\t Train loss: 0.02038867026567459\tVal loss: 0.020388422533869743\tVal accuracy: 0.5766727328300476 INFO\n",
      "train.py:train():160: Epoch 630 / 3000 -\t\t Train loss: 0.019758090376853943\tVal loss: 0.01972137577831745\tVal accuracy: 0.5906200408935547 INFO\n",
      "train.py:train():160: Epoch 640 / 3000 -\t\t Train loss: 0.020154202356934547\tVal loss: 0.020227482542395592\tVal accuracy: 0.5853991508483887 INFO\n",
      "train.py:train():160: Epoch 650 / 3000 -\t\t Train loss: 27.875701904296875\tVal loss: 0.020141461864113808\tVal accuracy: 0.582294225692749 INFO\n",
      "train.py:train():160: Epoch 660 / 3000 -\t\t Train loss: 0.01995212584733963\tVal loss: 0.019943207502365112\tVal accuracy: 0.5810714364051819 INFO\n",
      "train.py:train():160: Epoch 670 / 3000 -\t\t Train loss: 0.020228132605552673\tVal loss: 0.01998397521674633\tVal accuracy: 0.5861420035362244 INFO\n",
      "train.py:train():160: Epoch 680 / 3000 -\t\t Train loss: 0.020726291462779045\tVal loss: 0.02091415598988533\tVal accuracy: 0.5827615857124329 INFO\n",
      "train.py:train():160: Epoch 690 / 3000 -\t\t Train loss: 0.02022327482700348\tVal loss: 0.020313069224357605\tVal accuracy: 0.6078183650970459 INFO\n",
      "train.py:train():160: Epoch 700 / 3000 -\t\t Train loss: 0.019552450627088547\tVal loss: 0.019958585500717163\tVal accuracy: 0.5827240347862244 INFO\n",
      "train.py:train():160: Epoch 710 / 3000 -\t\t Train loss: 0.019730176776647568\tVal loss: 0.020299803465604782\tVal accuracy: 0.5640107989311218 INFO\n",
      "train.py:train():160: Epoch 720 / 3000 -\t\t Train loss: 0.020029932260513306\tVal loss: 0.01971055008471012\tVal accuracy: 0.5902611017227173 INFO\n",
      "train.py:train():160: Epoch 730 / 3000 -\t\t Train loss: 0.020086832344532013\tVal loss: 0.020090317353606224\tVal accuracy: 0.5926607847213745 INFO\n",
      "train.py:train():160: Epoch 740 / 3000 -\t\t Train loss: 0.021585671231150627\tVal loss: 0.019906900823116302\tVal accuracy: 0.5847481489181519 INFO\n",
      "train.py:train():160: Epoch 750 / 3000 -\t\t Train loss: 0.02016041800379753\tVal loss: 0.020152952522039413\tVal accuracy: 0.5893012285232544 INFO\n",
      "train.py:train():160: Epoch 760 / 3000 -\t\t Train loss: 145.45596313476562\tVal loss: 0.021242285147309303\tVal accuracy: 0.5709468722343445 INFO\n",
      "train.py:train():160: Epoch 770 / 3000 -\t\t Train loss: 0.04774023965001106\tVal loss: 0.020830603316426277\tVal accuracy: 0.5822440981864929 INFO\n",
      "train.py:train():160: Epoch 780 / 3000 -\t\t Train loss: 0.021842479705810547\tVal loss: 0.021493494510650635\tVal accuracy: 0.5830621123313904 INFO\n",
      "train.py:train():160: Epoch 790 / 3000 -\t\t Train loss: 0.028787370771169662\tVal loss: 0.020593784749507904\tVal accuracy: 0.5855327248573303 INFO\n",
      "train.py:train():160: Epoch 800 / 3000 -\t\t Train loss: 0.020359501242637634\tVal loss: 0.020270204171538353\tVal accuracy: 0.5978565812110901 INFO\n",
      "train.py:train():160: Epoch 810 / 3000 -\t\t Train loss: 0.0203598253428936\tVal loss: 0.020918255671858788\tVal accuracy: 0.5941840410232544 INFO\n",
      "train.py:train():160: Epoch 820 / 3000 -\t\t Train loss: 0.020929835736751556\tVal loss: 0.020969638600945473\tVal accuracy: 0.5842598676681519 INFO\n",
      "train.py:train():160: Epoch 830 / 3000 -\t\t Train loss: 0.020571449771523476\tVal loss: 0.02053772658109665\tVal accuracy: 0.591003954410553 INFO\n",
      "train.py:train():160: Epoch 840 / 3000 -\t\t Train loss: 0.020437950268387794\tVal loss: 0.020691189914941788\tVal accuracy: 0.5897144079208374 INFO\n",
      "train.py:train():160: Epoch 850 / 3000 -\t\t Train loss: 0.020605267956852913\tVal loss: 0.02068331465125084\tVal accuracy: 0.5725160241127014 INFO\n",
      "train.py:train():160: Epoch 860 / 3000 -\t\t Train loss: 0.02048433944582939\tVal loss: 0.02036452852189541\tVal accuracy: 0.5865259766578674 INFO\n",
      "train.py:train():160: Epoch 870 / 3000 -\t\t Train loss: 0.020374367013573647\tVal loss: 0.02023901976644993\tVal accuracy: 0.5908579230308533 INFO\n",
      "train.py:train():160: Epoch 880 / 3000 -\t\t Train loss: 0.020728040486574173\tVal loss: 2.8779711723327637\tVal accuracy: 0.592881977558136 INFO\n",
      "train.py:train():160: Epoch 890 / 3000 -\t\t Train loss: 0.01990235038101673\tVal loss: 0.020022867247462273\tVal accuracy: 0.5842974185943604 INFO\n",
      "train.py:train():160: Epoch 900 / 3000 -\t\t Train loss: 0.020001860335469246\tVal loss: 0.020442677661776543\tVal accuracy: 0.5917342901229858 INFO\n",
      "train.py:train():160: Epoch 910 / 3000 -\t\t Train loss: 0.01987680420279503\tVal loss: 0.020211590453982353\tVal accuracy: 0.5741186141967773 INFO\n",
      "train.py:train():160: Epoch 920 / 3000 -\t\t Train loss: 0.02009136974811554\tVal loss: 539.7513427734375\tVal accuracy: 0.5775866508483887 INFO\n",
      "train.py:train():160: Epoch 930 / 3000 -\t\t Train loss: 0.021029789000749588\tVal loss: 0.021474719047546387\tVal accuracy: 0.579255998134613 INFO\n",
      "train.py:train():160: Epoch 940 / 3000 -\t\t Train loss: 0.0199073925614357\tVal loss: 0.020022094249725342\tVal accuracy: 0.5859500765800476 INFO\n",
      "train.py:train():160: Epoch 950 / 3000 -\t\t Train loss: 0.01966865547001362\tVal loss: 0.019736172631382942\tVal accuracy: 0.5956989526748657 INFO\n",
      "train.py:train():160: Epoch 960 / 3000 -\t\t Train loss: 0.021012911573052406\tVal loss: 0.0210207961499691\tVal accuracy: 0.5929445624351501 INFO\n",
      "train.py:train():160: Epoch 970 / 3000 -\t\t Train loss: 0.019803937524557114\tVal loss: 0.02076592482626438\tVal accuracy: 0.5582473874092102 INFO\n",
      "train.py:train():160: Epoch 980 / 3000 -\t\t Train loss: 0.019933443516492844\tVal loss: 1.126261591911316\tVal accuracy: 0.5864508748054504 INFO\n",
      "train.py:train():160: Epoch 990 / 3000 -\t\t Train loss: 0.020852623507380486\tVal loss: 10658.6943359375\tVal accuracy: 0.5857664346694946 INFO\n",
      "train.py:train():160: Epoch 1000 / 3000 -\t\t Train loss: 0.020050525665283203\tVal loss: 0.038155727088451385\tVal accuracy: 0.5967757105827332 INFO\n",
      "train.py:train():160: Epoch 1010 / 3000 -\t\t Train loss: 0.02004024013876915\tVal loss: 0.020274925976991653\tVal accuracy: 0.5791183114051819 INFO\n",
      "train.py:train():160: Epoch 1020 / 3000 -\t\t Train loss: 0.5759533047676086\tVal loss: 0.01974310725927353\tVal accuracy: 0.5923603177070618 INFO\n",
      "train.py:train():160: Epoch 1030 / 3000 -\t\t Train loss: 0.019394053146243095\tVal loss: 0.019564038142561913\tVal accuracy: 0.5991837382316589 INFO\n",
      "train.py:train():160: Epoch 1040 / 3000 -\t\t Train loss: 0.019256284460425377\tVal loss: 0.019228285178542137\tVal accuracy: 0.5948851704597473 INFO\n",
      "train.py:train():160: Epoch 1050 / 3000 -\t\t Train loss: 0.01951204240322113\tVal loss: 0.019929364323616028\tVal accuracy: 0.5914004445075989 INFO\n",
      "train.py:train():160: Epoch 1060 / 3000 -\t\t Train loss: 0.0196387842297554\tVal loss: 0.01969209872186184\tVal accuracy: 0.5929111838340759 INFO\n",
      "train.py:train():160: Epoch 1070 / 3000 -\t\t Train loss: 0.019404055550694466\tVal loss: 0.019134409725666046\tVal accuracy: 0.6045882105827332 INFO\n",
      "train.py:train():160: Epoch 1080 / 3000 -\t\t Train loss: 0.019612809643149376\tVal loss: 0.019720187410712242\tVal accuracy: 0.5934620499610901 INFO\n",
      "train.py:train():160: Epoch 1090 / 3000 -\t\t Train loss: 0.019630609080195427\tVal loss: 0.01943431794643402\tVal accuracy: 0.596345841884613 INFO\n",
      "train.py:train():160: Epoch 1100 / 3000 -\t\t Train loss: 0.019510073587298393\tVal loss: 0.01961360312998295\tVal accuracy: 0.5991420149803162 INFO\n",
      "train.py:train():160: Epoch 1110 / 3000 -\t\t Train loss: 0.06830395013093948\tVal loss: 0.01956770196557045\tVal accuracy: 0.5841304659843445 INFO\n",
      "train.py:train():160: Epoch 1120 / 3000 -\t\t Train loss: 11.121066093444824\tVal loss: 2.6647143363952637\tVal accuracy: 0.6027769446372986 INFO\n",
      "train.py:train():160: Epoch 1130 / 3000 -\t\t Train loss: 0.02021905593574047\tVal loss: 3.641428232192993\tVal accuracy: 0.5799070596694946 INFO\n",
      "train.py:train():160: Epoch 1140 / 3000 -\t\t Train loss: 0.020015738904476166\tVal loss: 0.019789110869169235\tVal accuracy: 0.5909455418586731 INFO\n",
      "train.py:train():160: Epoch 1150 / 3000 -\t\t Train loss: 0.019704313948750496\tVal loss: 0.019774455577135086\tVal accuracy: 0.5915924310684204 INFO\n",
      "train.py:train():160: Epoch 1160 / 3000 -\t\t Train loss: 0.01956387795507908\tVal loss: 0.019553670659661293\tVal accuracy: 0.5967965722084045 INFO\n",
      "train.py:train():160: Epoch 1170 / 3000 -\t\t Train loss: 0.019712233915925026\tVal loss: 0.019738560542464256\tVal accuracy: 0.5838842391967773 INFO\n",
      "train.py:train():160: Epoch 1180 / 3000 -\t\t Train loss: 0.01997559703886509\tVal loss: 0.02036321721971035\tVal accuracy: 0.5866469740867615 INFO\n",
      "train.py:train():160: Epoch 1190 / 3000 -\t\t Train loss: 0.020825164392590523\tVal loss: 0.020898152142763138\tVal accuracy: 0.5803744792938232 INFO\n",
      "train.py:train():160: Epoch 1200 / 3000 -\t\t Train loss: 0.020306071266531944\tVal loss: 0.02024691365659237\tVal accuracy: 0.5916383266448975 INFO\n",
      "train.py:train():160: Epoch 1210 / 3000 -\t\t Train loss: 0.022018956020474434\tVal loss: 0.01942434161901474\tVal accuracy: 0.5949644446372986 INFO\n",
      "train.py:train():160: Epoch 1220 / 3000 -\t\t Train loss: 0.019849533215165138\tVal loss: 0.019719716161489487\tVal accuracy: 0.5863548517227173 INFO\n",
      "train.py:train():160: Epoch 1230 / 3000 -\t\t Train loss: 0.019496480002999306\tVal loss: 2.4956703186035156\tVal accuracy: 0.5924229025840759 INFO\n",
      "train.py:train():160: Epoch 1240 / 3000 -\t\t Train loss: 0.019531164318323135\tVal loss: 0.019182372838258743\tVal accuracy: 0.5926858186721802 INFO\n",
      "train.py:train():160: Epoch 1250 / 3000 -\t\t Train loss: 0.019488846883177757\tVal loss: 0.019613778218626976\tVal accuracy: 0.5873481035232544 INFO\n",
      "train.py:train():160: Epoch 1260 / 3000 -\t\t Train loss: 0.01965358294546604\tVal loss: 49.37487030029297\tVal accuracy: 0.6012244820594788 INFO\n",
      "train.py:train():160: Epoch 1270 / 3000 -\t\t Train loss: 0.7601231932640076\tVal loss: 0.019497781991958618\tVal accuracy: 0.5951647758483887 INFO\n",
      "train.py:train():160: Epoch 1280 / 3000 -\t\t Train loss: 0.019576555117964745\tVal loss: 0.019444867968559265\tVal accuracy: 0.595628023147583 INFO\n",
      "train.py:train():160: Epoch 1290 / 3000 -\t\t Train loss: 0.01937749795615673\tVal loss: 0.019170265644788742\tVal accuracy: 0.5977439284324646 INFO\n",
      "train.py:train():160: Epoch 1300 / 3000 -\t\t Train loss: 0.01931728422641754\tVal loss: 1.3112038373947144\tVal accuracy: 0.5918386578559875 INFO\n",
      "train.py:train():160: Epoch 1310 / 3000 -\t\t Train loss: 0.019122609868645668\tVal loss: 0.01936991885304451\tVal accuracy: 0.5943301320075989 INFO\n",
      "train.py:train():160: Epoch 1320 / 3000 -\t\t Train loss: 0.019721973687410355\tVal loss: 0.019749078899621964\tVal accuracy: 0.5868222713470459 INFO\n",
      "train.py:train():160: Epoch 1330 / 3000 -\t\t Train loss: 13.89642333984375\tVal loss: 0.020003674551844597\tVal accuracy: 0.5873606204986572 INFO\n",
      "train.py:train():160: Epoch 1340 / 3000 -\t\t Train loss: 0.019612960517406464\tVal loss: 0.020029360428452492\tVal accuracy: 0.5806791186332703 INFO\n",
      "train.py:train():160: Epoch 1350 / 3000 -\t\t Train loss: 0.019579598680138588\tVal loss: 0.019712910056114197\tVal accuracy: 0.5927442312240601 INFO\n",
      "train.py:train():160: Epoch 1360 / 3000 -\t\t Train loss: 0.019429540261626244\tVal loss: 0.01943601295351982\tVal accuracy: 0.5926065444946289 INFO\n",
      "train.py:train():160: Epoch 1370 / 3000 -\t\t Train loss: 0.01949566975235939\tVal loss: 0.019407153129577637\tVal accuracy: 0.5915423035621643 INFO\n",
      "train.py:train():160: Epoch 1380 / 3000 -\t\t Train loss: 0.019874507561326027\tVal loss: 0.019937612116336823\tVal accuracy: 0.575792133808136 INFO\n",
      "train.py:train():160: Epoch 1390 / 3000 -\t\t Train loss: 0.02027958258986473\tVal loss: 0.02056889794766903\tVal accuracy: 0.5714268088340759 INFO\n",
      "train.py:train():160: Epoch 1400 / 3000 -\t\t Train loss: 0.0198360588401556\tVal loss: 0.019922148436307907\tVal accuracy: 0.5889590382575989 INFO\n",
      "train.py:train():160: Epoch 1410 / 3000 -\t\t Train loss: 0.019816944375634193\tVal loss: 0.019849391654133797\tVal accuracy: 0.5843015909194946 INFO\n",
      "train.py:train():160: Epoch 1420 / 3000 -\t\t Train loss: 0.7612499594688416\tVal loss: 0.019261416047811508\tVal accuracy: 0.5903612971305847 INFO\n",
      "train.py:train():160: Epoch 1430 / 3000 -\t\t Train loss: 0.01938275247812271\tVal loss: 0.019603898748755455\tVal accuracy: 0.5890925526618958 INFO\n",
      "train.py:train():160: Epoch 1440 / 3000 -\t\t Train loss: 0.01923426054418087\tVal loss: 0.0195477195084095\tVal accuracy: 0.5816765427589417 INFO\n",
      "train.py:train():160: Epoch 1450 / 3000 -\t\t Train loss: 0.019373396411538124\tVal loss: 0.019262131303548813\tVal accuracy: 0.591750979423523 INFO\n",
      "train.py:train():160: Epoch 1460 / 3000 -\t\t Train loss: 0.020038338378071785\tVal loss: 0.020008116960525513\tVal accuracy: 0.5753330588340759 INFO\n",
      "train.py:train():160: Epoch 1470 / 3000 -\t\t Train loss: 0.020756486803293228\tVal loss: 0.020745325833559036\tVal accuracy: 0.5847856998443604 INFO\n",
      "train.py:train():160: Epoch 1480 / 3000 -\t\t Train loss: 0.020326292142271996\tVal loss: 0.01994195394217968\tVal accuracy: 0.592439591884613 INFO\n",
      "train.py:train():160: Epoch 1490 / 3000 -\t\t Train loss: 0.020557979121804237\tVal loss: 0.020549844950437546\tVal accuracy: 0.5783461928367615 INFO\n",
      "train.py:train():160: Epoch 1500 / 3000 -\t\t Train loss: 0.021018438041210175\tVal loss: 0.02074408158659935\tVal accuracy: 0.5780332088470459 INFO\n",
      "train.py:train():160: Epoch 1510 / 3000 -\t\t Train loss: 0.14610616862773895\tVal loss: 0.020875755697488785\tVal accuracy: 0.5868890285491943 INFO\n",
      "train.py:train():160: Epoch 1520 / 3000 -\t\t Train loss: 0.020485833287239075\tVal loss: 0.020535780116915703\tVal accuracy: 0.5794229507446289 INFO\n",
      "train.py:train():160: Epoch 1530 / 3000 -\t\t Train loss: 0.021079104393720627\tVal loss: 0.02105173096060753\tVal accuracy: 0.5751702785491943 INFO\n",
      "train.py:train():160: Epoch 1540 / 3000 -\t\t Train loss: 0.02149457484483719\tVal loss: 0.021564310416579247\tVal accuracy: 0.5836463570594788 INFO\n",
      "train.py:train():160: Epoch 1550 / 3000 -\t\t Train loss: 0.02158723585307598\tVal loss: 0.021479010581970215\tVal accuracy: 0.5772361159324646 INFO\n",
      "train.py:train():160: Epoch 1560 / 3000 -\t\t Train loss: 0.02185780555009842\tVal loss: 0.021795272827148438\tVal accuracy: 0.5727205276489258 INFO\n",
      "train.py:train():160: Epoch 1570 / 3000 -\t\t Train loss: 0.022131644189357758\tVal loss: 0.022468240931630135\tVal accuracy: 0.5581555962562561 INFO\n",
      "train.py:train():160: Epoch 1580 / 3000 -\t\t Train loss: 0.02186293713748455\tVal loss: 0.02242167666554451\tVal accuracy: 0.551223635673523 INFO\n",
      "train.py:train():160: Epoch 1590 / 3000 -\t\t Train loss: 0.021744059398770332\tVal loss: 0.021950282156467438\tVal accuracy: 0.5686682462692261 INFO\n",
      "train.py:train():160: Epoch 1600 / 3000 -\t\t Train loss: 0.021692520007491112\tVal loss: 0.022004883736371994\tVal accuracy: 0.5730127096176147 INFO\n",
      "train.py:train():160: Epoch 1610 / 3000 -\t\t Train loss: 0.021267399191856384\tVal loss: 0.021518748253583908\tVal accuracy: 0.5843725204467773 INFO\n",
      "train.py:train():160: Epoch 1620 / 3000 -\t\t Train loss: 0.021374240517616272\tVal loss: 16.632808685302734\tVal accuracy: 0.5761551856994629 INFO\n",
      "train.py:train():160: Epoch 1630 / 3000 -\t\t Train loss: 0.02088027261197567\tVal loss: 0.021024003624916077\tVal accuracy: 0.579285204410553 INFO\n",
      "train.py:train():160: Epoch 1640 / 3000 -\t\t Train loss: 0.020666444674134254\tVal loss: 0.020744582638144493\tVal accuracy: 0.5833709239959717 INFO\n",
      "train.py:train():160: Epoch 1650 / 3000 -\t\t Train loss: 0.12891273200511932\tVal loss: 0.020700376480817795\tVal accuracy: 0.5813510417938232 INFO\n",
      "train.py:train():160: Epoch 1660 / 3000 -\t\t Train loss: 0.020291054621338844\tVal loss: 0.020269855856895447\tVal accuracy: 0.5838425159454346 INFO\n",
      "train.py:train():160: Epoch 1670 / 3000 -\t\t Train loss: 0.020476382225751877\tVal loss: 31.38124656677246\tVal accuracy: 0.5826614499092102 INFO\n",
      "train.py:train():160: Epoch 1680 / 3000 -\t\t Train loss: 0.020478347316384315\tVal loss: 79.9236831665039\tVal accuracy: 0.5330028533935547 INFO\n",
      "train.py:train():160: Epoch 1690 / 3000 -\t\t Train loss: 0.020500091835856438\tVal loss: 0.02047662064433098\tVal accuracy: 0.5812132954597473 INFO\n",
      "train.py:train():160: Epoch 1700 / 3000 -\t\t Train loss: 0.01994580216705799\tVal loss: 161.96249389648438\tVal accuracy: 0.5766643285751343 INFO\n",
      "train.py:train():160: Epoch 1710 / 3000 -\t\t Train loss: 0.02016686089336872\tVal loss: 0.01998782530426979\tVal accuracy: 0.5905532240867615 INFO\n",
      "train.py:train():160: Epoch 1720 / 3000 -\t\t Train loss: 0.019931625574827194\tVal loss: 10.395461082458496\tVal accuracy: 0.5954611301422119 INFO\n",
      "train.py:train():160: Epoch 1730 / 3000 -\t\t Train loss: 0.020263105630874634\tVal loss: 0.02039949782192707\tVal accuracy: 0.5836880803108215 INFO\n",
      "train.py:train():160: Epoch 1740 / 3000 -\t\t Train loss: 0.020874816924333572\tVal loss: 0.020699549466371536\tVal accuracy: 0.5990418195724487 INFO\n",
      "train.py:train():160: Epoch 1750 / 3000 -\t\t Train loss: 0.020365053787827492\tVal loss: 0.021008718758821487\tVal accuracy: 0.5719901919364929 INFO\n",
      "train.py:train():160: Epoch 1760 / 3000 -\t\t Train loss: 0.020409684628248215\tVal loss: 0.020565535873174667\tVal accuracy: 0.5815262794494629 INFO\n",
      "train.py:train():160: Epoch 1770 / 3000 -\t\t Train loss: 0.020360194146633148\tVal loss: 0.020275458693504333\tVal accuracy: 0.5816556811332703 INFO\n",
      "train.py:train():160: Epoch 1780 / 3000 -\t\t Train loss: 1709.522216796875\tVal loss: 0.020908983424305916\tVal accuracy: 0.5704085230827332 INFO\n",
      "train.py:train():160: Epoch 1790 / 3000 -\t\t Train loss: 0.019747821614146233\tVal loss: 26.76541519165039\tVal accuracy: 0.590699315071106 INFO\n",
      "train.py:train():170: Early stopping triggered after 1799 epochs:\n",
      "Train loss: 0.020501604303717613\n",
      "Val loss: 0.02056266926229\n",
      "Val accuracy: 0.5936707258224487\n",
      "Epochs no improvement: 501 WARNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete: 0:33:05.852787\n",
      "Warning: not using the test data for evaluation, but the training data instead of ../data/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv\n",
      "The R2 score is  0.6571739315986633\n",
      "The R2 score with weighted variance is  0.6571739315986633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/src/evoscaper/utils/visualise.py:30: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "/workdir/src/evoscaper/utils/visualise.py:32: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(save_path, transparent=True, dpi=300)\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Failed to allocate request for 3.74GiB (4016777216B) on device ordinal 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m top_write_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(top_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcvae_scan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhpo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhpos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m hpos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_grad_clipping\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m hpos \u001b[38;5;241m=\u001b[39m \u001b[43mcvae_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_write_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_write_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         hpos = cvae_scan(hpos, top_write_dir=top_write_dir)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     hpos.loc['run_successful'] = False\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     hpos.loc['error_msg'] = 'sys exit'\u001b[39;00m\n\u001b[1;32m     32\u001b[0m df_hpos_main\u001b[38;5;241m.\u001b[39mloc[i] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(hpos) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(hpos) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m hpos\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/workdir/src/evoscaper/scripts/cvae_scan.py:180\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hpos, top_write_dir)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning: not using the test data for evaluation, but the training data instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dataset\u001b[38;5;241m.\u001b[39mfilenames_verify_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    178\u001b[0m     data_test \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 180\u001b[0m r2_test, mi \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mconfig_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_norm_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mx_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_write_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mx_datanormaliser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_methods_preprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                   \u001b[49m\u001b[43my_datanormaliser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_methods_preprocessing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Save stats\u001b[39;00m\n\u001b[1;32m    187\u001b[0m hpos \u001b[38;5;241m=\u001b[39m save_stats(hpos, save_path, total_ds, n_batches, r2_train, r2_test, mi, \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    188\u001b[0m     config_model\u001b[38;5;241m.\u001b[39menc_layers), \u001b[38;5;28mlen\u001b[39m(config_model\u001b[38;5;241m.\u001b[39mdec_layers), info_early_stop)\n",
      "File \u001b[0;32m/workdir/src/evoscaper/scripts/cvae_scan.py:132\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, params, rng, decoder, saves, data_test, config_dataset, config_norm_y, config_model, x_cols, config_filter, top_write_dir, x_datanormaliser, x_methods_preprocessing, y_datanormaliser, y_methods_preprocessing)\u001b[0m\n\u001b[1;32m    128\u001b[0m r2_test \u001b[38;5;241m=\u001b[39m r2_score(x\u001b[38;5;241m.\u001b[39mflatten(), pred_y\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m    130\u001b[0m vis(saves, x, pred_y, top_write_dir)\n\u001b[0;32m--> 132\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[43mtest_conditionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mconfig_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_norm_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_write_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mx_datanormaliser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_methods_preprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m                         \u001b[49m\u001b[43my_datanormaliser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_methods_preprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r2_test, mi\n",
      "File \u001b[0;32m/workdir/src/evoscaper/scripts/cvae_scan.py:97\u001b[0m, in \u001b[0;36mtest_conditionality\u001b[0;34m(params, rng, decoder, df, x_cols, config_dataset, config_norm_y, config_model, top_write_dir, x_datanormaliser, x_methods_preprocessing, y_datanormaliser, y_methods_preprocessing, cond)\u001b[0m\n\u001b[1;32m     89\u001b[0m n_categories \u001b[38;5;241m=\u001b[39m config_norm_y\u001b[38;5;241m.\u001b[39mcategorical_n_bins\n\u001b[1;32m     90\u001b[0m fake_circuits, z, sampled_cond \u001b[38;5;241m=\u001b[39m sample_reconstructions(params, rng, decoder,\n\u001b[1;32m     91\u001b[0m                                                         n_categories\u001b[38;5;241m=\u001b[39mn_categories, n_to_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39mconfig_model\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m     92\u001b[0m                                                         x_datanormaliser\u001b[38;5;241m=\u001b[39mx_datanormaliser, x_methods_preprocessing\u001b[38;5;241m=\u001b[39mx_methods_preprocessing,\n\u001b[1;32m     93\u001b[0m                                                         objective_cols\u001b[38;5;241m=\u001b[39mconfig_dataset\u001b[38;5;241m.\u001b[39mobjective_col,\n\u001b[1;32m     94\u001b[0m                                                         use_binned_sampling\u001b[38;5;241m=\u001b[39mconfig_norm_y\u001b[38;5;241m.\u001b[39mcategorical, use_onehot\u001b[38;5;241m=\u001b[39mconfig_norm_y\u001b[38;5;241m.\u001b[39mcategorical_onehot,\n\u001b[1;32m     95\u001b[0m                                                         cond_min\u001b[38;5;241m=\u001b[39mcond\u001b[38;5;241m.\u001b[39mmin(), cond_max\u001b[38;5;241m=\u001b[39mcond\u001b[38;5;241m.\u001b[39mmax())\n\u001b[0;32m---> 97\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimate_mutual_information_knn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# mi = estimate_mutual_information_knn(z.reshape(np.prod(\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#     z.shape[:-1]), z.shape[-1]), sampled_cond.reshape(np.prod(sampled_cond.shape[:-1]), sampled_cond.shape[-1]), k=5)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#                                fake_circuits, z, sampled_cond, config_norm_y.categorical_onehot,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#                                save_path=os.path.join(top_write_dir, 'combined_layer.png'), multiple='layer', fill=False)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mi\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m/workdir/src/evoscaper/model/evaluation.py:40\u001b[0m, in \u001b[0;36mestimate_mutual_information_knn\u001b[0;34m(z, c, k)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Estimate entropy terms\u001b[39;00m\n\u001b[1;32m     39\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-10\u001b[39m\n\u001b[0;32m---> 40\u001b[0m z_entropy \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(\u001b[43mknn_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m     41\u001b[0m c_entropy \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(knn_distances(c) \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m     42\u001b[0m joint_entropy \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(knn_distances(joint) \u001b[38;5;241m+\u001b[39m eps)\n",
      "File \u001b[0;32m/workdir/src/evoscaper/model/evaluation.py:31\u001b[0m, in \u001b[0;36mestimate_mutual_information_knn.<locals>.knn_distances\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknn_distances\u001b[39m(x):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find k-th nearest neighbor distances.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     dist_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     sorted_dists \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msort(dist_matrix, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sorted_dists[:, k]\n",
      "File \u001b[0;32m/workdir/src/evoscaper/model/evaluation.py:26\u001b[0m, in \u001b[0;36mestimate_mutual_information_knn.<locals>.pairwise_distances\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute pairwise distances between points.\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m x_sq \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m distances \u001b[38;5;241m=\u001b[39m x_sq[:, jnp\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m---> 26\u001b[0m     x_sq[jnp\u001b[38;5;241m.\u001b[39mnewaxis, :] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msqrt(jnp\u001b[38;5;241m.\u001b[39mmaximum(distances, \u001b[38;5;241m0\u001b[39m))\n",
      "    \u001b[0;31m[... skipping hidden 19 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Failed to allocate request for 3.74GiB (4016777216B) on device ordinal 0"
     ]
    }
   ],
   "source": [
    "from bioreaction.misc.misc import load_json_as_dict\n",
    "\n",
    "top_dir = os.path.join('data', '03_cvae_multi', make_datetime_str())\n",
    "os.makedirs(top_dir, exist_ok=True)\n",
    "\n",
    "df_hpos2 = pd.DataFrame(load_json_as_dict(\n",
    "    'data/03_cvae_multi/2025_01_15__10_59_22/df_hpos_main.json'))\n",
    "df_hpos_main = df_hpos.iloc[df_hpos2[df_hpos2['run_successful'].isin(\n",
    "    [False, 'TO_BE_RECORDED'])].index]\n",
    "for i in range(len(df_hpos_main)):\n",
    "    hpos = df_hpos_main.reset_index().iloc[i]\n",
    "    top_write_dir = os.path.join(top_dir, 'cvae_scan', f'hpo_{hpos[\"index\"]}')\n",
    "    # hpos['use_grad_clipping'] = True\n",
    "    # hpos = cvae_scan(hpos, top_write_dir=top_write_dir)\n",
    "    try:\n",
    "        try:\n",
    "            hpos = cvae_scan(hpos, top_write_dir=top_write_dir)\n",
    "            hpos.loc['run_successful'] = True\n",
    "            hpos.loc['error_msg'] = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if 'nan' in e.lower() and (hpos['use_grad_clipping'] == False):\n",
    "                try:\n",
    "                    hpos['use_grad_clipping'] = True\n",
    "                    hpos = cvae_scan(hpos, top_write_dir=top_write_dir)\n",
    "                    hpos.loc['run_successful'] = True\n",
    "                    hpos.loc['error_msg'] = ''\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    hpos.loc['run_successful'] = False\n",
    "                    hpos.loc['error_msg'] = str(e)\n",
    "            else:\n",
    "                hpos.loc['run_successful'] = False\n",
    "                hpos.loc['error_msg'] = str(e)\n",
    "    except:\n",
    "        hpos.loc['run_successful'] = False\n",
    "        hpos.loc['error_msg'] = 'sys exit'\n",
    "\n",
    "    df_hpos_main.loc[i] = pd.Series(hpos) if type(\n",
    "        hpos) == dict else hpos.drop('index')\n",
    "    # df_hpos_main.loc[i] = pd.DataFrame.from_dict(hpos).drop('index')\n",
    "    if not os.path.exists(top_dir):\n",
    "        os.makedirs(top_dir)\n",
    "    df_hpos_main.to_csv(os.path.join(top_dir, 'df_hpos_main.csv'))\n",
    "    write_json(df_hpos_main.to_dict(), os.path.join(\n",
    "        top_dir, 'df_hpos_main.json'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
