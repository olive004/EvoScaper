{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `hk.Module`s must be initialized inside an `hk.transform`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 269\u001b[0m\n\u001b[1;32m    264\u001b[0m     model, params \u001b[38;5;241m=\u001b[39m train_conditional_vae()\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Model ready for further use.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 264\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mMain function to demonstrate Conditional VAE\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Train Conditional VAE\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m model, params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_conditional_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Model ready for further use.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 213\u001b[0m, in \u001b[0;36mtrain_conditional_vae\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m    210\u001b[0m sample_condition \u001b[38;5;241m=\u001b[39m c[:batch_size]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Transform the model into a pair of pure functions\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m model \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mConditionalVAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_dim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Initialize parameters\u001b[39;00m\n\u001b[1;32m    216\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;28mnext\u001b[39m(rng), sample_input, sample_condition)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/haiku/_src/module.py:151\u001b[0m, in \u001b[0;36mModuleMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Now attempt to initialize the object.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m init \u001b[38;5;241m=\u001b[39m wrap_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m ran_super_ctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ran_super_ctor:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/haiku/_src/module.py:439\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the original method with a group name set before and after.\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m base\u001b[38;5;241m.\u001b[39mframe_stack:\n\u001b[0;32m--> 439\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    440\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `hk.Module`s must be initialized inside an `hk.transform`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Submodules are associated with this method. We allow users to associate\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# submodules with a different method than the one being called via\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# `@name_like(\"other_method\")`. Interceptors and custom getters are still\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# provided the actual method name (e.g. \"submodule_method_name\" is only used\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# for naming submodules).\u001b[39;00m\n\u001b[1;32m    447\u001b[0m submodule_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(unbound_method, _CUSTOM_NAME, method_name)\n",
      "\u001b[0;31mValueError\u001b[0m: All `hk.Module`s must be initialized inside an `hk.transform`."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConditionalVAE(hk.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 condition_dim: int, \n",
    "                 latent_dim: int = 20, \n",
    "                 name: str = 'conditional_vae'):\n",
    "        \"\"\"\n",
    "        Conditional Variational Autoencoder using Haiku\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input data\n",
    "            condition_dim (int): Dimension of conditional information\n",
    "            latent_dim (int): Dimension of latent space\n",
    "            name (str): Name of the module\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def __call__(self, x, c, is_training=True):\n",
    "        \"\"\"\n",
    "        Forward pass of the Conditional VAE\n",
    "        \n",
    "        Args:\n",
    "            x (jnp.ndarray): Input data\n",
    "            c (jnp.ndarray): Conditional information\n",
    "            is_training (bool): Training mode flag\n",
    "        \n",
    "        Returns:\n",
    "            x_reconst (jnp.ndarray): Reconstructed input\n",
    "            mu (jnp.ndarray): Mean of latent distribution\n",
    "            logvar (jnp.ndarray): Log variance of latent distribution\n",
    "        \"\"\"\n",
    "        # Concatenate input and condition for encoder\n",
    "        encoder_input = jnp.concatenate([x, c], axis=-1)\n",
    "        \n",
    "        # Encoder network\n",
    "        h = hk.Sequential([\n",
    "            hk.Linear(256), jax.nn.relu,\n",
    "            hk.Linear(128), jax.nn.relu\n",
    "        ])(encoder_input)\n",
    "        \n",
    "        # Latent space parameters\n",
    "        mu = hk.Linear(self.latent_dim)(h)\n",
    "        logvar = hk.Linear(self.latent_dim)(h)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = self._reparameterize(mu, logvar, is_training)\n",
    "        \n",
    "        # Concatenate latent sample and condition for decoder\n",
    "        decoder_input = jnp.concatenate([z, c], axis=-1)\n",
    "        \n",
    "        # Decoder network\n",
    "        x_reconst = hk.Sequential([\n",
    "            hk.Linear(128), jax.nn.relu,\n",
    "            hk.Linear(256), jax.nn.relu,\n",
    "            hk.Linear(self.input_dim), jax.sigmoid\n",
    "        ])(decoder_input)\n",
    "        \n",
    "        return x_reconst, mu, logvar\n",
    "\n",
    "    def _reparameterize(self, mu, logvar, is_training):\n",
    "        \"\"\"\n",
    "        Reparameterization trick\n",
    "        \n",
    "        Args:\n",
    "            mu (jnp.ndarray): Mean of latent distribution\n",
    "            logvar (jnp.ndarray): Log variance of latent distribution\n",
    "            is_training (bool): Training mode flag\n",
    "        \n",
    "        Returns:\n",
    "            z (jnp.ndarray): Sampled latent vector\n",
    "        \"\"\"\n",
    "        if not is_training:\n",
    "            return mu\n",
    "        \n",
    "        std = jnp.exp(0.5 * logvar)\n",
    "        eps = jax.random.normal(hk.next_rng_key(), mu.shape)\n",
    "        return mu + eps * std\n",
    "\n",
    "def compute_loss(params, x, c, model, rng):\n",
    "    \"\"\"\n",
    "    Compute VAE loss\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Model parameters\n",
    "        x (jnp.ndarray): Input data\n",
    "        c (jnp.ndarray): Conditional information\n",
    "        model (ConditionalVAE): VAE model\n",
    "        rng (jax.random.PRNGKey): Random number generator key\n",
    "    \n",
    "    Returns:\n",
    "        loss (jnp.ndarray): Total loss\n",
    "        aux_data (dict): Auxiliary information\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    x_reconst, mu, logvar = model(x, c, is_training=True)\n",
    "    \n",
    "    # Reconstruction loss (Binary Cross Entropy)\n",
    "    recon_loss = -jnp.sum(x * jnp.log(x_reconst + 1e-10) + \n",
    "                           (1 - x) * jnp.log(1 - x_reconst + 1e-10), axis=-1)\n",
    "    recon_loss = jnp.mean(recon_loss)\n",
    "    \n",
    "    # KL Divergence loss\n",
    "    kl_loss = -0.5 * jnp.sum(1 + logvar - mu**2 - jnp.exp(logvar), axis=-1)\n",
    "    kl_loss = jnp.mean(kl_loss)\n",
    "    \n",
    "    # Total loss (with beta for KL divergence regularization)\n",
    "    beta = 1.0\n",
    "    loss = recon_loss + beta * kl_loss\n",
    "    \n",
    "    return loss, {\n",
    "        'recon_loss': recon_loss,\n",
    "        'kl_loss': kl_loss\n",
    "    }\n",
    "\n",
    "def generate_mock_data(key, n_samples=1000, input_dim=10, condition_dim=5):\n",
    "    \"\"\"\n",
    "    Generate mock data for training\n",
    "    \n",
    "    Args:\n",
    "        key (jax.random.PRNGKey): Random number generator key\n",
    "        n_samples (int): Number of samples\n",
    "        input_dim (int): Dimension of input data\n",
    "        condition_dim (int): Dimension of conditional information\n",
    "    \n",
    "    Returns:\n",
    "        x (jnp.ndarray): Input data\n",
    "        c (jnp.ndarray): Conditional information\n",
    "    \"\"\"\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    x = jax.random.uniform(key1, (n_samples, input_dim))\n",
    "    c = jax.random.uniform(key2, (n_samples, condition_dim))\n",
    "    return x, c\n",
    "\n",
    "def create_train_step(model):\n",
    "    \"\"\"\n",
    "    Create a training step function\n",
    "    \n",
    "    Args:\n",
    "        model (ConditionalVAE): VAE model\n",
    "    \n",
    "    Returns:\n",
    "        train_step (function): Function for performing a single training step\n",
    "    \"\"\"\n",
    "    @jax.jit\n",
    "    def train_step(state, x, c, rng):\n",
    "        \"\"\"\n",
    "        Perform a single training step\n",
    "        \n",
    "        Args:\n",
    "            state (TrainState): Current training state\n",
    "            x (jnp.ndarray): Input data\n",
    "            c (jnp.ndarray): Conditional information\n",
    "            rng (jax.random.PRNGKey): Random number generator key\n",
    "        \n",
    "        Returns:\n",
    "            new_state (TrainState): Updated training state\n",
    "            loss (jnp.ndarray): Current loss value\n",
    "        \"\"\"\n",
    "        def loss_fn(params):\n",
    "            # Compute loss with current parameters\n",
    "            loss, aux = compute_loss(params, x, c, model, rng)\n",
    "            return loss, aux\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, aux), grads = grad_fn(state.params)\n",
    "        \n",
    "        # Update optimizer state and parameters\n",
    "        new_state = state.apply_gradients(grads=grads)\n",
    "        \n",
    "        return new_state, loss\n",
    "\n",
    "    return train_step\n",
    "\n",
    "def train_conditional_vae(epochs=100, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train Conditional VAE\n",
    "    \n",
    "    Args:\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        model (ConditionalVAE): Trained Conditional VAE model\n",
    "        params (dict): Trained model parameters\n",
    "    \"\"\"\n",
    "    # Set random seeds\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # Generate mock data\n",
    "    x, c = generate_mock_data(key)\n",
    "    \n",
    "    # Model and initialization\n",
    "    input_dim = x.shape[1]\n",
    "    condition_dim = c.shape[1]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    rng = hk.PRNGSequence(key)\n",
    "    sample_input = x[:batch_size]\n",
    "    sample_condition = c[:batch_size]\n",
    "    \n",
    "    # Transform the model into a pair of pure functions\n",
    "    model = hk.transform(ConditionalVAE(input_dim, condition_dim))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    params = model.init(next(rng), sample_input, sample_condition)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optax.adam(learning_rate=1e-3)\n",
    "    \n",
    "    # Create train state\n",
    "    from flax.training import train_state\n",
    "    class TrainState(train_state.TrainState):\n",
    "        pass\n",
    "    \n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer\n",
    "    )\n",
    "    \n",
    "    # Create training step function\n",
    "    train_step = create_train_step(model)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        shuffled_indices = jax.random.permutation(shuffle_key, x.shape[0])\n",
    "        x = x[shuffled_indices]\n",
    "        c = c[shuffled_indices]\n",
    "        \n",
    "        # Batch training\n",
    "        total_loss = 0\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            batch_x = x[i:i+batch_size]\n",
    "            batch_c = c[i:i+batch_size]\n",
    "            \n",
    "            key, rng_key = jax.random.split(key)\n",
    "            state, loss = train_step(state, batch_x, batch_c, rng_key)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Print average loss\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss / (x.shape[0] // batch_size):.4f}')\n",
    "    \n",
    "    return model, state.params\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate Conditional VAE\n",
    "    \"\"\"\n",
    "    # Train Conditional VAE\n",
    "    model, params = train_conditional_vae()\n",
    "    \n",
    "    print(\"Training complete. Model ready for further use.\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cairosvg\n",
    "# !apt install libcairo2\n",
    "import cairosvg\n",
    "\n",
    "# Load the SVG file\n",
    "svg_file_path = '../data/adaptation_m0_log_adaptation.svg'\n",
    "\n",
    "# Convert SVG to PNG\n",
    "png_file_path = svg_file_path.replace('svg', 'png')\n",
    "cairosvg.svg2png(url=svg_file_path, write_to=png_file_path, output_width=500, output_height=500)\n",
    "\n",
    "print(f'SVG file has been converted to PNG and saved at {png_file_path}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
