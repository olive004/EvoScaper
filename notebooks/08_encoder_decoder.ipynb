{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder & Decoder of genetic circuits\n",
    "\n",
    "In this notebook, we will construct an encoder-decoder to create a representation space that correlates with topologies or motifs in a genetic circuit.\n",
    "\n",
    "Inspired by [Generative aptamer discovery using RaptGen](https://www.nature.com/articles/s43588-022-00249-6#Fig1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1701906829.862382  168224 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[gpu(id=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from synbio_morpher.utils.data.data_format_tools.common import load_json_as_dict\n",
    "from synbio_morpher.utils.results.analytics.naming import get_true_interaction_cols\n",
    "from synbio_morpher.utils.data.data_format_tools.common import write_json\n",
    "from synbio_morpher.utils.misc.string_handling import prettify_keys_for_label\n",
    "from typing import List\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "from jaxtyping import Array, Float, Int  # https://github.com/google/jaxtyping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import r2_score  \n",
    "                \n",
    "import wandb\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "\n",
    "\n",
    "# if __package__ is None:\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "__package__ = os.path.basename(module_path)\n",
    "\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.mlp import MLP, MLP_fn\n",
    "from src.losses.losses import loss_fn, compute_accuracy_categorical, compute_accuracy_regression\n",
    "from src.utils.math import custom_round, convert_to_scientific_exponent\n",
    "from src.utils.data_preprocessing import drop_duplicates_keep_first_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../data/processed/ensemble_mutation_effect_analysis/2023_07_17_105328/tabulated_mutation_info.csv'\n",
    "fn_test_data = '../data/raw/ensemble_mutation_effect_analysis/2023_10_03_204819/tabulated_mutation_info.csv'\n",
    "data = pd.read_csv(fn)\n",
    "try:\n",
    "    data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "We will use a simple MLP for the encoder and decoder. There are several options for the head, but for now, we will use regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "N_BATCHES = 1200\n",
    "TOTAL_DS = BATCH_SIZE * N_BATCHES\n",
    "MAX_TOTAL_DS = TOTAL_DS\n",
    "TRAIN_SPLIT = 0.8\n",
    "LEARNING_RATE = 5e-4\n",
    "LEARNING_RATE_SCHED = 'cosine_decay'\n",
    "# LEARNING_RATE_SCHED = 'constant'\n",
    "WARMUP_EPOCHS = 20\n",
    "L2_REG_ALPHA = 0.01\n",
    "EPOCHS = 5000\n",
    "PRINT_EVERY = EPOCHS // 10\n",
    "SEED = 1\n",
    "INPUT_SPECIES = 'RNA_1'\n",
    "target_circ_func = 'sensitivity_wrt_species-6'\n",
    "input_concat_diffs = False\n",
    "input_concat_axis = 0\n",
    "\n",
    "# MLP Architecture\n",
    "LAYER_SIZES = [128, 64, 32, 64, 128]\n",
    "USE_CATEGORICAL = False\n",
    "USE_DROPOUT = False\n",
    "USE_L2_REG = False\n",
    "USE_WARMUP = False\n",
    "\n",
    "loss_fn = partial(\n",
    "    loss_fn, loss_type='categorical' if USE_CATEGORICAL else 'mse', use_l2_reg=USE_L2_REG)\n",
    "compute_accuracy = compute_accuracy_categorical if USE_CATEGORICAL else compute_accuracy_regression\n",
    "\n",
    "subtask = 'test_'\n",
    "save_path = 'saves_' + subtask + str(datetime.now()).split(' ')[0].replace(\n",
    "    '-', '_') + '__' + str(datetime.now()).split(' ')[-1].split('.')[0].replace(':', '_')\n",
    "save_path = os.path.join('weight_saves', '08_encoder_decoder', save_path)\n",
    "\n",
    "rng = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_convert_to_scientific_exponent = np.vectorize(\n",
    "    convert_to_scientific_exponent)\n",
    "filt = data['sample_name'] == INPUT_SPECIES\n",
    "numerical_resolution = 2\n",
    "\n",
    "# Balance the dataset\n",
    "df = drop_duplicates_keep_first_n(data[filt], get_true_interaction_cols(\n",
    "    data, 'energies', remove_symmetrical=True), n=100)\n",
    "df[target_circ_func] = df[target_circ_func].round(\n",
    "    np.abs(int(f'{df[target_circ_func].min():.0e}'.split('e')[1]))-1)\n",
    "df = drop_duplicates_keep_first_n(\n",
    "    df, column=target_circ_func, n=200)\n",
    "\n",
    "TOTAL_DS = np.min([TOTAL_DS, MAX_TOTAL_DS, len(df)])\n",
    "N_BATCHES = TOTAL_DS // BATCH_SIZE\n",
    "TOTAL_DS = N_BATCHES * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [get_true_interaction_cols(data, 'energies', remove_symmetrical=True)]\n",
    "\n",
    "x = [df[i].iloc[:TOTAL_DS].values[:, :, None] for i in x_cols]\n",
    "x = np.concatenate(x, axis=input_concat_axis+1).squeeze()\n",
    "\n",
    "y = x\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)\n",
    "\n",
    "N_HEAD = x.shape[1]\n",
    "\n",
    "\n",
    "if x.shape[0] < TOTAL_DS:\n",
    "    print(\n",
    "        f'WARNING: The filtered data is not as large as the requested total dataset size: {x.shape[0]} vs. requested {TOTAL_DS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"test_encoder_decoder\",\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"architecture\": \"MLP\",\n",
    "#         \"dataset\": \"Circuits\",\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"input_species\": INPUT_SPECIES,\n",
    "#         \"input_concat_diffs\": input_concat_diffs,\n",
    "#         \"input_concat_axis\": input_concat_axis,\n",
    "#         \"layer_sizes\": LAYER_SIZES,\n",
    "#         \"learning_rate\": LEARNING_RATE,\n",
    "#         \"learning_rate_schedule\": LEARNING_RATE_SCHED,\n",
    "#         \"n_head\": N_HEAD,\n",
    "#         \"seed\": SEED,\n",
    "#         \"target_circuit_function\": target_circ_func,\n",
    "#         \"total_dataset_size\": TOTAL_DS,\n",
    "#         \"train_split_percentage\": TRAIN_SPLIT,\n",
    "#         \"use_categorical\": USE_CATEGORICAL,\n",
    "#         \"use_dropout\": USE_DROPOUT,\n",
    "#         \"use_L2_reg\": USE_L2_REG,\n",
    "#         \"use_warmup\": USE_WARMUP,\n",
    "#         \"warmup_epochs\": WARMUP_EPOCHS\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wadh6511/Kode/env_evo/lib/python3.10/site-packages/haiku/_src/initializers.py:126: UserWarning: Explicitly requested dtype float64  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  unscaled = jax.random.truncated_normal(\n",
      "/home/wadh6511/Kode/env_evo/lib/python3.10/site-packages/haiku/_src/base.py:682: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    }
   ],
   "source": [
    "model = hk.transform(partial(MLP_fn, init_kwargs={\n",
    "                     'layer_sizes': LAYER_SIZES, 'n_head': N_HEAD, 'use_categorical': USE_CATEGORICAL}))\n",
    "\n",
    "params = model.init(rng, x[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LEARNING_RATE_SCHED == 'cosine_decay':\n",
    "    learning_rate_scheduler = optax.cosine_decay_schedule(\n",
    "        LEARNING_RATE, decay_steps=EPOCHS, alpha=L2_REG_ALPHA)\n",
    "else:\n",
    "    learning_rate_scheduler = LEARNING_RATE\n",
    "optimiser = optax.sgd(learning_rate=learning_rate_scheduler)\n",
    "\n",
    "if USE_WARMUP:\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0., end_value=LEARNING_RATE,\n",
    "        transition_steps=WARMUP_EPOCHS * N_BATCHES)\n",
    "    cosine_epochs = max(EPOCHS - WARMUP_EPOCHS, 1)\n",
    "    cosine_fn = optax.cosine_decay_schedule(\n",
    "        init_value=LEARNING_RATE,\n",
    "        decay_steps=cosine_epochs * N_BATCHES)\n",
    "    schedule_fn = optax.join_schedules(\n",
    "        schedules=[warmup_fn, cosine_fn],\n",
    "        boundaries=[WARMUP_EPOCHS * N_BATCHES])\n",
    "    optimiser = optax.sgd(learning_rate=schedule_fn)\n",
    "\n",
    "optimiser_state = optimiser.init(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i_batch, xy, Batches, *content]\n",
    "        \n",
    "x = x.reshape(N_BATCHES, 1, BATCH_SIZE, x.shape[-1])\n",
    "y = y.reshape(N_BATCHES, 1, BATCH_SIZE, y.shape[-1])\n",
    "\n",
    "x_train, y_train = x[:int(TRAIN_SPLIT * N_BATCHES)], y[:int(TRAIN_SPLIT * N_BATCHES)]\n",
    "x_val, y_val = x[int(TRAIN_SPLIT * N_BATCHES):], y[int(TRAIN_SPLIT * N_BATCHES):]\n",
    "xy_train = np.concatenate([x_train, y_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(params, x, y, optimiser_state, model, rng, l2_reg_alpha, optimiser):\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(\n",
    "        params, rng, model, x, y, l2_reg_alpha=l2_reg_alpha)\n",
    "\n",
    "    updates, optimiser_state = optimiser.update(grads, optimiser_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, optimiser_state, loss, grads\n",
    "\n",
    "\n",
    "def eval_step(params, rng, model: MLP, x, y, l2_reg_alpha):\n",
    "    \"\"\" Return the average of loss and accuracy on validation data \"\"\"\n",
    "    loss = loss_fn(params, rng, model, x, y, l2_reg_alpha=l2_reg_alpha)\n",
    "    acc = compute_accuracy(params, rng, model, x, y)\n",
    "    return acc, loss\n",
    "\n",
    "\n",
    "def run_batches(params, model, xy_train, rng, l2_reg_alpha, optimiser, optimiser_state):\n",
    "\n",
    "    f_train_step = partial(train_step, model=model, rng=rng,\n",
    "                           l2_reg_alpha=l2_reg_alpha, optimiser=optimiser)\n",
    "\n",
    "    def f(carry, inp):\n",
    "\n",
    "        params, optimiser_state = carry[0], carry[1]\n",
    "        x_batch, y_batch = inp[0], inp[1]\n",
    "\n",
    "        params, optimiser_state, loss, grads = f_train_step(\n",
    "            params, x_batch, y_batch, optimiser_state)\n",
    "        return (params, optimiser_state), (loss, grads)\n",
    "\n",
    "    # for x_batch, y_batch in xy_train:\n",
    "    (params, optimiser_state), (train_loss, grads) = jax.lax.scan(\n",
    "        f, (params, optimiser_state), xy_train)\n",
    "    return params, optimiser_state, train_loss, grads\n",
    "\n",
    "\n",
    "# def run_batches(params, model, xy_train, rng, l2_reg_alpha, optimiser, optimiser_state):\n",
    "\n",
    "#     f_train_step = partial(train_step, model=model, rng=rng,\n",
    "#                            l2_reg_alpha=l2_reg_alpha, optimiser=optimiser)\n",
    "\n",
    "#     for x_batch, y_batch in xy_train:\n",
    "\n",
    "#         if len(x_batch) and len(y_batch):\n",
    "#             params, optimiser_state, train_loss, grads = f_train_step(\n",
    "#                 params, x_batch, y_batch, optimiser_state)\n",
    "\n",
    "#     return params, optimiser_state, train_loss, grads\n",
    "\n",
    "\n",
    "def train(params, rng, model, xy_train, x_val, y_val,\n",
    "          optimiser, optimiser_state,\n",
    "          l2_reg_alpha, epochs,\n",
    "          save_every: int = 50):\n",
    "\n",
    "    def f(carry, _):\n",
    "        params, optimiser_state = carry[0], carry[1]\n",
    "\n",
    "        params, optimiser_state, train_loss, grads = run_batches(\n",
    "            params, model, xy_train, rng, l2_reg_alpha, optimiser, optimiser_state)\n",
    "\n",
    "        val_acc, val_loss = eval_step(\n",
    "            params, rng, model, x_val, y_val, l2_reg_alpha)\n",
    "\n",
    "        return (params, optimiser_state), (params, grads, train_loss, val_loss, val_acc)\n",
    "\n",
    "    try:\n",
    "        (params, optimiser_state), (params_stack, grads, train_loss, val_loss, val_acc) = jax.lax.scan(\n",
    "            f, init=(params, optimiser_state), xs=None, length=epochs)\n",
    "        saves = {\n",
    "            'params': params_stack,\n",
    "            'grads': grads,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc\n",
    "        }\n",
    "    except:\n",
    "        saves = {}\n",
    "        for e in range(epochs):\n",
    "            (params, optimiser_state), (params_stack, grads, train_loss,\n",
    "                                        val_loss, val_acc) = f((params, optimiser_state), None)\n",
    "            \n",
    "            if np.mod(e, save_every) == 0:\n",
    "                saves[e] = {\n",
    "                    'params': params_stack,\n",
    "                    'grads': grads,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_acc\n",
    "                }\n",
    "                print(\n",
    "                    f'Epoch {e} / {epochs} -\\t\\t Train loss: {np.mean(train_loss)}\\tVal loss: {val_loss}\\tVal accuracy: {val_acc}')\n",
    "\n",
    "    return params, saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1701907428.046993  168224 hlo_rematerialization.cc:2946] Can't reduce memory use below -281.32GiB (-302068888450 bytes) by rematerialization; only reduced to 305.63GiB (328171781772 bytes), down from 305.63GiB (328171781772 bytes) originally\n",
      "2023-12-07 00:03:58.450269: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 111.39GiB (rounded to 119603200000)requested by op \n",
      "2023-12-07 00:03:58.450710: W external/tsl/tsl/framework/bfc_allocator.cc:497] ***********************************************************************_____________________________\n",
      "2023-12-07 00:03:58.451297: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2644] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 119603200000 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:    5.43MiB\n",
      "              constant allocation:        36B\n",
      "        maybe_live_out allocation:  305.53GiB\n",
      "     preallocated temp allocation:  108.24MiB\n",
      "                 total allocation:  305.64GiB\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 111.39GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,64,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 111.39GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,128,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 27.85GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,32,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 27.85GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,64,32]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 10.44GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,128,6]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 10.44GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,6,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 1.74GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70 deduplicated_name=\"fusion.62\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 1.74GiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70 deduplicated_name=\"fusion.62\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 891.11MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70 deduplicated_name=\"fusion.64\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 891.11MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70 deduplicated_name=\"fusion.64\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 445.56MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,32]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 156.25MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,64,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 156.25MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,128,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 83.54MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,730,6]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 39.06MiB\n",
      "\t\tOperator: op_name=\"jit(scan)/jit(main)/while/body/dynamic_update_slice\" source_file=\"/tmp/ipykernel_168224/3825306338.py\" source_line=70\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[5000,64,32]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5000 -\t\t Train loss: 14.57142448425293\tVal loss: 14.00648021697998\tVal accuracy: 0.0029599270783364773\n",
      "Epoch 500 / 5000 -\t\t Train loss: 7.585797309875488\tVal loss: 7.6465840339660645\tVal accuracy: 0.01891222596168518\n",
      "Epoch 1000 / 5000 -\t\t Train loss: 4.285400390625\tVal loss: 4.306591510772705\tVal accuracy: 0.04694615304470062\n",
      "Epoch 1500 / 5000 -\t\t Train loss: 2.805971145629883\tVal loss: 2.801469564437866\tVal accuracy: 0.10410262644290924\n",
      "Epoch 2000 / 5000 -\t\t Train loss: 2.1839683055877686\tVal loss: 2.1748478412628174\tVal accuracy: 0.10983037203550339\n",
      "Epoch 2500 / 5000 -\t\t Train loss: 1.928215503692627\tVal loss: 1.9192010164260864\tVal accuracy: 0.11695269495248795\n",
      "Epoch 3000 / 5000 -\t\t Train loss: 1.700813889503479\tVal loss: 1.6921613216400146\tVal accuracy: 0.13616375625133514\n",
      "Epoch 3500 / 5000 -\t\t Train loss: 1.4203041791915894\tVal loss: 1.4125937223434448\tVal accuracy: 0.1542506217956543\n",
      "Epoch 4000 / 5000 -\t\t Train loss: 1.0447938442230225\tVal loss: 1.0384917259216309\tVal accuracy: 0.21945440769195557\n",
      "Epoch 4500 / 5000 -\t\t Train loss: 0.716334879398346\tVal loss: 0.7123602628707886\tVal accuracy: 0.23828125\n"
     ]
    }
   ],
   "source": [
    "params, saves = train(params, rng, model, xy_train, x_val, y_val, optimiser, optimiser_state,\n",
    "                      l2_reg_alpha=L2_REG_ALPHA, epochs=EPOCHS,\n",
    "                      save_every=PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(saves, out_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m7\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mlist\u001b[39m(saves\u001b[39m.\u001b[39mkeys()), [v[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m saves\u001b[39m.\u001b[39mvalues()])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m7\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mlist\u001b[39m(saves\u001b[39m.\u001b[39mkeys()), [v[\u001b[39m'\u001b[39;49m\u001b[39mtrain_loss\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m saves\u001b[39m.\u001b[39mvalues()])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2277616468363531312d50432d425832333638312d5453227d/home/wadh6511/Kode/EvoScaper/notebooks/08_encoder_decoder.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7*3, 6))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.plot(list(saves.keys()), [v['train_loss'] for v in saves.values()])\n",
    "plt.ylabel('train_loss')\n",
    "plt.xlabel('step')\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.plot(list(saves.keys()), [v['val_loss'] for v in saves.values()])\n",
    "plt.ylabel('val_loss')\n",
    "plt.xlabel('step')\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.plot(list(saves.keys()), [v['val_accuracy'] for v in saves.values()])\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.xlabel('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
