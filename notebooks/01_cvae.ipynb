{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE for genetic circuits\n",
    "\n",
    "This notebook follows the previous VAE notebook very closely, but implementing a conditional VAE instead. Loosely following [this blog post](https://agustinus.kristia.de/techblog/2016/12/17/conditional-vae/) with the associated [github](https://github.com/wiseodd/generative-models/blob/master/VAE/conditional_vae/cvae_pytorch.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0), cuda(id=1)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from synbio_morpher.utils.data.data_format_tools.common import load_json_as_dict\n",
    "from synbio_morpher.utils.results.analytics.naming import get_true_interaction_cols\n",
    "from synbio_morpher.utils.data.data_format_tools.common import write_json\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "USE_ONLY_ONE_GPU = False\n",
    "if USE_ONLY_ONE_GPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 0 or 1\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "\n",
    "\n",
    "# if __package__ is None:\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "__package__ = os.path.basename(module_path)\n",
    "\n",
    "PRNG = jax.random.PRNGKey(0)\n",
    "\n",
    "jax.devices()\n",
    "\n",
    "# jupyter nbconvert --to notebook --execute 01_cvae.ipynb --output=01_cvae_2.ipynb --ExecutePreprocessor.timeout=-1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoscaper.scripts.init_from_hpos import init_from_hpos, make_loss\n",
    "from evoscaper.utils.math import bin_to_nearest_edge, arrayise\n",
    "from evoscaper.utils.preprocess import make_datetime_str\n",
    "from evoscaper.utils.optimiser import make_optimiser\n",
    "from evoscaper.utils.train import train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evoscaper.scripts.simulate_circuits import summarise_simulated_cicruits, save\n",
    "# from synbio_morpher.utils.data.data_format_tools.common import load_json_as_dict\n",
    "\n",
    "# analytics = load_json_as_dict('data/simulate_circuits/2025_01_29__18_12_38/analytics.json')\n",
    "# config = load_json_as_dict('data/simulate_circuits/2025_01_29__18_12_38/config.json')\n",
    "# interactions = np.load('data/simulate_circuits/2025_01_29__18_12_38/interactions.npy')\n",
    "# data = summarise_simulated_cicruits(analytics, config, interactions)\n",
    "# save({'tabulated_mutation_info.json': data}, 'data/simulate_circuits/2025_01_29__18_12_38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = '../data/raw/summarise_simulation/2024_11_21_144918/tabulated_mutation_info.csv'\n",
    "# fn = '../data/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv'\n",
    "# fn = '../data/raw/summarise_simulation/2024_12_05_210221/tabulated_mutation_info.csv'\n",
    "# fn = './data/simulate_circuits/2025_01_29__18_12_38/tabulated_mutation_info.json'\n",
    "# fn_test_data = '../data/raw/summarise_simulation/2023_07_17_222747/tabulated_mutation_info.csv'\n",
    "# data = pd.concat([pd.read_csv(fn), pd.read_csv(fn_test_data)])\n",
    "# data = pd.read_json(fn)\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'dssim0128'\n",
    "t_str = make_datetime_str()\n",
    "top_write_dir = os.path.join('data', '01_cvae', t_str)\n",
    "save_path = os.path.join(top_write_dir, 'saves_' + t_str + f'_{task}')\n",
    "os.makedirs(top_write_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "\n",
    "hpos_architecture = {\n",
    "    'seed_arch': 1,\n",
    "    'hidden_size': 16,\n",
    "    'enc_ls': 32,\n",
    "    'dec_ls': 32,\n",
    "    'num_enc_layers': 2,\n",
    "    'num_dec_layers': 2,\n",
    "    'factor_expanding_ls': 1,\n",
    "    'factor_contracting_ls': 1,\n",
    "    'model': 'CVAE',\n",
    "    'use_sigmoid_decoder': False,\n",
    "    'enc_init': 'HeNormal',\n",
    "    'dec_init': 'HeNormal',\n",
    "    'init_model_with_random': True,\n",
    "    'activation': 'leaky_relu',\n",
    "}\n",
    "\n",
    "hpos_training = {\n",
    "    'seed_train': 1,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 1000,\n",
    "    'patience': 500,\n",
    "    'threshold_early_val_acc': 0.95,\n",
    "    'learning_rate': 5e-3,\n",
    "    'loss_func': 'mse',\n",
    "    'accuracy_func': 'accuracy_regression',\n",
    "    'use_dropout': False,\n",
    "    'dropout_rate': 0.1,\n",
    "    'use_l2_reg': False,\n",
    "    'l2_reg_alpha': 5e-2,\n",
    "    'use_kl_div': True,\n",
    "    # inspired by https://github.com/elttaes/VAE-MNIST-Haiku-Jax/blob/main/cVAE_mnist.ipynb\n",
    "    'kl_weight': 2.5e-4,\n",
    "    'use_grad_clipping': False,\n",
    "    'use_contrastive_loss': False,\n",
    "    'temperature': 1.5,\n",
    "    'contrastive_func': 'info_nce',\n",
    "    'threshold_similarity': 0.9,\n",
    "    'power_factor_distance': 3\n",
    "}\n",
    "hpos_training['print_every'] = hpos_training['epochs'] // 50\n",
    "\n",
    "hpos_optimization = {\n",
    "    'seed_opt': 1,\n",
    "    'opt_method': 'adam',\n",
    "    'opt_min_lr': 1e-6,\n",
    "    'opt_min_delta': 1e-4,\n",
    "    'learning_rate_sched': 'cosine_decay',\n",
    "    'use_warmup': True,\n",
    "    'warmup_epochs': 20,\n",
    "}\n",
    "\n",
    "hpos_dataset = {\n",
    "    'seed_dataset': 1,\n",
    "    'include_diffs': False,\n",
    "    # 'objective_col': ('Log sensitivity', 'Log precision'),\n",
    "    # 'objective_col': 'adaptation',\n",
    "    'objective_col': 'Log sensitivity',\n",
    "    'output_species': ('RNA_2',),\n",
    "    'signal_species': ('RNA_0',),\n",
    "    # 'filenames_train_config': f'{data_dir}/raw/summarise_simulation/2024_11_21_160955/ensemble_config.json',\n",
    "    # 'filenames_train_table': f'{data_dir}/raw/summarise_simulation/2024_11_21_160955/tabulated_mutation_info.csv',\n",
    "    # 'filenames_train_config': f'{data_dir}/raw/summarise_simulation/2024_12_05_210221/ensemble_config.json',\n",
    "    # 'filenames_train_table': f'{data_dir}/raw/summarise_simulation/2024_12_05_210221/tabulated_mutation_info.csv',\n",
    "    # 'filenames_verify_config': f'./data/simulate_circuits/2025_02_01__00_22_38/config.json',\n",
    "    # 'filenames_verify_table': f'./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json',\n",
    "    # 'filenames_train_config': f'./data/simulate_circuits/2025_01_29__18_12_38/config.json',\n",
    "    # 'filenames_train_table': f'./data/simulate_circuits/2025_01_29__18_12_38/tabulated_mutation_info.json',\n",
    "    # 'filenames_verify_config': f'./data/simulate_circuits/2025_02_01__00_22_38/config.json',\n",
    "    # 'filenames_verify_table': f'./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json',\n",
    "    'filenames_train_config': f'./data/simulate_circuits/2025_01_29__14_52_44/config.json',\n",
    "    'filenames_train_table': f'./data/simulate_circuits/2025_01_29__14_52_44/tabulated_mutation_info.json',\n",
    "    'filenames_verify_config': f'./data/simulate_circuits/2025_02_01__00_22_38/config.json',\n",
    "    'filenames_verify_table': f'./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json',\n",
    "    'use_test_data': False,\n",
    "    # 'total_ds': None,   # TO BE RECORDED\n",
    "    'total_ds_max': 5e6,\n",
    "    'train_split': 0.8,\n",
    "    'x_type': 'energies',\n",
    "    # XY filtering:\n",
    "    'filt_x_nans': True,\n",
    "    'filt_y_nans': True,\n",
    "    'filt_sensitivity_nans': True,\n",
    "    'filt_precision_nans': True,\n",
    "    'filt_n_same_x_max': 1,\n",
    "    'filt_n_same_x_max_bins': None,\n",
    "    'filt_response_time_high': True,\n",
    "    'filt_response_time_perc_max': 0.8,\n",
    "    # XY preprocessing:\n",
    "    'prep_x_standardise': False,\n",
    "    'prep_y_standardise': False,\n",
    "    'prep_x_min_max': True,\n",
    "    'prep_y_min_max': True,\n",
    "    'prep_x_robust_scaling': True,\n",
    "    'prep_y_robust_scaling': True,\n",
    "    'prep_x_logscale': False,\n",
    "    'prep_y_logscale': False,\n",
    "    'prep_x_categorical': False,\n",
    "    'prep_y_categorical': False,\n",
    "    'prep_x_categorical_onehot': False,\n",
    "    'prep_y_categorical_onehot': False,\n",
    "    'prep_x_categorical_n_bins': 10,\n",
    "    'prep_y_categorical_n_bins': 10,\n",
    "    'prep_x_categorical_method': 'quantile',\n",
    "    'prep_y_categorical_method': 'quantile',\n",
    "    'prep_x_negative': True,\n",
    "    'prep_y_negative': False\n",
    "}\n",
    "\n",
    "\n",
    "hpos_eval = {\n",
    "    'eval_n_to_sample': 1e3\n",
    "}\n",
    "\n",
    "info_to_be_recorded = {\n",
    "    'filename_saved_model': 'TO_BE_RECORDED',\n",
    "    'total_ds': 'TO_BE_RECORDED',\n",
    "    'n_batches': 'TO_BE_RECORDED',\n",
    "    'R2_train': 'TO_BE_RECORDED',\n",
    "    'R2_test': 'TO_BE_RECORDED',\n",
    "    'mutual_information_conditionality': 'TO_BE_RECORDED',\n",
    "    'n_layers_enc': 'TO_BE_RECORDED',\n",
    "    'n_layers_dec': 'TO_BE_RECORDED',\n",
    "    'run_successful': 'TO_BE_RECORDED',\n",
    "    'info_early_stop': 'TO_BE_RECORDED',\n",
    "    'error_msg': 'TO_BE_RECORDED',\n",
    "}\n",
    "\n",
    "hpos_all = {}\n",
    "for d in [hpos_architecture, hpos_training, hpos_optimization, hpos_dataset, hpos_eval, info_to_be_recorded]:\n",
    "    hpos_all.update(d)\n",
    "\n",
    "with open(os.path.join(top_write_dir, 'hpos_all.json'), 'w') as f:\n",
    "    json.dump(hpos_all, f, indent=4)\n",
    "hpos = pd.Series(hpos_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    rng, rng_model, rng_dataset,\n",
    "    config_norm_x, config_norm_y, config_filter, config_optimisation, config_dataset, config_training, config_model,\n",
    "    data, x_cols, df,\n",
    "    x, cond, y, x_train, cond_train, y_train, x_val, cond_val, y_val,\n",
    "    total_ds, n_batches, BATCH_SIZE, x_datanormaliser, x_methods_preprocessing, y_datanormaliser, y_methods_preprocessing,\n",
    "    params, encoder, decoder, model, h2mu, h2logvar, reparam\n",
    ") = init_from_hpos(hpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3228904/3651033320.py:11: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  sns.histplot(x=df[k], y=df[x_cols].min(axis=1), palette='viridis')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Data after transformation')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(4*7, 5))\n",
    "ax = plt.subplot(1, 4, 1)\n",
    "for k in config_dataset.objective_col:\n",
    "    sns.histplot(x=df[k], y=df[x_cols].sum(axis=1))\n",
    "if config_norm_x.logscale:\n",
    "    plt.xscale('log')\n",
    "plt.ylabel('Energies sum')\n",
    "plt.title('Energies sum')\n",
    "ax = plt.subplot(1, 4, 2)\n",
    "for k in config_dataset.objective_col:\n",
    "    sns.histplot(x=df[k], y=df[x_cols].min(axis=1), palette='viridis')\n",
    "    # hue=df[x_cols].mean(axis=1))\n",
    "if config_norm_x.logscale:\n",
    "    plt.xscale('log')\n",
    "plt.ylabel('Energies min')\n",
    "plt.title('Energies min')\n",
    "ax = plt.subplot(1, 4, 3)\n",
    "for k in config_dataset.objective_col:\n",
    "    sns.histplot(x=df[k], y=df[x_cols].mean(axis=1))\n",
    "if config_norm_x.logscale:\n",
    "    plt.xscale('log')\n",
    "plt.ylabel('Energies mean')\n",
    "plt.title('Energies mean')\n",
    "\n",
    "plt.suptitle('Data after transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce circuits that are overrepresented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sensitivity after balancing')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5*4, 4))\n",
    "ax = plt.subplot(1,4,1)\n",
    "sns.histplot(data[x_cols].sum(axis=1), bins=50, log_scale=[False, False], element='step')\n",
    "plt.title('Sum of energies per circuit before balancing')\n",
    "# ylim = ax.get_ylim()\n",
    "ax = plt.subplot(1,4,2)\n",
    "sns.histplot(df[x_cols].sum(axis=1), bins=50, log_scale=[False, False], element='step')\n",
    "plt.title('Sum of energies per circuit after balancing')\n",
    "# plt.ylim(ylim)\n",
    "ax = plt.subplot(1,4,3)\n",
    "sns.histplot(data, x='adaptation', bins=50, log_scale=[False, False], element='step')\n",
    "plt.title('Adaptability before balancing')\n",
    "ax = plt.subplot(1,4,4)\n",
    "sns.histplot(df, x='sensitivity_wrt_species-6', bins=50, log_scale=[True, False], element='step')\n",
    "plt.title('Sensitivity after balancing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Energies of species in a peak sensitivity range')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "\n",
    "sns.histplot(df[df[x_cols] != 0][df['Log sensitivity'].between(*df['Log sensitivity'].mode().iloc[0] + np.array([-0.1, 0.1]))][x_cols[:3]], bins=100, element='step',\n",
    "             ax=axs[0], log_scale=[False, False])\n",
    "axs[0].set_title(x_cols[:3])\n",
    "sns.histplot(df[df[x_cols] != 0][df['Log sensitivity'].between(*df['Log sensitivity'].mode().iloc[0] + np.array([-0.1, 0.1]))][x_cols[3:]], bins=100, element='step',\n",
    "             ax=axs[1], log_scale=[False, False])\n",
    "axs[1].set_title(x_cols[3:])\n",
    "plt.suptitle('Energies of species in a peak sensitivity range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the number of null circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, \"Balance of dataset for conditioning variable ['Log sensitivity']\")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance by bins\n",
    "nbin = 50\n",
    "bin_max = int(np.power(10, 4))\n",
    "# df = balance_dataset(df, cols=config_dataset.objective_col, nbin=nbin, bin_max=bin_max, use_log=False)\n",
    "# df = balance_dataset(data, cols=X_COLS, nbin=300, bin_max=bin_max, use_log=False, func1=lambda x: np.sum(x, axis=1))\n",
    "\n",
    "fig = plt.figure(figsize=(4*7, 5))\n",
    "ax = plt.subplot(1,4,1)\n",
    "sns.histplot(data[config_dataset.objective_col].to_numpy().flatten(), bins=50, log_scale=[False, True], element='step', fill=False)\n",
    "plt.title('Original: # Bins =' + str(50))\n",
    "ax = plt.subplot(1,4,2)\n",
    "sns.histplot(df[config_dataset.objective_col].to_numpy().flatten(), bins=nbin, log_scale=[False, True], element='step', fill=False)\n",
    "plt.title('# Bins =' + str(nbin))\n",
    "ax = plt.subplot(1,4,3)\n",
    "sns.histplot(df[config_dataset.objective_col].to_numpy().flatten(), bins=int(nbin*1.5), log_scale=[False, True], element='step', fill=False)\n",
    "plt.title('# Bins =' + str(int(nbin*1.5)))\n",
    "ax = plt.subplot(1,4,4)\n",
    "sns.histplot(df[config_dataset.objective_col].to_numpy().flatten(), bins=nbin*4, log_scale=[False, True], element='step', fill=False)\n",
    "plt.title('# Bins =' + str(nbin*4))\n",
    "\n",
    "plt.suptitle('Balance of dataset for conditioning variable ' + str(config_dataset.objective_col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Dataset after balancing')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(2*8, 6))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "sns.histplot(cond.flatten(), log_scale=[False, False], bins=100, element='step', fill=True)\n",
    "plt.title(f'Conditioning var: {config_dataset.objective_col}')\n",
    "plt.xlabel(f'Transformed {config_dataset.objective_col}')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "sns.histplot(x.reshape(np.prod(x.shape[:-1]), -1), element='step', fill=False, bins=200, log_scale=[False, True])\n",
    "plt.title(f'Input: {config_dataset.x_type}')\n",
    "plt.xlabel(f'Transformed {config_dataset.x_type}')\n",
    "\n",
    "plt.suptitle('Dataset after balancing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = make_optimiser(config_optimisation.learning_rate_sched, config_training.learning_rate,\n",
    "                           config_training.epochs, config_training.l2_reg_alpha, config_optimisation.use_warmup,\n",
    "                           config_optimisation.warmup_epochs, n_batches, config_optimisation.opt_method)\n",
    "optimiser_state = optimiser.init(params)\n",
    "\n",
    "# Losses\n",
    "loss_fn, compute_accuracy = make_loss(\n",
    "    config_training.loss_func, config_training.use_l2_reg, config_training.use_kl_div, config_training.kl_weight)\n",
    "\n",
    "dummy_x = jax.random.normal(PRNG, x.shape)\n",
    "dummy_cond = jax.random.normal(PRNG, cond.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = encoder(params, PRNG, np.concatenate([x, cond], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 256, 6)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(params, PRNG, x, cond, return_all=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Weight init with dummy xy using HeNormal and activation leaky_relu')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(3*6, 4))\n",
    "i_model = model(params, PRNG, dummy_x, dummy_cond)\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(i_model.flatten(), bins=100, log_scale=[True, False], element='step', fill=False)\n",
    "plt.title('Initial output of model (log)')\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(i_model.reshape(np.prod(i_model.shape[:-1]), -1), bins=100, log_scale=[False, False], element='step', fill=False)\n",
    "# sns.histplot(model(params, PRNG, x, cond), bins=100, log_scale=[True, False], element='step', fill=False)\n",
    "plt.title('Initial output of model')\n",
    "plt.subplot(1,3,3)\n",
    "i_enc = encoder(params, PRNG, np.concatenate([dummy_x, dummy_cond], axis=-1))\n",
    "sns.histplot(i_enc.reshape(np.prod(i_enc.shape[:-1]), -1), bins=100, log_scale=[False, True], element='step', fill=False, legend=False)\n",
    "plt.title('Initial embedding of model')\n",
    "plt.suptitle(f'Weight init with dummy xy using {config_model.dec_init} and activation {config_model.activation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Weight init with actual xy using HeNormal and activation leaky_relu')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(3*6, 4))\n",
    "plt.subplot(1,3,1)\n",
    "i_model = model(params, PRNG, x, cond)\n",
    "sns.histplot(i_model.flatten(), bins=100, log_scale=[True, False], element='step', fill=False)\n",
    "plt.title('Initial output of model (log)')\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(i_model.reshape(np.prod(i_model.shape[:-1]), -1), bins=100, log_scale=[False, False], element='step', fill=False)\n",
    "# sns.histplot(model(params, PRNG, x, cond), bins=100, log_scale=[True, False], element='step', fill=False)\n",
    "plt.title('Initial output of model')\n",
    "plt.subplot(1,3,3)\n",
    "i_enc = encoder(params, PRNG, np.concatenate([x, cond], axis=-1))\n",
    "sns.histplot(i_enc.reshape(np.prod(i_enc.shape[:-1]), -1), bins=100, log_scale=[False, True], element='step', fill=False, legend=False)\n",
    "plt.title('Initial embedding of model')\n",
    "plt.suptitle(f'Weight init with actual xy using {config_model.dec_init} and activation {config_model.activation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer encoder/~create_layers/linear_0:\n",
      "  Weight mean: 0.019077366217970848\n",
      "  Weight std: 0.5224789977073669\n",
      "Layer encoder/~create_layers/linear_1:\n",
      "  Weight mean: 0.0010722652077674866\n",
      "  Weight std: 0.25021567940711975\n",
      "Layer encoder/~create_layers/linear_2:\n",
      "  Weight mean: -0.009146679192781448\n",
      "  Weight std: 0.26599517464637756\n",
      "Layer cvae/~/h2mu:\n",
      "  Weight mean: 0.0059508285485208035\n",
      "  Weight std: 0.22761370241641998\n",
      "Layer cvae/~/h2logvar:\n",
      "  Weight mean: -0.00723856408149004\n",
      "  Weight std: 0.22661632299423218\n",
      "Layer decoder/~create_layers/linear_0:\n",
      "  Weight mean: -0.023605331778526306\n",
      "  Weight std: 0.3303181529045105\n",
      "Layer decoder/~create_layers/linear_1:\n",
      "  Weight mean: 0.01603890210390091\n",
      "  Weight std: 0.3654947280883789\n",
      "Layer decoder/~create_layers/linear_2:\n",
      "  Weight mean: 0.000654762436170131\n",
      "  Weight std: 0.23727074265480042\n",
      "Layer decoder/~create_layers/linear_3:\n",
      "  Weight mean: -0.028386231511831284\n",
      "  Weight std: 0.26381611824035645\n"
     ]
    }
   ],
   "source": [
    "# Potential diagnostic code\n",
    "def print_layer_stats(model):\n",
    "    for name, module in model.items():\n",
    "        if 'w' in module:\n",
    "            print(f\"Layer {name}:\")\n",
    "            print(f\"  Weight mean: {module['w'].mean().item()}\")\n",
    "            print(f\"  Weight std: {module['w'].std().item()}\")\n",
    "\n",
    "print_layer_stats(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Early stopping triggered after 11 epochs:\n",
      "Train loss: 0.003585029160603881\n",
      "Val loss: 0.0035434989258646965\n",
      "Val accuracy: 0.9954329133033752\n",
      "Epochs no improvement: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:12.537960\n"
     ]
    }
   ],
   "source": [
    "tstart = datetime.now()\n",
    "params, saves, info_early_stop = train(params, rng, model,\n",
    "                                       x_train, cond_train, y_train, x_val, cond_val, y_val,\n",
    "                                       optimiser, optimiser_state, config_training,\n",
    "                                       epochs=config_training.epochs, loss_fn=loss_fn, compute_accuracy=compute_accuracy,\n",
    "                                       save_every=config_training.print_every, include_params_in_all_saves=False,\n",
    "                                       patience=config_training.patience)\n",
    "print(datetime.now() - tstart)\n",
    "\n",
    "pred_y = model(params, rng, x_train, cond=cond_train)\n",
    "r2_train = r2_score(y_train.flatten(), pred_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9749464392662048 -23.02585\n"
     ]
    }
   ],
   "source": [
    "from evoscaper.scripts.cvae_scan import test\n",
    "\n",
    "\n",
    "if True:  # config_dataset.use_test_data:\n",
    "    data_test = pd.read_csv(config_dataset.filenames_verify_table) if config_dataset.filenames_verify_table.endswith('.csv') else pd.read_json(config_dataset.filenames_verify_table)\n",
    "\n",
    "    r2_test, mi = test(model, params, rng, decoder, saves, data_test,\n",
    "                        config_dataset, config_norm_y, config_model,\n",
    "                        x_cols, config_filter, top_write_dir,\n",
    "                        x_datanormaliser, x_methods_preprocessing,\n",
    "                        y_datanormaliser, y_methods_preprocessing, visualise=False)\n",
    "    print(r2_test, mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(272640), np.int64(68352))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(x_train.shape[:2]), np.prod(x_val.shape[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Preprocessing of conditional data')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(np.concatenate([df[i].iloc[:total_ds].values[:, None] for i in x_cols], axis=1).squeeze().flatten(), bins=50, log_scale=[False, True], element='step', fill=False, label='Original')\n",
    "sns.histplot(x_datanormaliser.create_chain_preprocessor_inverse(x_methods_preprocessing)(x).flatten(), bins=50, log_scale=[False, True], element='step', fill=False, label='Preprocessed and then inverted')\n",
    "plt.legend()\n",
    "plt.title('Preprocessing of input data')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(df[config_dataset.objective_col].iloc[:total_ds].values.flatten(), bins=50, log_scale=[False, True], element='step', fill=False, label='Original')\n",
    "sns.histplot(np.concatenate([y_datanormaliser.create_chain_preprocessor_inverse(y_methods_preprocessing)(cond, col=c).flatten() for c in config_dataset.objective_col]), bins=50, log_scale=[False, True], element='step', fill=False, label='Preprocessed and then inverted')\n",
    "plt.legend()\n",
    "plt.title('Preprocessing of conditional data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Training')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(7*3, 6))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.plot(list(saves.keys()), [v['train_loss'] for v in saves.values()])\n",
    "plt.ylabel('train_loss')\n",
    "plt.xlabel('step')\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.plot(list(saves.keys()), [v['val_loss'] for v in saves.values()])\n",
    "plt.ylabel('val_loss')\n",
    "plt.xlabel('step')\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.plot(list(saves.keys()), [v['val_accuracy'] for v in saves.values()])\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.xlabel('step')\n",
    "\n",
    "plt.suptitle('Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(saves, out_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/01_cvae/2025_02_07__22_15_21/saves_2025_02_07__22_15_21_dssim0128\n"
     ]
    }
   ],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 256, 6)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_11_27__15_01_35_saves_test')\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_11_27__16_58_36_saves_test')\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_11_28__14_12_59_saves_test')\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_11_29__10_43_59_saves_test')\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_02__13_49_00_saves_test')  # sensitivity\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_06__14_54_23_saves_test')  # generated from interactions\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_06__17_57_39_saves_test')  # generated from interactions with sigmoid in decoder for 500 epochs\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__17_45_52_saves_no_sigmoid_decoder')  # generated from interactions with sigmoid in decoder for 2000 epochs\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__18_59_40_saves_KL_div')  # generated from interactions + relu + sigmoid + KL divergence\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__20_55_42_saves_no_KL')  # generated from interactions + relu + sigmoid\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__21_21_04_saves_no_KL')  # generated from interactions + relu \n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__22_17_13_saves_leaky_relu')  # generated from interactions + leaky relu \n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__22_48_33_saves_KL_leakyrelu')  # generated from interactions + leaky relu + KL\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_08__23_01_53_saves_KL_leakyrelu')  # generated from interactions + leaky relu + KL + encoder head hidden size\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__14_24_25_saves_new_MLP')  # generated from interactions + new MLP\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__15_28_08_saves_layers_1')  # generated from interactions + only 1 layer for enc and dec\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__15_51_40_saves_he_init')  # generated from interactions + only 1 layer for enc and dec\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__16_13_34_saves_he_init_KL_ls1')  # generated from interactions + only 1 layer for enc and dec + KL\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__16_49_29_saves_robust_KL_e2000')  # generated from interactions + KL + robust scaling + 2000 epochs\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__22_09_41_saves_nosigmoid_KL_new_reparam')  # generated from interactions + KL + no sigmoid + new reparameterisation function + sensitivity ---> DID NOT WORK, NANS\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_09__23_27_37_saves_sens_brd_KL_weight')  # generated from interactions + KL + no sigmoid + new reparameterisation function + sensitivity ---> DID NOT WORK, NANS\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__13_36_05_saves_sens_brd_KL_weight')  # generated from interactions + no sigmoid + new reparameterisation function + sensitivity 500 epochs\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__14_31_37_saves_sens_en_lr1e3')  # generated from interactions + log sensitivity 3000 epochs + lr 1e-3\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__14_45_23_saves_sens_en_lr1e3')  # generated from interactions + log sensitivity 3000 epochs + lr 1e-3 + KL + 3 layers instead of 1\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__17_00_32_saves_onehot')  # generated from interactions + log sensitivity + energies + onehot\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__17_46_49_saves_onehot_lr1e1')  # generated from interactions + log sensitivity + energies + onehot + lr 1e-1\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__17_53_31_saves_onehot_nominmax')  # generated from interactions + log sensitivity + energies + onehot + lr 1e-1 + no minmax just robust scaling\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__18_20_05_saves_onehot_minmax_epochs10k')  # generated from interactions + log sensitivity + energies + onehot + lr 1e-1 + 10k epochs + minmax\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_10__21_59_51_saves_onehot_quantile')  # generated from interactions + log sensitivity + onehot + quantile categorical + lr 1e-1 + 10k epochs + minmax\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_11__11_39_29_saves_log_sens_sigmoid_nocat')  # generated from interactions + log sensitivity + no onehot + lr 5e-2 + 10k epochs + minmax + sigmoid\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_11__14_22_59_saves_winit_glorot')  # generated from interactions + log sensitivity + no onehot + lr 5e-2 + sigmoid + minmax + glorot init\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_11__15_17_29_saves_henorm_nosig_minmax')  # generated from interactions + log sensitivity + no onehot + lr 1e-1 + minmax + no sigmoid + he normal init\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_11__17_54_40_saves_henorm_nosig_minmax')  # generated from interactions + log sensitivity + no onehot + lr 1e-1 + minmax + no sigmoid + he normal init\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_12__16_17_49_saves_Eneg_henorm_nosigm_nocat')  # generated from interactions + log sensitivity + no onehot + lr 1e-1 + minmax + no sigmoid + he normal init\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_13__10_59_59_saves_Eneg_onehot')  # generated from interactions + log sensitivity + onehot + lr5e-2 + minmax + no sigmoid + he normal init\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_15__18_08_07_saves_Eneg_onehot_ls3')  # generated from interactions + log sensitivity + onehot + lr5e-2 + minmax + no sigmoid + he normal init + 3 layers + hidden size 64\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_15__18_36_21_saves_Eneg_onehot_ls3_hs32')  # generated from interactions + log sensitivity + onehot + lr5e-2 + minmax + no sigmoid + he normal init + 3 layers + hidden size 32\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_17__13_19_13_saves_Eneg_onehot_ls2_hs32')  # generated from interactions + log sensitivity + onehot + lr5e-2 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_17__15_38_11_saves_KLw25e2')  # generated from interactions + log sensitivity + energies negative onehot + lr1e-1 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32 + KL weight 2.5e-2\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_17__16_22_43_saves_KLw25e3')  # generated from interactions + log sensitivity + energies negative onehot + lr5e-2 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32 + KL weight 2.5e-3\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_17__18_48_25_saves_KLw1e3_lr1e1')  # generated from interactions + log sensitivity + energies negative onehot + lr1e-1 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32 + KL weight 1e-3\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_17__19_05_55_saves_KLw5e4_L21e1')  # generated from interactions + log sensitivity + energies negative onehot + lr1e-1 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32 + KL weight 5e-4 + L2 reg\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_18__12_19_07_saves_bug_fix')  # generated from interactions + log sensitivity + energies negative onehot + lr5e-2 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32 + KL weight 2.5e-4\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_18__17_16_40_saves_L25e2_lr1e1')  # generated from interactions + log sensitivity + energies negative onehot + lr1e-1 + minmax + no sigmoid + he normal init + 2 layers + hidden size 32 + KL weight 2.5e-4\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_18__21_17_58_saves_lr1e1_nocat_nsamebins50')  # generated from interactions + log sensitivity + energies negative NO onehot + lr1e-1 + minmax + no sigmoid + he normal init + 3 layers + hidden size 32 + KL weight 2.5e-4 + n x same bins 50\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_18__22_06_02_saves_lr1e1_nocat_nsamebins50_ls3_bs256')  # generated from interactions + log sensitivity + energies negative NO onehot + lr1e-1 + minmax + no sigmoid + he normal init + 3 layers + hidden size 32 + KL weight 2.5e-4 + n x same bins 50\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', '2024_12_19__16_10_53_saves_hs16_ls32_nlayers2_adam')  # generated from interactions + log sensitivity + energies negative onehot + lr1e-2 + minmax + no sigmoid + he normal init + 2 layers + hidden size 16 + ls 32 + KL weight 2.5e-4 + n x same bins 50\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', 'saves_2025_01_09__15_33_42_adapt')  # objective col adaptation\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', 'saves_2025_01_09__21_26_05_prec_sens')  # objective col sens and prec\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', 'saves_2025_01_14__17_44_21_sens')  # objective col log sens \n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', 'saves_2025_01_17__16_01_57_sens_no_cat')  # objective col log sens without categorical encoding\n",
    "fn_saves = os.path.join('weight_saves', '01_cvae', 'saves_2025_01_21__22_40_13_no_cat_sens_prec')  # objective col log sens without categorical encoding\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_23__15_38_01', 'saves_2025_01_23__15_38_01_sens')  # Redo with just sensitivity + no categorical + no KL (KL did not work)\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_23__15_42_09', 'saves_2025_01_23__15_42_09_sens_cat')  # Same as above but with categorical (onehot) \n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_23__15_47_33', 'saves_2025_01_23__15_47_33_sens_kl')  # Same as above but no categorical (onehot) and with KL\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_23__15_52_15', 'saves_2025_01_23__15_52_15_sens_contloss')  # Same as above but no categorical (onehot) with contrastive loss\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_24__13_53_02', 'saves_2025_01_24__13_53_02_sens_contloss_t2')  # Same as above but no categorical (onehot) with contrastive loss temperature = 2\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_24__15_40_26', 'saves_2025_01_24__15_40_26_sens_contloss_t1_5')  # Same as above but no categorical (onehot) with contrastive loss temperature = 1.5\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_25__12_23_51', 'saves_2025_01_25__12_23_51_sens_kl_25e3')  # Same as above but no categorical (onehot) and with KL weight = 2.5e-3\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_26__14_20_07', 'saves_2025_01_26__14_20_07_sens_kl_1e3')  # Same as above but no categorical (onehot) and with KL weight = 1e-3\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_26__16_16_36', 'saves_2025_01_26__16_16_36_sens_kl_8e4')  # Same as above but no categorical (onehot) and with KL weight = 8e-4\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_26__16_32_02', 'saves_2025_01_26__16_32_02_sens_kl_5e4')  # Same as above but no categorical (onehot) and with KL weight = 5e-4\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_31__21_46_50', 'saves_2025_01_31__21_46_50_ds2')  # New dataset 2 ('./data/simulate_circuits/2025_01_29__18_12_38/tabulated_mutation_info.json) + no response time filer\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_31__21_52_42', 'saves_2025_01_31__21_52_42_ds2_resp')  # New dataset 2 ('./data/simulate_circuits/2025_01_29__18_12_38/tabulated_mutation_info.json) + with response time filter\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_31__23_07_54', 'saves_2025_01_31__23_07_54_ds1_fresp')  # New dataset 1 ('./data/simulate_circuits/2025_01_29__14_52_44/tabulated_mutation_info.json) + with response time filter\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_01_31__23_11_09', 'saves_2025_01_31__23_11_09_ds1_thresh95')  # New dataset 1 ('./data/simulate_circuits/2025_01_29__14_52_44/tabulated_mutation_info.json) + with response time filter + threshold 0.95\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_02_02__16_05_03', 'saves_2025_02_02__16_05_03_ds3')  # New dataset 3 ('./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json)\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_02_02__16_18_20', 'saves_2025_02_02__16_18_20_ds3')  # New dataset 3 ('./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json) --> redid balancing func\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_02_02__18_26_09', 'saves_2025_02_02__18_26_09_ds3')  # New dataset 3 ('./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json) forgot to set balancing hpo\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_02_02__19_14_12', 'saves_2025_02_02__19_14_12_ds0_ds3test')  # Train with old dataset ds0 (data/raw/summarise_simulation/2024_12_05_210221) and verify with new dataset 3 ('./data/simulate_circuits/2025_02_01__00_22_38/tabulated_mutation_info.json) \n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_02_07__17_29_29', 'saves_2025_02_07__17_29_29_dssim1')  # Train with most recent dataset of parameter-based simulation (not sequence-based)\n",
    "fn_saves = os.path.join('data', '01_cvae', '2025_02_07__22_15_21', 'saves_2025_02_07__22_15_21_dssim0128')  # Train less recent dataset of parameter-based simulation \n",
    "saves_loaded = load_json_as_dict(fn_saves)\n",
    "\n",
    "p = saves_loaded[str(list(saves_loaded.keys())[-1])]['params']\n",
    "p = arrayise(p)\n",
    "# p['vae/~/h2mu'] = p.pop('vae/~/linear')\n",
    "# p['vae/~/h2logvar'] = p.pop('vae/~/linear_1')\n",
    "\n",
    "pred_y = model(p, PRNG, x, cond)\n",
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Training')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(6*3, 5))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.plot(list(map(float, saves_loaded.keys())), np.array([float(v['train_loss']) for v in saves_loaded.values()]))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('train_loss')\n",
    "plt.xlabel('step')\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.plot(list(map(float, saves_loaded.keys())), np.array([float(v['val_loss']) for v in saves_loaded.values()]))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('val_loss')\n",
    "plt.xlabel('step')\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.plot(list(map(float, saves_loaded.keys())), np.array([float(v['val_accuracy']) for v in saves_loaded.values()]))\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.xlabel('step')\n",
    "plt.suptitle('Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3228904/3813921430.py:4: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  g = sns.histplot(x=pred_y.flatten(), y=y.flatten(), palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] The R2 score is  0.9794152975082397\n",
      "[Training] The R2 score with weighted variance is  0.9794152975082397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3228904/3813921430.py:18: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  g = sns.histplot(x=pred_y_val.flatten(), y=y_val.flatten(), palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] The R2 score is  0.979361355304718\n",
      "[Validation] The R2 score with weighted variance is  0.9793612957000732\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(2*6, 4))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "g = sns.histplot(x=pred_y.flatten(), y=y.flatten(), palette='viridis')\n",
    "# g = sns.scatterplot(x=pred_y.flatten(), y=y.flatten(), alpha=0.1, hue=np.sqrt(np.abs(pred_y.flatten() - y.flatten())), palette='viridis')\n",
    "# g.legend_.set_title('Sqare root of difference')\n",
    "plt.title(f'Training - Actual vs. predicted decoded circuits\\nR2: {r2_score(y.flatten(), pred_y.flatten()):.2f}')\n",
    "plt.xlabel('Predicted circuit binding energy')\n",
    "plt.ylabel('Actual circuit binding energy')\n",
    "\n",
    "print('[Training] The R2 score is ', r2_score(y.flatten(), pred_y.flatten()))\n",
    "print('[Training] The R2 score with weighted variance is ', r2_score(\n",
    "    y.flatten(), pred_y.flatten(), multioutput='variance_weighted'))\n",
    "\n",
    "pred_y_val = model(p, PRNG, x_val, cond_val)\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "g = sns.histplot(x=pred_y_val.flatten(), y=y_val.flatten(), palette='viridis')\n",
    "# g = sns.scatterplot(x=pred_y_val.flatten(), y=y_val.flatten(), alpha=0.1, hue=np.sqrt(np.abs(pred_y_val.flatten() - y_val.flatten())), palette='viridis')\n",
    "# g.legend_.set_title('Sqare root of difference')\n",
    "plt.title(f'Validation data - Actual vs. predicted decoded circuits\\nR2: {r2_score(y_val.flatten(), pred_y_val.flatten()):.2f}')\n",
    "plt.xlabel('Predicted circuit binding energy')\n",
    "plt.ylabel('Actual circuit binding energy')\n",
    "\n",
    "print('[Validation] The R2 score is ', r2_score(y_val.flatten(), pred_y_val.flatten()))\n",
    "print('[Validation] The R2 score with weighted variance is ', r2_score(\n",
    "    y_val.flatten(), pred_y_val.flatten(), multioutput='variance_weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample fake circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_to_sample = 10000\n",
    "\n",
    "sampled_cond = np.interp(jax.random.normal(PRNG, (n_to_sample, cond.shape[-1])), [0, 1], [cond.min(), cond.max()])\n",
    "sampled_h = np.random.rand(n_to_sample, config_model.enc_ls)\n",
    "# mu = np.random.normal(size=(n_to_sample, HIDDEN_SIZE)) * 10\n",
    "# logvar = np.random.normal(size=(n_to_sample, HIDDEN_SIZE)) * 0.5\n",
    "# mu = h2mu(p, PRNG, sampled_h) \n",
    "# logvar = h2logvar(p, PRNG, sampled_h)\n",
    "# z = sample_z(mu=mu, logvar=logvar, key=PRNG)\n",
    "z = jax.random.normal(PRNG, (n_to_sample, config_model.hidden_size))\n",
    "z = np.concatenate([z, sampled_cond], axis=-1)\n",
    "\n",
    "fake_circuits = decoder(p, PRNG, z)\n",
    "# fake_circuits = np.where(fake_circuits > 0, 0, fake_circuits)\n",
    "sampled_cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Generated fake circuits')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.histplot(fake_circuits.flatten(), bins=50, log_scale=[False, True], element='step', fill=False)\n",
    "plt.title('Generated fake circuits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Interactions for CVAE: 10000 circuits')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7*3, 8))\n",
    "fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "g = sns.histplot(fake_circuits, element='step', bins=30,\n",
    "                 palette='viridis', multiple='fill')\n",
    "plt.title('Fake circuits')\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax = plt.subplot(2, 3, 2)\n",
    "g = sns.histplot(np.where(fake_circuits > x.max(), x.max(), np.where(fake_circuits < x.min(), x.min(), fake_circuits)), \n",
    "                 element='step', bins=30,\n",
    "                 palette='viridis', multiple='fill')\n",
    "plt.title('Fake circuits clipped')\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax = plt.subplot(2, 3, 3)\n",
    "g = sns.histplot(x.reshape(np.prod(x.shape[:-1]), x.shape[-1])[:n_to_sample],\n",
    "                 element='step', bins=30, palette='viridis', multiple='fill')\n",
    "plt.title('Real circuits')\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax = plt.subplot(2, 3, 4)\n",
    "g = sns.histplot(fake_circuits, element='step', bins=30, palette='viridis',\n",
    "                 multiple='layer', fill=False, log_scale=[False, True])\n",
    "plt.title('Fake circuits')\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax = plt.subplot(2, 3, 5)\n",
    "g = sns.histplot(np.where(fake_circuits > x.max(), x.max(), np.where(fake_circuits < x.min(), x.min(), fake_circuits)), \n",
    "                 element='step', bins=30, palette='viridis',\n",
    "                 multiple='layer', fill=False, log_scale=[False, True])\n",
    "plt.title('Fake circuits clipped')\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax = plt.subplot(2, 3, 6)\n",
    "x_hist = x.reshape(np.prod(x.shape[:-1]), x.shape[-1])[:n_to_sample]\n",
    "g2 = sns.histplot(x_hist, element='step', bins=30, palette='viridis',\n",
    "                  multiple='layer', fill=False, log_scale=[False, True])\n",
    "plt.title('Real circuits')\n",
    "sns.move_legend(g2, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "g.set_ylim(g2.get_ylim())\n",
    "\n",
    "plt.suptitle(f'Interactions for CVAE: {n_to_sample} circuits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'CVAE: 2000 circuits')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(13, 4))\n",
    "\n",
    "show_max = 2000\n",
    "\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(fake_circuits[:show_max], alpha=0.1)\n",
    "plt.title('Fake circuits')\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "fake_circuits_1 = np.where(fake_circuits[:show_max] > x.max(), x.max(), fake_circuits[:show_max])\n",
    "sns.scatterplot(np.where(fake_circuits_1 < x.min(), x.min(), fake_circuits_1), alpha=0.1)\n",
    "plt.title('Fake circuits within [min, max]')\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(\n",
    "    x.reshape(np.prod(x.shape[:-1]), x.shape[-1])[:show_max], alpha=0.1)\n",
    "plt.title('Real circuits')\n",
    "\n",
    "plt.suptitle(f'CVAE: {show_max} circuits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total to sample:  10000\n"
     ]
    }
   ],
   "source": [
    "n_obj = len(config_dataset.objective_col)\n",
    "n_categories = 5 if not config_norm_y.categorical_onehot else config_norm_y.categorical_n_bins\n",
    "n_to_sample = 2000\n",
    "print('Total to sample: ', n_categories ** n_obj * n_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 2000, 6), (5, 2000, 1))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config_norm_y.categorical_onehot:\n",
    "    sampled_cond = np.repeat(np.arange(config_norm_y.categorical_n_bins)[:, None], repeats=n_to_sample, axis=1)\n",
    "    sampled_cond = jax.nn.one_hot(sampled_cond, config_norm_y.categorical_n_bins)\n",
    "    for k in config_dataset.objective_col[1:]:\n",
    "        sampled_cond2 = np.repeat(np.arange(config_norm_y.categorical_n_bins)[:, None], repeats=n_to_sample, axis=1)\n",
    "        sampled_cond2 = jax.nn.one_hot(sampled_cond2, config_norm_y.categorical_n_bins)\n",
    "        sampled_cond = np.concatenate([sampled_cond, sampled_cond2], axis=-1)\n",
    "else:\n",
    "    sampled_cond_nonrepeated = np.array(list(itertools.product(*([np.linspace(cond.min(), cond.max(), n_categories).tolist()] * n_obj))))\n",
    "    sampled_cond = np.repeat(sampled_cond_nonrepeated, repeats=n_to_sample, axis=1).reshape(n_categories ** n_obj, n_to_sample, n_obj)\n",
    "    # sampled_cond = np.repeat(s[:, :, None], repeats=n_to_sample, axis=2)\n",
    "    # s = np.repeat(np.linspace(cond.min(), cond.max(), n_categories)[:, None], repeats=n_to_sample, axis=1)[:, :, None]\n",
    "z = jax.random.normal(PRNG, (n_categories ** n_obj, n_to_sample, config_model.hidden_size))\n",
    "z = np.concatenate([z, sampled_cond], axis=-1)\n",
    "\n",
    "fake_circuits = jax.vmap(partial(decoder, params=p, rng=PRNG))(inputs=z)\n",
    "\n",
    "fake_circuits.shape, sampled_cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Fake circuits params')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sns.histplot(decoder(p, PRNG, z).flatten(), bins=50)\n",
    "sns.histplot(fake_circuits.flatten(), bins=50, element='step')\n",
    "plt.title('Fake circuits params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, \"CVAE: fake circuits (['Log sensitivity'])\")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_show_max = 10\n",
    "ncols = 2\n",
    "nrows = n_show_max // ncols + 1\n",
    "for k in config_dataset.objective_col:\n",
    "\n",
    "    fig = plt.figure(figsize=(10*ncols, 5*nrows))\n",
    "    show_max = 300 #n_to_sample\n",
    "    widest_lim = [0, 0]\n",
    "    for i, (zi, cat) in enumerate(zip(z[:n_show_max], fake_circuits[:n_show_max])):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        # g = sns.scatterplot(cat[:show_max])\n",
    "        g = sns.histplot(cat, element='step', bins=30, palette='viridis', multiple='fill')\n",
    "        if config_norm_y.categorical_onehot:\n",
    "            plt.title(f'Fake circuit: {k} = {y_datanormaliser.metadata[k][\"category_map\"][i]:.2f}')\n",
    "        else:\n",
    "            plt.title(f'Fake circuit: {k} = {str(sampled_cond_nonrepeated[i])}')\n",
    "        sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "        widest_lim[0] = np.min([g.get_xlim()[0], widest_lim[0]])\n",
    "        widest_lim[1] = np.max([g.get_xlim()[1], widest_lim[1]])\n",
    "        g.set_xlim(widest_lim)\n",
    "\n",
    "    break\n",
    "plt.suptitle(f'CVAE: fake circuits ({config_dataset.objective_col})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to real circuit's objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fns(x, fns: list):\n",
    "    og_shape = x.shape\n",
    "    if x.ndim > 2:\n",
    "        x = x.reshape(*(np.prod(x.shape[:-1]), x.shape[-1]))\n",
    "    for fcn in fns:\n",
    "        x = fcn(x)\n",
    "    return x.reshape(og_shape)\n",
    "\n",
    "for k in config_dataset.objective_col:\n",
    "    df = df[~df[k].isna()]\n",
    "    df[k + '_nearest_edge'] = bin_to_nearest_edge(df[k].to_numpy(), n_bins=n_categories)\n",
    "fake_circuits = x_datanormaliser.create_chain_preprocessor_inverse(x_methods_preprocessing)(fake_circuits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, \"CVAE: real circuits ['Log sensitivity']\")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in config_dataset.objective_col:\n",
    "    fig = plt.figure(figsize=(10*2, 5*5))\n",
    "    show_max = 300 #n_to_sample\n",
    "    widest_lim = [0, 0]\n",
    "    for i, edge in enumerate(sorted(df[k + '_nearest_edge'].unique())):\n",
    "        cat = df[df[k + '_nearest_edge'] == edge]\n",
    "        cat = cat[x_cols].to_numpy()\n",
    "        \n",
    "        ax = plt.subplot(5, 2, i+1)\n",
    "        # g = sns.scatterplot(cat[:show_max])\n",
    "        g = sns.histplot(cat, element='step', bins=30, palette='magma', multiple='fill')\n",
    "        plt.title(f'Real circuit: {k} = {str(edge)[:6]}')\n",
    "        sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "        widest_lim[0] = np.min([g.get_xlim()[0], widest_lim[0]])\n",
    "        widest_lim[1] = np.max([g.get_xlim()[1], widest_lim[1]])\n",
    "        g.set_xlim(widest_lim)\n",
    "        \n",
    "    break\n",
    "\n",
    "plt.suptitle(f'CVAE: real circuits {config_dataset.objective_col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'CVAE: circuit comparison fake vs. real Log sensitivity')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in config_dataset.objective_col:\n",
    "    fig = plt.figure(figsize=(10*2, n_categories*5))\n",
    "    show_max = 300 #n_to_sample\n",
    "    for i, (zi, fake, edge) in enumerate(zip(z, fake_circuits, sorted(df[k + '_nearest_edge'].unique()))):\n",
    "        widest_lim = [0, 0]\n",
    "        real = df[df[k + '_nearest_edge'] == edge][x_cols].to_numpy()\n",
    "        \n",
    "        ax = plt.subplot(n_categories, 2, 2*i+1)\n",
    "        # g = sns.scatterplot(cat[:show_max])\n",
    "        g1 = sns.histplot(fake, element='step', bins=30, palette='viridis', multiple='fill')\n",
    "        sc = np.array(sorted(np.unique(sampled_cond)))[:, None]\n",
    "        sc = y_datanormaliser.create_chain_preprocessor_inverse(y_methods_preprocessing)(sc, col=k)\n",
    "        # plt.title(f'{len(fake)} fake circuits: {k} = {str(sc.flatten()[i])[:6]}')\n",
    "        if config_norm_y.categorical_onehot:\n",
    "            plt.title(f'{len(fake)} fake circuits: {k} = {y_datanormaliser.metadata[k][\"category_map\"][i]:.2f}')\n",
    "        else:\n",
    "            plt.title(f'{len(fake)} fake circuits: {k} = {str(sc.flatten()[i])[:6]}')\n",
    "        sns.move_legend(g1, 'upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        ax = plt.subplot(n_categories, 2, 2*i+2)\n",
    "        # g = sns.scatterplot(cat[:show_max])\n",
    "        g2 = sns.histplot(real, element='step', bins=30, palette='magma', multiple='fill')\n",
    "        plt.title(f'{len(real)} real circuits: {k} = {str(edge)[:6]}')\n",
    "        sns.move_legend(g2, 'upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        widest_lim = [np.min([g1.get_xlim()[0], g2.get_xlim()[0], widest_lim[0]]), np.max([g1.get_xlim()[1], g2.get_xlim()[1], widest_lim[1]])]\n",
    "        # g1.set_xlim(widest_lim)\n",
    "        # g2.set_xlim(widest_lim)\n",
    "\n",
    "    break\n",
    "\n",
    "plt.suptitle(f'CVAE: circuit comparison fake vs. real {k}', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'CVAE: circuit comparison fake vs. real Log sensitivity')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = config_dataset.objective_col[0]\n",
    "\n",
    "fig = plt.figure(figsize=(10*2, n_categories*5))\n",
    "show_max = 300 #n_to_sample\n",
    "for i, (zi, fake, edge) in enumerate(zip(z, fake_circuits, sorted(df[k + '_nearest_edge'].unique()))):\n",
    "    real = df[df[k + '_nearest_edge'] == edge][x_cols].to_numpy()\n",
    "    \n",
    "    ax = plt.subplot(n_categories, 2, 2*i+1)\n",
    "    # g = sns.scatterplot(cat[:show_max])\n",
    "    g1 = sns.histplot(fake, element='step', bins=30, palette='viridis', multiple='layer', fill=False, log_scale=[False, True])\n",
    "    sc = np.array(sorted(np.unique(sampled_cond)))[:, None]\n",
    "    sc = y_datanormaliser.create_chain_preprocessor_inverse(y_methods_preprocessing)(sc, col=k)\n",
    "    if config_norm_y.categorical_onehot:\n",
    "        plt.title(f'{len(fake)} fake circuits: {k} = {y_datanormaliser.metadata[k][\"category_map\"][i]:.2f}')\n",
    "    else:\n",
    "        plt.title(f'{len(fake)} fake circuits: {k} = {str(sc.flatten()[i])[:6]}')\n",
    "    sns.move_legend(g1, 'upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    ax = plt.subplot(n_categories, 2, 2*i+2)\n",
    "    # g = sns.scatterplot(cat[:show_max])\n",
    "    g2 = sns.histplot(real, element='step', bins=30, palette='magma', multiple='layer', fill=False, log_scale=[False, True])\n",
    "    plt.title(f'{len(real)} real circuits: {k} = {str(edge)[:6]}')\n",
    "    sns.move_legend(g2, 'upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    widest_xlim = [np.min([g1.get_xlim()[0], g2.get_xlim()[0]]), np.max([g1.get_xlim()[1], g2.get_xlim()[1]])]\n",
    "    widest_ylim = [np.min([g1.get_ylim()[0], g2.get_ylim()[0]]), np.max([g1.get_ylim()[1], g2.get_ylim()[1]])]\n",
    "    # g1.set_xlim(widest_xlim)\n",
    "    # g2.set_ylim(widest_ylim)\n",
    "\n",
    "\n",
    "plt.suptitle(f'CVAE: circuit comparison fake vs. real {k}', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
