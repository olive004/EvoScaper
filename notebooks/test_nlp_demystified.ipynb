{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Demystified | Transformers, Pre-training, and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: BPEmb in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: gensim in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from BPEmb) (4.3.1)\n",
      "Requirement already satisfied: numpy in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from BPEmb) (1.23.5)\n",
      "Requirement already satisfied: requests in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from BPEmb) (2.31.0)\n",
      "Requirement already satisfied: sentencepiece in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from BPEmb) (0.1.99)\n",
      "Requirement already satisfied: tqdm in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from BPEmb) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from gensim->BPEmb) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from gensim->BPEmb) (6.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->BPEmb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->BPEmb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->BPEmb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->BPEmb) (2023.5.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 22:14:03.467441: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-15 22:14:03.497469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 22:14:03.920513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install BPEmb\n",
    "\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers From Scratch\n",
    "\n",
    "We'll build a transformer from scratch, layer-by-layer. We'll start with the **Multi-Head Self-Attention** layer since that's the most involved bit. Once we have that working, the rest of the model will look familiar if you've been following the course so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot Product Self-Attention\n",
    "\n",
    "\n",
    "Inside each attention head is a **Scaled Dot Product Self-Attention** operation as we covered in the slides. Given *queries*, *keys*, and *values*, the operation returns a new \"mix\" of the values.\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$\n",
    "\n",
    "The following function implements this and also takes a mask to account for padding and for masking future tokens for decoding (i.e. **look-ahead mask**). We'll cover masking later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    key_dim = jnp.shape(key)[-1]\n",
    "    scaled_scores = jnp.matmul(query, key.swapaxes(-1, -2)) / np.sqrt(key_dim)\n",
    "    scaled_scores = scaled_scores.astype(jnp.float32)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_scores = jnp.where(mask==0, -np.inf, scaled_scores)\n",
    "\n",
    "    softmax = jax.nn.softmax\n",
    "    weights = softmax(scaled_scores) \n",
    "    return jnp.matmul(weights, value), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      " [[0.5488135  0.71518937 0.60276338 0.54488318]\n",
      " [0.4236548  0.64589411 0.43758721 0.891773  ]\n",
      " [0.96366276 0.38344152 0.79172504 0.52889492]]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 3\n",
    "embed_dim = 4\n",
    "\n",
    "queries = np.random.rand(seq_len, embed_dim)\n",
    "keys = np.random.rand(seq_len, embed_dim)\n",
    "values = np.random.rand(seq_len, embed_dim)\n",
    "\n",
    "print(\"Queries:\\n\", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output\n",
      " [[0.3879928  0.5350287  0.13623255 0.7588937 ]\n",
      " [0.39492333 0.53118    0.1382761  0.755936  ]\n",
      " [0.39012212 0.53832173 0.12822185 0.75093997]] \n",
      "\n",
      "Weights\n",
      " [[0.26820898 0.34192598 0.38986498]\n",
      " [0.2510088  0.35895866 0.39003247]\n",
      " [0.25670317 0.3151627  0.4281342 ]]\n"
     ]
    }
   ],
   "source": [
    "output, attn_weights = scaled_dot_product_attention(queries, keys, values)\n",
    "\n",
    "print(\"Output\\n\", output, \"\\n\")\n",
    "print(\"Weights\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(hk.Module):\n",
    "    \"\"\" A 1-layer MLP. Adapted from https://theaisummer.com/jax-transformer/#the-linear-layer \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim: float,\n",
    "                 name: Optional[str] = None,\n",
    "                 activation = jax.nn.relu):\n",
    "        super().__init__(name=name)\n",
    "        self.hiddens = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        # hiddens = x.shape[-1]\n",
    "        # initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
    "        x = hk.Linear(self.hiddens)(x)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating queries, keys, and values for multiple heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of each head: 4\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 3\n",
    "embed_dim = 12\n",
    "num_heads = 3\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "print(f\"Dimension of each head: {head_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using separate weight matrices per head\n",
    "\n",
    "Suppose these are our input embeddings. Here we have a batch of 1 containing a sequence of length 3, with each element being a 12-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (1, 3, 12) \n",
      "\n",
      "Input:\n",
      " [[[0.6 0.6 0.9 0.7 0.4 0.4 0.7 0.1 0.7 0.7 0.2 0.1]\n",
      "  [0.3 0.4 0.6 0.4 1.  0.1 0.2 0.2 0.7 0.3 0.5 0.2]\n",
      "  [0.2 0.1 0.7 0.1 0.2 0.4 0.8 0.1 0.8 0.1 1.  0.5]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
    "print(\"Input shape: \", x.shape, \"\\n\")\n",
    "print(\"Input:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll declare three sets of query weights (one for each head), three sets of key weights, and three sets of value weights. Remember each weight matrix should have a dimension of  d x d/h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three sets of query weights (one for each head):\n",
      "wq0:\n",
      " [[1.  0.6 0.7 0. ]\n",
      " [0.3 0.1 0.3 0.1]\n",
      " [0.3 0.4 0.1 0.7]\n",
      " [0.6 0.3 0.5 0.1]\n",
      " [0.6 0.9 0.3 0.7]\n",
      " [0.1 0.7 0.3 0.2]\n",
      " [0.6 0.  0.8 0. ]\n",
      " [0.7 0.3 0.7 1. ]\n",
      " [0.2 0.6 0.6 0.6]\n",
      " [0.2 1.  0.4 0.8]\n",
      " [0.7 0.3 0.8 0.4]\n",
      " [0.9 0.6 0.9 0.7]]\n",
      "wq1:\n",
      " [[0.7 0.5 1.  0.6]\n",
      " [0.4 0.6 0.  0.3]\n",
      " [0.7 0.3 0.6 0.4]\n",
      " [0.1 0.3 0.6 0.6]\n",
      " [0.6 0.7 0.7 0.4]\n",
      " [0.9 0.4 0.4 0.9]\n",
      " [0.8 0.7 0.1 0.9]\n",
      " [0.7 1.  0.1 0.9]\n",
      " [0.2 0.6 0.1 0.8]\n",
      " [0.8 0.6 0.4 0.1]\n",
      " [0.7 0.5 0.7 0.9]\n",
      " [1.  0.9 0.  0.4]]\n",
      "wq2:\n",
      " [[0.7 0.5 1.  0.6]\n",
      " [0.4 0.6 0.  0.3]\n",
      " [0.7 0.3 0.6 0.4]\n",
      " [0.1 0.3 0.6 0.6]\n",
      " [0.6 0.7 0.7 0.4]\n",
      " [0.9 0.4 0.4 0.9]\n",
      " [0.8 0.7 0.1 0.9]\n",
      " [0.7 1.  0.1 0.9]\n",
      " [0.2 0.6 0.1 0.8]\n",
      " [0.8 0.6 0.4 0.1]\n",
      " [0.7 0.5 0.7 0.9]\n",
      " [1.  0.9 0.  0.4]]\n"
     ]
    }
   ],
   "source": [
    "# The query weights for each head.\n",
    "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "\n",
    "# The key weights for each head. \n",
    "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "\n",
    "# The value weights for each head.\n",
    "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "wv2 = np.random.rand(embed_dim, head_dim).round(1)\n",
    "\n",
    "print(\"The three sets of query weights (one for each head):\")\n",
    "print(\"wq0:\\n\", wq0)\n",
    "print(\"wq1:\\n\", wq1)\n",
    "print(\"wq2:\\n\", wq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate our *queries*, *keys*, and *values* for each head by multiplying our input by the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geneated queries, keys, and values for the first head.\n",
    "q0 = np.dot(x, wq0)\n",
    "k0 = np.dot(x, wk0)\n",
    "v0 = np.dot(x, wv0)\n",
    "\n",
    "# Geneated queries, keys, and values for the second head.\n",
    "q1 = np.dot(x, wq1)\n",
    "k1 = np.dot(x, wk1)\n",
    "v1 = np.dot(x, wv1)\n",
    "\n",
    "# Geneated queries, keys, and values for the third head.\n",
    "q2 = np.dot(x, wq2)\n",
    "k2 = np.dot(x, wk2)\n",
    "v2 = np.dot(x, wv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the resulting *query*, *key*, and *value* vectors for the first head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q, K, and V for first head:\n",
      "\n",
      "q0 (1, 3, 4):\n",
      " [[[2.75 2.9  2.86 2.35]\n",
      "  [2.44 2.6  2.34 2.42]\n",
      "  [2.54 2.11 2.95 2.14]]] \n",
      "\n",
      "k0 (1, 3, 4):\n",
      " [[[3.18 3.45 3.1  2.62]\n",
      "  [2.32 2.39 2.34 2.27]\n",
      "  [2.26 2.38 2.7  2.3 ]]] \n",
      "\n",
      "v0 (1, 3, 4):\n",
      " [[[3.99 4.23 2.71 2.65]\n",
      "  [3.16 3.67 1.82 2.6 ]\n",
      "  [3.19 3.77 2.08 2.75]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q, K, and V for first head:\\n\")\n",
    "\n",
    "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
    "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
    "print(f\"v0 {v0.shape}:\\n\", v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Q, K, V vectors, we can just pass them to our self-attention operation. Here we're calculating the output and attention weights for the first head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from first attention head:  [[[3.960351  4.211777  2.6832957 2.6515236]\n",
      "  [3.9453316 4.202458  2.6695168 2.6521323]\n",
      "  [3.9410832 4.199962  2.6660192 2.652564 ]]] \n",
      "\n",
      "Attention weights from first head:  [[[0.96347106 0.01419647 0.0223325 ]\n",
      "  [0.9450061  0.02245    0.03254395]\n",
      "  [0.9397198  0.02309422 0.03718603]]]\n"
     ]
    }
   ],
   "source": [
    "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
    "\n",
    "print(\"Output from first attention head: \", out0, \"\\n\")\n",
    "print(\"Attention weights from first head: \", attn_weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the other two (attention weights are ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from second attention head:  [[[2.6091306 2.6788611 2.7399383 2.6570723]\n",
      "  [2.6075811 2.6769052 2.7397096 2.6520724]\n",
      "  [2.6086218 2.6782243 2.739854  2.6554472]]] \n",
      "\n",
      "Output from third attention head:  [[[3.350872  2.3895252 2.9842615 3.5212994]\n",
      "  [3.318234  2.3826733 2.9540384 3.495711 ]\n",
      "  [3.337096  2.3867323 2.9716554 3.5106113]]]\n"
     ]
    }
   ],
   "source": [
    "out1, _ = scaled_dot_product_attention(q1, k1, v1)\n",
    "out2, _ = scaled_dot_product_attention(q2, k2, v2)\n",
    "\n",
    "print(\"Output from second attention head: \", out1, \"\\n\")\n",
    "print(\"Output from third attention head: \", out2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we covered in the slides, once we have each head's output, we concatenate them and then put them through a linear layer for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined output from all heads (1, 3, 12):\n",
      "[[[3.960351  4.211777  2.6832957 2.6515236 2.6091306 2.6788611 2.7399383\n",
      "   2.6570723 3.350872  2.3895252 2.9842615 3.5212994]\n",
      "  [3.9453316 4.202458  2.6695168 2.6521323 2.6075811 2.6769052 2.7397096\n",
      "   2.6520724 3.318234  2.3826733 2.9540384 3.495711 ]\n",
      "  [3.9410832 4.199962  2.6660192 2.652564  2.6086218 2.6782243 2.739854\n",
      "   2.6554472 3.337096  2.3867323 2.9716554 3.5106113]]]\n"
     ]
    }
   ],
   "source": [
    "combined_out_a = np.concatenate((out0, out1, out2), axis=-1)\n",
    "print(f\"Combined output from all heads {combined_out_a.shape}:\")\n",
    "print(combined_out_a)\n",
    "\n",
    "# The final step would be to run combined_out_a through a linear/dense layer \n",
    "# for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's a complete run of **multi-head self-attention** using separate sets of weights per head.<br>\n",
    "\n",
    "Let's now get the same thing done using a single query weight matrix, single key weight matrix, and single value weight matrix.<br><br>\n",
    "These were our separate per-head query weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weights for first head: \n",
      " [[1.  0.6 0.7 0. ]\n",
      " [0.3 0.1 0.3 0.1]\n",
      " [0.3 0.4 0.1 0.7]\n",
      " [0.6 0.3 0.5 0.1]\n",
      " [0.6 0.9 0.3 0.7]\n",
      " [0.1 0.7 0.3 0.2]\n",
      " [0.6 0.  0.8 0. ]\n",
      " [0.7 0.3 0.7 1. ]\n",
      " [0.2 0.6 0.6 0.6]\n",
      " [0.2 1.  0.4 0.8]\n",
      " [0.7 0.3 0.8 0.4]\n",
      " [0.9 0.6 0.9 0.7]] \n",
      "\n",
      "Query weights for second head: \n",
      " [[0.7 0.5 1.  0.6]\n",
      " [0.4 0.6 0.  0.3]\n",
      " [0.7 0.3 0.6 0.4]\n",
      " [0.1 0.3 0.6 0.6]\n",
      " [0.6 0.7 0.7 0.4]\n",
      " [0.9 0.4 0.4 0.9]\n",
      " [0.8 0.7 0.1 0.9]\n",
      " [0.7 1.  0.1 0.9]\n",
      " [0.2 0.6 0.1 0.8]\n",
      " [0.8 0.6 0.4 0.1]\n",
      " [0.7 0.5 0.7 0.9]\n",
      " [1.  0.9 0.  0.4]] \n",
      "\n",
      "Query weights for third head: \n",
      " [[0.7 0.2 0.5 0.1]\n",
      " [0.2 0.  0.8 0.2]\n",
      " [0.3 0.9 0.7 0. ]\n",
      " [0.2 0.6 0.6 0.2]\n",
      " [0.9 0.6 0.5 0.6]\n",
      " [0.7 0.3 0.4 0.2]\n",
      " [0.2 0.9 0.7 0.5]\n",
      " [0.2 0.3 0.1 0.4]\n",
      " [0.3 0.7 0.4 0.2]\n",
      " [0.  0.1 0.7 0.5]\n",
      " [0.5 0.9 1.  0.2]\n",
      " [0.7 0.3 0.  0.8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
    "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
    "print(\"Query weights for third head: \\n\", wq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single query weight matrix (12, 12): \n",
      " [[1.  0.6 0.7 0.  0.7 0.5 1.  0.6 0.7 0.2 0.5 0.1]\n",
      " [0.3 0.1 0.3 0.1 0.4 0.6 0.  0.3 0.2 0.  0.8 0.2]\n",
      " [0.3 0.4 0.1 0.7 0.7 0.3 0.6 0.4 0.3 0.9 0.7 0. ]\n",
      " [0.6 0.3 0.5 0.1 0.1 0.3 0.6 0.6 0.2 0.6 0.6 0.2]\n",
      " [0.6 0.9 0.3 0.7 0.6 0.7 0.7 0.4 0.9 0.6 0.5 0.6]\n",
      " [0.1 0.7 0.3 0.2 0.9 0.4 0.4 0.9 0.7 0.3 0.4 0.2]\n",
      " [0.6 0.  0.8 0.  0.8 0.7 0.1 0.9 0.2 0.9 0.7 0.5]\n",
      " [0.7 0.3 0.7 1.  0.7 1.  0.1 0.9 0.2 0.3 0.1 0.4]\n",
      " [0.2 0.6 0.6 0.6 0.2 0.6 0.1 0.8 0.3 0.7 0.4 0.2]\n",
      " [0.2 1.  0.4 0.8 0.8 0.6 0.4 0.1 0.  0.1 0.7 0.5]\n",
      " [0.7 0.3 0.8 0.4 0.7 0.5 0.7 0.9 0.5 0.9 1.  0.2]\n",
      " [0.9 0.6 0.9 0.7 1.  0.9 0.  0.4 0.7 0.3 0.  0.8]]\n"
     ]
    }
   ],
   "source": [
    "wq = np.concatenate((wq0, wq1, wq2), axis=1)\n",
    "print(f\"Single query weight matrix {wq.shape}: \\n\", wq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same vein, pretend we declared a single key weight matrix, and single value weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single key weight matrix (12, 12):\n",
      " [[0.3 0.4 0.6 0.8 0.7 0.8 0.3 0.6 0.1 0.1 0.7 0.2]\n",
      " [0.6 0.9 0.3 0.8 0.1 0.5 1.  0.9 0.4 0.6 0.9 0.7]\n",
      " [0.2 1.  0.7 0.2 0.3 1.  0.2 0.9 0.3 0.1 0.1 0.3]\n",
      " [0.9 0.7 0.3 0.2 0.9 0.8 0.6 0.9 0.3 0.5 0.7 0.7]\n",
      " [0.5 0.  0.2 0.4 0.3 0.8 0.6 0.  0.3 0.4 0.2 0.8]\n",
      " [0.4 0.5 0.3 0.6 0.3 0.1 1.  0.5 0.1 0.7 0.8 0.8]\n",
      " [0.9 0.1 0.5 0.1 0.5 0.6 0.4 0.1 0.3 0.4 0.6 0.3]\n",
      " [0.7 0.4 0.6 0.2 0.8 0.2 0.5 0.2 0.4 0.2 0.5 0. ]\n",
      " [0.1 0.5 0.4 0.9 0.1 0.9 1.  1.  0.8 0.1 0.5 0.3]\n",
      " [0.8 0.7 0.9 0.1 0.9 0.8 0.3 0.1 0.6 1.  0.6 0. ]\n",
      " [0.6 0.6 1.  0.3 0.4 0.2 0.1 0.1 0.4 0.5 0.5 0.7]\n",
      " [0.2 0.1 0.  0.9 0.7 0.  0.8 0.1 0.3 0.1 0.4 1. ]] \n",
      "\n",
      "Single value weight matrix (12, 12):\n",
      " [[0.2 0.9 0.5 0.5 0.6 0.  0.  0.4 0.7 0.1 0.7 0.9]\n",
      " [0.9 0.5 0.7 0.4 0.1 0.3 0.2 0.3 0.3 0.4 0.4 0.7]\n",
      " [0.9 0.7 0.7 0.3 0.1 0.  0.1 0.6 0.2 0.2 0.4 0.3]\n",
      " [0.8 0.6 0.2 0.2 1.  1.  0.4 0.2 0.8 0.2 0.5 0.9]\n",
      " [0.8 1.  0.5 0.6 0.6 0.5 1.  0.1 0.6 0.9 0.9 0.8]\n",
      " [0.9 0.5 1.  0.6 0.8 0.3 0.2 0.7 0.7 1.  1.  0.5]\n",
      " [0.8 0.9 0.8 0.2 0.2 0.7 0.5 0.4 0.1 0.3 0.2 0.4]\n",
      " [0.6 0.4 0.1 0.4 0.6 0.3 0.7 0.4 0.1 0.6 0.4 0.9]\n",
      " [0.3 0.8 0.  1.  0.4 0.8 0.9 0.  1.  0.5 0.3 0.6]\n",
      " [0.4 0.4 0.  0.2 0.2 0.3 0.8 1.  0.9 0.4 0.6 0.3]\n",
      " [0.4 0.9 0.1 0.9 1.  0.9 0.3 1.  0.5 0.4 0.2 0.5]\n",
      " [0.9 0.5 0.3 0.2 0.2 0.1 1.  0.2 0.3 0.4 0.5 0.8]]\n"
     ]
    }
   ],
   "source": [
    "wk = np.concatenate((wk0, wk1, wk2), axis=1)\n",
    "wv = np.concatenate((wv0, wv1, wv2), axis=1)\n",
    "\n",
    "print(f\"Single key weight matrix {wk.shape}:\\n\", wk, \"\\n\")\n",
    "print(f\"Single value weight matrix {wv.shape}:\\n\", wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can calculate all our queries, keys, and values with three dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_s = np.dot(x, wq)\n",
    "k_s = np.dot(x, wk)\n",
    "v_s = np.dot(x, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our resulting query vectors (we'll call them \"combined queries\"). How do we simulate different heads with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vectors using a single weight matrix (1, 3, 12):\n",
      " [[[2.75 2.9  2.86 2.35 3.53 3.2  2.57 3.41 2.13 3.14 3.66 1.64]\n",
      "  [2.44 2.6  2.34 2.42 2.75 2.8  2.22 2.75 2.2  2.74 2.82 1.54]\n",
      "  [2.54 2.11 2.95 2.14 3.31 2.85 1.89 3.49 2.12 3.34 2.95 1.51]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query vectors using a single weight matrix {q_s.shape}:\\n\", q_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, we need to separate these vectors such they're treated like three separate sets by the self-attention operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.75 2.9  2.86 2.35]\n",
      "  [2.44 2.6  2.34 2.42]\n",
      "  [2.54 2.11 2.95 2.14]]] \n",
      "\n",
      "[[[3.53 3.2  2.57 3.41]\n",
      "  [2.75 2.8  2.22 2.75]\n",
      "  [3.31 2.85 1.89 3.49]]] \n",
      "\n",
      "[[[2.13 3.14 3.66 1.64]\n",
      "  [2.2  2.74 2.82 1.54]\n",
      "  [2.12 3.34 2.95 1.51]]]\n"
     ]
    }
   ],
   "source": [
    "print(q0, \"\\n\")\n",
    "print(q1, \"\\n\")\n",
    "print(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each set of per-head queries looks like we took the combined queries, and chopped them vertically every four dimensions.\n",
    "<br><br>\n",
    "We can split our combined queries into $\\text{d}\\ \\text{x}\\ \\text{d/h}$ heads using **reshape** and **transpose**.<br><br>\n",
    "The first step is to *reshape* our combined queries from a shape of:<br>\n",
    "(batch_size, seq_len, embed_dim)<br>\n",
    "\n",
    "into a shape of<br>\n",
    " (batch_size, seq_len, num_heads, head_dim).\n",
    " <br>\n",
    "\n",
    " https://www.tensorflow.org/api_docs/python/tf/reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined queries: (1, 3, 12)\n",
      " [[[2.75 2.9  2.86 2.35 3.53 3.2  2.57 3.41 2.13 3.14 3.66 1.64]\n",
      "  [2.44 2.6  2.34 2.42 2.75 2.8  2.22 2.75 2.2  2.74 2.82 1.54]\n",
      "  [2.54 2.11 2.95 2.14 3.31 2.85 1.89 3.49 2.12 3.34 2.95 1.51]]] \n",
      "\n",
      "Reshaped into separate heads: (1, 3, 3, 4)\n",
      " [[[[2.75 2.9  2.86 2.35]\n",
      "   [3.53 3.2  2.57 3.41]\n",
      "   [2.13 3.14 3.66 1.64]]\n",
      "\n",
      "  [[2.44 2.6  2.34 2.42]\n",
      "   [2.75 2.8  2.22 2.75]\n",
      "   [2.2  2.74 2.82 1.54]]\n",
      "\n",
      "  [[2.54 2.11 2.95 2.14]\n",
      "   [3.31 2.85 1.89 3.49]\n",
      "   [2.12 3.34 2.95 1.51]]]]\n"
     ]
    }
   ],
   "source": [
    "# Note: we can achieve the same thing by passing -1 instead of seq_len.\n",
    "q_s_reshaped = jnp.reshape(q_s, (batch_size, seq_len, num_heads, head_dim))\n",
    "print(f\"Combined queries: {q_s.shape}\\n\", q_s, \"\\n\")\n",
    "print(f\"Reshaped into separate heads: {q_s_reshaped.shape}\\n\", q_s_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our desired shape. The next step is to *transpose* it such that simulates vertically chopping our combined queries. By transposing, our matrix dimensions become:<br>\n",
    "(batch_size, num_heads, seq_len, head_dim)<br>\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries transposed into \"separate\" heads (1, 3, 3, 4):\n",
      " [[[[2.75 2.9  2.86 2.35]\n",
      "   [2.44 2.6  2.34 2.42]\n",
      "   [2.54 2.11 2.95 2.14]]\n",
      "\n",
      "  [[3.53 3.2  2.57 3.41]\n",
      "   [2.75 2.8  2.22 2.75]\n",
      "   [3.31 2.85 1.89 3.49]]\n",
      "\n",
      "  [[2.13 3.14 3.66 1.64]\n",
      "   [2.2  2.74 2.82 1.54]\n",
      "   [2.12 3.34 2.95 1.51]]]]\n"
     ]
    }
   ],
   "source": [
    "q_s_transposed = jnp.transpose(q_s_reshaped, axes=[0, 2, 1, 3])\n",
    "print(f\"Queries transposed into \\\"separate\\\" heads {q_s_transposed.shape}:\\n\", \n",
    "      q_s_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this against the separate per-head queries we calculated previously, we see the same result except we now have all our queries in a single matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The separate per-head query matrices from before: \n",
      "[[[2.75 2.9  2.86 2.35]\n",
      "  [2.44 2.6  2.34 2.42]\n",
      "  [2.54 2.11 2.95 2.14]]] \n",
      "\n",
      "[[[3.53 3.2  2.57 3.41]\n",
      "  [2.75 2.8  2.22 2.75]\n",
      "  [3.31 2.85 1.89 3.49]]] \n",
      "\n",
      "[[[2.13 3.14 3.66 1.64]\n",
      "  [2.2  2.74 2.82 1.54]\n",
      "  [2.12 3.34 2.95 1.51]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The separate per-head query matrices from before: \")\n",
    "print(q0, \"\\n\")\n",
    "print(q1, \"\\n\")\n",
    "print(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys for all heads in a single matrix (1, 3, 12): \n",
      " [[[[3.18 3.45 3.1  2.62]\n",
      "   [2.32 2.39 2.34 2.27]\n",
      "   [2.26 2.38 2.7  2.3 ]]\n",
      "\n",
      "  [[2.9  4.27 3.36 3.43]\n",
      "   [2.06 3.3  2.73 2.35]\n",
      "   [2.03 2.69 2.58 2.19]]\n",
      "\n",
      "  [[2.28 2.48 3.32 2.6 ]\n",
      "   [1.94 1.76 2.27 2.5 ]\n",
      "   [1.93 1.63 2.42 2.55]]]] \n",
      "\n",
      "Values for all heads in a single matrix (1, 3, 12): \n",
      " [[[[3.99 4.23 2.71 2.65]\n",
      "   [3.16 3.67 1.82 2.6 ]\n",
      "   [3.19 3.77 2.08 2.75]]\n",
      "\n",
      "  [[2.61 2.68 2.74 2.66]\n",
      "   [2.4  2.37 2.78 1.85]\n",
      "   [2.4  2.56 2.48 2.41]]\n",
      "\n",
      "  [[3.4  2.4  3.03 3.56]\n",
      "   [2.76 2.32 2.52 3.12]\n",
      "   [2.42 2.14 2.04 2.73]]]]\n"
     ]
    }
   ],
   "source": [
    "k_s_transposed = jnp.transpose(jnp.reshape(k_s, (batch_size, -1, num_heads, head_dim)), axes=[0, 2, 1, 3])\n",
    "v_s_transposed = jnp.transpose(jnp.reshape(v_s, (batch_size, -1, num_heads, head_dim)), axes=[0, 2, 1, 3])\n",
    "\n",
    "print(f\"Keys for all heads in a single matrix {k_s.shape}: \\n\", k_s_transposed, \"\\n\")\n",
    "print(f\"Values for all heads in a single matrix {v_s.shape}: \\n\", v_s_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up this way, we can now calculate the outputs from all attention heads with a single call to our self-attention operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self attention output:\n",
      " [[[[3.960351  4.211777  2.6832957 2.6515236]\n",
      "   [3.9453316 4.202458  2.6695168 2.6521323]\n",
      "   [3.9410832 4.199962  2.6660192 2.652564 ]]\n",
      "\n",
      "  [[2.6091306 2.6788611 2.7399383 2.6570723]\n",
      "   [2.6075811 2.6769052 2.7397096 2.6520724]\n",
      "   [2.6086218 2.6782243 2.739854  2.6554472]]\n",
      "\n",
      "  [[3.350872  2.3895252 2.9842615 3.5212994]\n",
      "   [3.318234  2.3826733 2.9540384 3.495711 ]\n",
      "   [3.337096  2.3867323 2.9716554 3.5106113]]]]\n"
     ]
    }
   ],
   "source": [
    "all_heads_output, all_attn_weights = scaled_dot_product_attention(q_s_transposed, \n",
    "                                                                  k_s_transposed, \n",
    "                                                                  v_s_transposed)\n",
    "print(\"Self attention output:\\n\", all_heads_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can compare this against the outputs from individual heads we calculated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per head outputs from using separate sets of weights per head:\n",
      "[[[3.960351  4.211777  2.6832957 2.6515236]\n",
      "  [3.9453316 4.202458  2.6695168 2.6521323]\n",
      "  [3.9410832 4.199962  2.6660192 2.652564 ]]] \n",
      "\n",
      "[[[2.6091306 2.6788611 2.7399383 2.6570723]\n",
      "  [2.6075811 2.6769052 2.7397096 2.6520724]\n",
      "  [2.6086218 2.6782243 2.739854  2.6554472]]] \n",
      "\n",
      "[[[3.350872  2.3895252 2.9842615 3.5212994]\n",
      "  [3.318234  2.3826733 2.9540384 3.495711 ]\n",
      "  [3.337096  2.3867323 2.9716554 3.5106113]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Per head outputs from using separate sets of weights per head:\")\n",
    "print(out0, \"\\n\")\n",
    "print(out1, \"\\n\")\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the final concatenated result, we need to reverse our **reshape** and **transpose** operation, starting with the **transpose** this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output from using single query, key, value matrices:\n",
      " [[[3.960351  4.211777  2.6832957 2.6515236 2.6091306 2.6788611 2.7399383\n",
      "   2.6570723 3.350872  2.3895252 2.9842615 3.5212994]\n",
      "  [3.9453316 4.202458  2.6695168 2.6521323 2.6075811 2.6769052 2.7397096\n",
      "   2.6520724 3.318234  2.3826733 2.9540384 3.495711 ]\n",
      "  [3.9410832 4.199962  2.6660192 2.652564  2.6086218 2.6782243 2.739854\n",
      "   2.6554472 3.337096  2.3867323 2.9716554 3.5106113]]] \n",
      "\n",
      "Final output from using separate query, key, value matrices per head:\n",
      " [[[3.960351  4.211777  2.6832957 2.6515236 2.6091306 2.6788611 2.7399383\n",
      "   2.6570723 3.350872  2.3895252 2.9842615 3.5212994]\n",
      "  [3.9453316 4.202458  2.6695168 2.6521323 2.6075811 2.6769052 2.7397096\n",
      "   2.6520724 3.318234  2.3826733 2.9540384 3.495711 ]\n",
      "  [3.9410832 4.199962  2.6660192 2.652564  2.6086218 2.6782243 2.739854\n",
      "   2.6554472 3.337096  2.3867323 2.9716554 3.5106113]]]\n"
     ]
    }
   ],
   "source": [
    "combined_out_b = jnp.reshape(jnp.transpose(all_heads_output, axes=[0, 2, 1, 3]), \n",
    "                            newshape=(batch_size, seq_len, embed_dim))\n",
    "print(\"Final output from using single query, key, value matrices:\\n\", \n",
    "      combined_out_b, \"\\n\")\n",
    "print(\"Final output from using separate query, key, value matrices per head:\\n\", \n",
    "      combined_out_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encapsulate everything we just covered in a class.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(hk.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.d_head = self.d_model // self.num_heads\n",
    "\n",
    "        self.wq = hk.Linear(self.d_model)\n",
    "        self.wk = hk.Linear(self.d_model)\n",
    "        self.wv = hk.Linear(self.d_model)\n",
    "\n",
    "        # Linear layer to generate the final output.\n",
    "        self.dense = hk.Linear(self.d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        split_inputs = jnp.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
    "        return jnp.transpose(split_inputs, axes=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        merged_inputs = jnp.transpose(x, axes=[0, 2, 1, 3])\n",
    "        return jnp.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        qs = self.wq(q)\n",
    "        ks = self.wk(k)\n",
    "        vs = self.wv(v)\n",
    "\n",
    "        qs = self.split_heads(qs)\n",
    "        ks = self.split_heads(ks)\n",
    "        vs = self.split_heads(vs)\n",
    "\n",
    "        output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
    "        output = self.merge_heads(output)\n",
    "\n",
    "        return self.dense(output), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHSA output(1, 3, 12):\n",
      "[[[ 0.08374095  0.28195387  0.45835775 -0.20913911 -0.00389449\n",
      "    0.51052946  0.38135567 -0.07603626 -0.8463101   0.7846994\n",
      "    0.3223069   0.7794864 ]\n",
      "  [ 0.08046716  0.27548125  0.46089178 -0.21426184  0.00246251\n",
      "    0.5044182   0.38591275 -0.08152059 -0.8504362   0.7838822\n",
      "    0.32661867  0.7838007 ]\n",
      "  [ 0.08387989  0.27426255  0.47376037 -0.2213613   0.00847191\n",
      "    0.49273473  0.37900913 -0.08729805 -0.8445233   0.77479714\n",
      "    0.3281104   0.7750339 ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wadh6511/Kode/env_evo/lib/python3.9/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    }
   ],
   "source": [
    "# mhsa = MultiHeadSelfAttention(12, 3)\n",
    "def fwdx(q, k, v, mask):\n",
    "    mhsa = MultiHeadSelfAttention(12, 3)\n",
    "    return mhsa(q, k, v, mask)\n",
    "    \n",
    "mhsa = hk.transform(fwdx)\n",
    "rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
    "p = mhsa.init(next(rng), x, x, x, None)\n",
    "output, attn_weights = mhsa.apply(p, None, x, x, x, None)\n",
    "print(f\"MHSA output{output.shape}:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "We can now build our **Encoder Block**. In addition to the **Multi-Head Self Attention** layer, the **Encoder Block** also has **skip connections**, **layer normalization steps**, and a **two-layer feed-forward neural network**. The original **Attention Is All You Need** paper also included some **dropout** applied to the self-attention output which isn't shown in the illustration below (see references for a link to the paper).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1D8sLDyQMqqhCjHWOn-I7rZKHugWxFyLy\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a two-layer feed forward neural network is used in multiple places in the transformer, here's a function which creates and returns one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(d_model, hidden_dim):\n",
    "    return hk.Sequential([\n",
    "        Dense(hidden_dim, activation=jax.nn.relu),\n",
    "        Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our encoder block containing all the layers and steps from the preceding illustration (plus dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mhsa = MultiHeadSelfAttention(12, 3)\n",
    "# def fwdx(q, k, v, mask):\n",
    "#     mhsa = MultiHeadSelfAttention(12, 3)\n",
    "#     return mhsa(q, k, v, mask)\n",
    "    \n",
    "# mhsa = hk.transform(fwdx)\n",
    "# rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
    "# p = mhsa.init(next(rng), x, x, x, None)\n",
    "# output, attn_weights = mhsa.apply(p, None, x, x, x, None)\n",
    "# print(f\"MHSA output{output.shape}:\")\n",
    "# print(output)\n",
    "\n",
    "\n",
    "class EncoderBlock(hk.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        # self.params_mhsa = self.mhsa.init(x, x, x, None)\n",
    "        self.ffn = feed_forward_network(d_model, hidden_dim)\n",
    "\n",
    "        # self.dropout1 = partial(hk.dropout, rng=next(rng), rate=dropout_rate)\n",
    "        # self.dropout2 = partial(hk.dropout, rng=next(rng), rate=dropout_rate)\n",
    "\n",
    "        self.layernorm1 = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "        self.layernorm2 = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "    \n",
    "    def __call__(self, x, training, mask):\n",
    "        mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
    "        mhsa_output = hk.dropout(next(rng), self.dropout_rate, mhsa_output) #, training=training)\n",
    "        mhsa_output = self.layernorm1(x + mhsa_output)\n",
    "\n",
    "        ffn_output = self.ffn(mhsa_output)\n",
    "        ffn_output = hk.dropout(next(rng), self.dropout_rate, ffn_output) #, training=training)\n",
    "        output = self.layernorm2(mhsa_output + ffn_output)\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an embedding dimension of 12, and we want 3 attention heads and a feed forward network with a hidden dimension of 48 (4x the embedding dimension). We would declare and use a single encoder block like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from single encoder block (1, 3, 12):\n",
      "[[[ 0.75087804 -1.5568879   0.4482081   0.51610124 -0.86587226\n",
      "    0.9495493   0.83768344 -0.03637511  0.1033235   1.3288491\n",
      "   -0.41613346 -2.0593245 ]\n",
      "  [ 0.5299162  -1.9707414  -0.98887664  0.32056147  0.9441586\n",
      "    0.5829572  -0.16058876  0.62748796  0.40224558  1.0025263\n",
      "    0.601236   -1.8908825 ]\n",
      "  [-0.4772026  -1.9451121  -0.66309965 -0.35051018 -0.9115741\n",
      "    1.1237394   0.894805    0.15113567  1.0693713   0.20748584\n",
      "    1.6580435  -0.7570817 ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def fwd_enc(x, training, mask):\n",
    "    encoder_block = EncoderBlock(12, 3, 48)\n",
    "    return encoder_block(x, training, mask)\n",
    "    \n",
    "encoder_block = hk.transform(fwd_enc)\n",
    "params_enc = encoder_block.init(next(rng), x, True, None)\n",
    "block_output,  _ = encoder_block.apply(params_enc, next(rng), x, True, None)\n",
    "print(f\"Output from single encoder block {block_output.shape}:\")\n",
    "print(block_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now deal with the actual input to the **initial** encoder block. The inputs are going to be *positional word embeddings*. That is, word embeddings with some positional information added to them.\n",
    "<br>\n",
    "\n",
    "Let's start with **subword** tokenization. For demonstration, we'll use a subword tokenizer called **BPEmb**. It uses **Byte-Pair Encoding** and supports over two hundred languages. \n",
    "\n",
    "https://bpemb.h-its.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400869/400869 [00:00<00:00, 4519790.14B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3784656/3784656 [00:00<00:00, 25668469.85B/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the English tokenizer.\n",
    "bpemb_en = BPEmb(lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library comes with embeddings for a number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "Embedding size: 100\n"
     ]
    }
   ],
   "source": [
    "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
    "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
    "print(\"Embedding size:\", bpemb_embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.305548, -0.325598, -0.134716, -0.078735, -0.660545,  0.076211,\n",
       "       -0.735487,  0.124533, -0.294402,  0.459688,  0.030137,  0.174041,\n",
       "       -0.224223,  0.486189, -0.504649, -0.459699,  0.315747,  0.477885,\n",
       "        0.091398,  0.427867,  0.016524, -0.076833, -0.899727,  0.493158,\n",
       "       -0.022309, -0.422785, -0.154148,  0.204981,  0.379834,  0.070588,\n",
       "        0.196073, -0.368222,  0.473406,  0.007409,  0.004303, -0.007823,\n",
       "       -0.19103 , -0.202509,  0.109878, -0.224521, -0.35741 , -0.611633,\n",
       "        0.329958, -0.212956, -0.497499, -0.393839, -0.130101, -0.216903,\n",
       "       -0.105595, -0.076007, -0.483942, -0.139704, -0.161647,  0.136985,\n",
       "        0.415363, -0.360143,  0.038601, -0.078804, -0.030421,  0.324129,\n",
       "        0.223378, -0.523636, -0.048317, -0.032248, -0.117367,  0.470519,\n",
       "        0.225816, -0.222065, -0.225007, -0.165904, -0.334389, -0.20157 ,\n",
       "        0.572352, -0.268794,  0.301929, -0.005563,  0.387491,  0.261031,\n",
       "       -0.11613 ,  0.074982, -0.008433,  0.259987, -0.099893, -0.268875,\n",
       "       -0.054047, -0.534776, -0.111101, -0.051742,  0.214114,  0.04293 ,\n",
       "        0.039873, -0.453112,  0.087382, -0.333201, -0.034079, -0.833045,\n",
       "        0.155232, -1.132393, -0.294766,  0.327572], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding for the word \"car\".\n",
    "bpemb_en.vectors[bpemb_en.words.index('car')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the embeddings since we're going to use our own embedding layer. What we're interested in are the subword tokens and their respective ids. The ids will be used as indexes into our embedding layer.<br>\n",
    "\n",
    "If this doesn't sound familiar, refer to the module on word vectors:<br>\n",
    "https://www.nlpdemystified.org/course/word-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the subword tokens for our example sentence from the slides. **BPEmb** places underscores in front of any tokens which are whole words or intended to begin words.<br>\n",
    "\n",
    "Remember that subword tokenizers are trained using count frequencies over a corpus. So these subword tokens are specific to **BPEmb**. Another subword tokenizer may output something different. This is why it's important that when we use a pretrained model, we make sure to use the pretrained model's tokenizer. We'll see this when we use pretrained transformers later in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"Where can I find a pizzeria?\"\n",
    "tokens = bpemb_en.encode(sample_sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve each subword token's respective id using the *encode_ids* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 571  280  386 1934    4   24  248 4339  177 9967]\n"
     ]
    }
   ],
   "source": [
    "token_seq = np.array(bpemb_en.encode_ids(\"Where can I find a pizzeria?\"))\n",
    "print(token_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to tokenize and vectorize sentences, we can declare and use an embedding layer with the same vocabulary size as **BPEmb** and a desired embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for:  Where can I find a pizzeria? ((10, 12))\n",
      "[[ 1.6094604  -0.52538157 -0.8895934  -0.5229291  -1.085535    0.42052966\n",
      "  -1.6960709   1.1735058   0.37995493 -0.49312064 -0.42313188  0.15053356]\n",
      " [ 1.7967463   0.3124195   0.55778956 -0.9394734   0.420552    0.33601767\n",
      "  -0.18352616 -1.0491136  -0.64903814 -0.15722333  0.9599779   1.6771305 ]\n",
      " [ 0.30299115 -0.31363884 -0.11355441 -0.03616833 -0.5260365  -0.53790313\n",
      "  -0.4002148   0.8586032  -0.92141944 -1.4005889  -0.473717    1.5010474 ]\n",
      " [-0.04111395 -1.6209657  -0.22683966  1.1904185   0.67703724 -0.29590532\n",
      "   0.29054746  0.56238264 -0.7022962  -1.0431658  -0.5001555   0.30058387]\n",
      " [-0.08979569 -0.23829778 -0.77581775  0.13661242 -0.70797837 -0.14180613\n",
      "   1.4101925   0.4633062  -0.802325    0.20152093 -1.5932127  -0.32765976]\n",
      " [ 0.08688173 -0.84189755  0.1865582  -0.13941963 -0.22614297  1.7828015\n",
      "  -0.36164925 -0.5667005   1.6301054  -0.14879316  0.56012404  1.0910765 ]\n",
      " [-0.10597399 -0.37248015  1.5519582  -0.8088619  -0.40161896  0.37520653\n",
      "   0.34490103 -1.4559733  -0.0778239   0.48939258 -0.23919137  0.24744713]\n",
      " [-0.17058884  1.9320704   0.7197472   0.1033903   0.1129631  -1.8959417\n",
      "   1.2829589  -0.92632467 -0.23008057 -0.9498429  -0.43903473  0.94316983]\n",
      " [ 0.13365357  1.0586346   0.3567331  -1.3332795  -0.7974733   0.4924634\n",
      "  -0.00355521 -0.21514836  1.1861658   0.39946437  1.641544   -0.7319606 ]\n",
      " [-1.5631582   1.492841    0.25754195 -0.428455    0.15870778 -0.04778207\n",
      "  -0.8699946  -0.6513993   0.58053035  1.7492111   0.5487333   0.3480518 ]]\n"
     ]
    }
   ],
   "source": [
    "token_embed = hk.transform(lambda x: hk.Embed(bpemb_vocab_size, embed_dim)(x))\n",
    "p_token = token_embed.init(next(rng), token_seq)\n",
    "token_embeddings = token_embed.apply(p_token, next(rng), token_seq)\n",
    "\n",
    "# The untrained embeddings for our sample sentence.\n",
    "print(\"Embeddings for: \", sample_sentence, f\"({token_embeddings.shape})\")\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to add *positional* information to each token embedding. As we covered in the slides, the original paper used sinusoidals but it's more common these days to just use another set of embeddings. We'll do the latter here.<br>\n",
    "\n",
    "Here, we're declaring an embedding layer with rows equalling a maximum sequence length and columns equalling our token embedding size. We then generate a vector of position ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 256\n",
    "pos_embed = hk.transform(lambda x: hk.Embed(max_seq_len, embed_dim)(x))\n",
    "\n",
    "# Generate ids for each position of the token sequence.\n",
    "pos_idx = jnp.arange(len(token_seq))\n",
    "print(pos_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these position ids to index into the positional embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position embeddings for the input sequence ((10, 12))\n",
      " [[ 0.7781124   1.7142845  -0.2877947  -0.23030427  0.7116986   0.8313217\n",
      "  -0.12955669 -0.15172009  0.05667705  0.08876359 -1.1676164   0.05004088]\n",
      " [-0.6225034   1.2481763  -0.328634    0.02548295 -1.8445315  -0.0901474\n",
      "   1.7251247   0.41959327 -1.4409891   0.64706844  0.17834616 -0.42675495]\n",
      " [ 1.7740337  -0.13828552 -1.1180384  -0.41133064 -1.1012032  -1.6601293\n",
      "   0.99315846 -0.58263636 -1.106893    0.5538767  -0.64504117  0.7844581 ]\n",
      " [ 1.5640007  -0.02428322 -0.79117036  0.755527    1.2078457  -0.3640467\n",
      "  -1.3301423  -0.3062533   0.55079937 -0.6502779  -1.1676248   0.19947718]\n",
      " [-0.29926735 -1.040653    1.5888288   1.4027246  -0.2158562  -1.52093\n",
      "  -0.04889803 -0.73962593  0.0248518   0.23585802  1.0188191   0.07387792]\n",
      " [ 0.59272784 -0.12436684  0.77351147  1.0164343  -1.0248847  -0.22747943\n",
      "  -1.4248497  -0.8895201   0.08861105 -0.4611798  -1.1168866   1.851782  ]\n",
      " [ 0.9001088   1.1408491   0.7369474  -0.9238413   0.5058439  -0.19420528\n",
      "  -0.5600042   0.40692487  0.17933993  1.6527487   0.15665542 -0.61779475]\n",
      " [-1.8594835   0.69005954  1.9792973   0.98515916  0.784665    1.5329835\n",
      "  -0.19567819 -1.65638    -1.7633958   0.62084943 -1.6396554   0.5323547 ]\n",
      " [ 1.261646    0.45486167  0.47382712 -0.69578177 -1.388747   -0.15051195\n",
      "  -0.09942233 -0.13377899  0.13497272 -1.2058444  -0.08319332  1.211307  ]\n",
      " [-0.84471405  0.5668124  -0.09846301 -1.2755808   0.1903597  -1.2094915\n",
      "  -1.16234    -1.2764647  -0.5515159   0.7342174  -0.75727844 -0.39490286]]\n"
     ]
    }
   ],
   "source": [
    "# These are our positon embeddings.\n",
    "p_pos = pos_embed.init(next(rng), pos_idx)\n",
    "position_embeddings = pos_embed.apply(p_pos, next(rng), pos_idx)\n",
    "print(f\"Position embeddings for the input sequence ({position_embeddings.shape})\\n\", position_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to add our token and position embeddings. The result will be the input to the first encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the initial encoder block ((10, 12)):\n",
      " [[ 2.3875728   1.188903   -1.1773882  -0.7532333  -0.37383646  1.2518513\n",
      "  -1.8256276   1.0217857   0.43663198 -0.40435705 -1.5907483   0.20057444]\n",
      " [ 1.1742429   1.5605959   0.22915557 -0.91399044 -1.4239795   0.24587026\n",
      "   1.5415986  -0.62952036 -2.0900273   0.4898451   1.138324    1.2503755 ]\n",
      " [ 2.077025   -0.45192435 -1.2315928  -0.44749898 -1.6272397  -2.1980324\n",
      "   0.59294367  0.27596682 -2.0283124  -0.8467122  -1.1187582   2.2855055 ]\n",
      " [ 1.5228868  -1.6452489  -1.01801     1.9459455   1.8848829  -0.65995204\n",
      "  -1.0395948   0.25612932 -0.15149683 -1.6934438  -1.6677804   0.50006104]\n",
      " [-0.38906306 -1.2789508   0.81301105  1.539337   -0.92383456 -1.6627362\n",
      "   1.3612945  -0.27631974 -0.7774732   0.43737894 -0.57439363 -0.25378183]\n",
      " [ 0.6796096  -0.96626437  0.96006966  0.8770147  -1.2510277   1.555322\n",
      "  -1.786499   -1.4562206   1.7187164  -0.60997295 -0.5567626   2.9428585 ]\n",
      " [ 0.79413486  0.76836896  2.2889056  -1.7327032   0.10422492  0.18100125\n",
      "  -0.21510315 -1.0490484   0.10151603  2.1421413  -0.08253595 -0.37034762]\n",
      " [-2.0300722   2.62213     2.6990445   1.0885495   0.89762807 -0.3629582\n",
      "   1.0872806  -2.5827048  -1.9934764  -0.32899344 -2.07869     1.4755245 ]\n",
      " [ 1.3952996   1.5134963   0.8305602  -2.0290613  -2.1862202   0.34195146\n",
      "  -0.10297754 -0.34892735  1.3211385  -0.80638003  1.5583507   0.47934645]\n",
      " [-2.4078722   2.0596533   0.15907894 -1.7040358   0.34906748 -1.2572736\n",
      "  -2.0323346  -1.9278641   0.02901447  2.4834285  -0.20854515 -0.04685107]]\n"
     ]
    }
   ],
   "source": [
    "input = token_embeddings + position_embeddings\n",
    "print(f\"Input to the initial encoder block ({input.shape}):\\n\", input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an encoder block and a way to embed our tokens with position information, we can create the **encoder** itself.<br>\n",
    "\n",
    "Given a batch of vectorized sequences, the encoder creates positional embeddings, runs them through its encoder blocks, and returns contextualized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(hk.Module):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size,\n",
    "                max_seq_len, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_embed = hk.Embed(src_vocab_size, self.d_model)\n",
    "        self.pos_embed = hk.Embed(max_seq_len, self.d_model)\n",
    "\n",
    "        # The original Attention Is All You Need paper applied dropout to the\n",
    "        # input before feeding it to the first encoder block.\n",
    "        # self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Create encoder blocks.\n",
    "        self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) \n",
    "            for _ in range(num_blocks)]\n",
    "    \n",
    "    def __call__(self, input, training, mask):\n",
    "        token_embeds = self.token_embed(input)\n",
    "\n",
    "        # Generate position indices for a batch of input sequences.\n",
    "        num_pos = input.shape[0] * self.max_seq_len\n",
    "        pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
    "        pos_idx = np.reshape(pos_idx, input.shape)\n",
    "        pos_embeds = self.pos_embed(pos_idx)\n",
    "\n",
    "        x = hk.dropout(next(rng), self.dropout_rate, token_embeds + pos_embeds) #, training=training)\n",
    "\n",
    "        # Run input through successive encoder blocks.\n",
    "        for block in self.blocks:\n",
    "            x, weights = block(x, training, mask)\n",
    "\n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're wondering about this code block here:\n",
    "\n",
    "\n",
    "```\n",
    "num_pos = input.shape[0] * self.max_seq_len\n",
    "pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
    "pos_idx = np.reshape(pos_idx, input.shape)\n",
    "pos_embeds = self.pos_embed(pos_idx)\n",
    "```\n",
    "\n",
    "\n",
    "This generates positional embeddings for a *batch* of input sequences. Suppose this was our batch of input sequences to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "[[6772 8373 2283 4289 6732  120 8717  222 3676 4332]\n",
      " [3632 9768 9114 7326 7557 2544 6432   62 6490 2023]\n",
      " [8342 8595 2437 2852  127 1508 9989 2508 7322 9299]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of 3 sequences, each of length 10 (10 is also the \n",
    "# maximum sequence length in this case).\n",
    "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
    "print(seqs.shape)\n",
    "print(seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to retrieve a positional embedding for every element in this batch. The first step is to create the respective positional ids..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
    "print(pos_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then reshape them to match the input batch dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "[[0 1 2 3 4 5 6 7 8 9]\n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      " [0 1 2 3 4 5 6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "pos_ids = np.reshape(pos_ids, (3, 10))\n",
    "print(pos_ids.shape)\n",
    "print(pos_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve position embeddings for every token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 12)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed.apply(p_pos, next(rng), pos_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our encoder on a batch of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?'],\n",
       " ['▁mass', '▁hy', 'ster', 'ia', '▁over', '▁l', 'ister', 'ia', '.'],\n",
       " ['▁i', '▁a', 'in', \"'\", 't', '▁no', '▁circle', '▁back', '▁girl', '.']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch = [\n",
    "    \"Where can I find a pizzeria?\",\n",
    "    \"Mass hysteria over listeria.\",\n",
    "    \"I ain't no circle back girl.\"\n",
    "]\n",
    "\n",
    "bpemb_en.encode(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized inputs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[571, 280, 386, 1934, 4, 24, 248, 4339, 177, 9967],\n",
       " [1535, 1354, 1238, 177, 380, 43, 871, 177, 9935],\n",
       " [386, 4, 6, 9937, 9915, 467, 5410, 810, 3692, 9935]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs = bpemb_en.encode_ids(input_batch)\n",
    "print(\"Vectorized inputs:\")\n",
    "input_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the input sequences aren't the same length in this batch. In this case, we need to pad them out so that they are. If you're unfamiliar with why, refer to the notebook on Recurrent Neural Networks:<br>\n",
    "https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_recurrent_neural_networks.ipynb<br>\n",
    "\n",
    "We'll do this using *pad_sequences*.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the encoder:\n",
      "(3, 10)\n",
      "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
      " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
      " [ 386    4    6 9937 9915  467 5410  810 3692 9935]]\n"
     ]
    }
   ],
   "source": [
    "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
    "print(\"Input to the encoder:\")\n",
    "print(padded_input_seqs.shape)\n",
    "print(padded_input_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our input now has padding, now's a good time to cover **masking**.\n",
    "<br>\n",
    "\n",
    "So given a mask, wherever there's a mask position set to 0, the corresponding position in the attention scores will be set to *-inf*. The resulting attention weight for the position will then be zero and no attending will occur for that position.\n",
    "<br>\n",
    "\n",
    "In the slides, we covered *look-ahead* masks for the decoder to prevent it from attending to future tokens, but we also need masks for padding.\n",
    "<br>\n",
    "\n",
    "In total, there are three masks involved:\n",
    "1. The *encoder mask* to mask out any padding in the encoder sequences.\n",
    "\n",
    "2. The *decoder mask* which is used in the decoder's **first** multi-head self-attention layer. It's a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask.\n",
    "\n",
    "3. The *memory mask* which is used in the decoder's **second** multi-head self-attention layer. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding. In practice, 1 and 3 are often the same.\n",
    "\n",
    "The *scaled_dot_product_attention* function has this line:\n",
    "```\n",
    "  if mask is not None:\n",
    "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an encoder mask for our batch of input sequences.<br>\n",
    "\n",
    "Wherever there's padding, we want the mask position set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
      " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
      " [ 386    4    6 9937 9915  467 5410  810 3692 9935]] \n",
      "\n",
      "Encoder mask:\n",
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 0]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "enc_mask = np.where(padded_input_seqs, 1, 0)\n",
    "print(\"Input:\")\n",
    "print(padded_input_seqs, '\\n')\n",
    "print(\"Encoder mask:\")\n",
    "print(enc_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the dimension of the attention matrix (for this example) is going to be:<br>\n",
    "*(batch size, number of heads, query size, key size)*<br>\n",
    "(3, 3, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to expand the mask dimensions like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "       [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]],\n",
       "\n",
       "\n",
       "       [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mask = enc_mask[:, None, None, :]\n",
    "enc_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, the encoder mask will now be *broadcasted*.<br>\n",
    "https://www.tensorflow.org/xla/broadcasting\n",
    "\n",
    "Now we can declare an encoder and pass it batches of vectorized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_encoder(padded_input_seqs, training, mask):\n",
    "    num_encoder_blocks = 6\n",
    "\n",
    "    # d_model is the embedding dimension used throughout.\n",
    "    d_model = 12\n",
    "\n",
    "    num_heads = 3\n",
    "\n",
    "    # Feed-forward network hidden dimension width.\n",
    "    ffn_hidden_dim = 48\n",
    "\n",
    "    src_vocab_size = bpemb_vocab_size\n",
    "    max_input_seq_len = padded_input_seqs.shape[1]\n",
    "\n",
    "    return Encoder(\n",
    "        num_encoder_blocks,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        ffn_hidden_dim,\n",
    "        src_vocab_size,\n",
    "        max_input_seq_len)(padded_input_seqs, training, mask)\n",
    "    \n",
    "encoder = hk.transform(build_encoder)\n",
    "params_enc2 = encoder.init(next(rng), padded_input_seqs, training=True, mask=enc_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass our input sequences and mask to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output (3, 10, 12):\n",
      "[[[ 1.41915858e+00 -3.07213008e-01  1.10027516e+00 -4.75663185e-01\n",
      "   -1.30171478e-01  5.72483182e-01  5.82186282e-01  4.49498504e-01\n",
      "   -1.79342103e+00  1.08983183e+00 -1.35931861e+00 -1.14764631e+00]\n",
      "  [ 1.74795461e+00 -1.60028651e-01  7.03835070e-01 -3.66131395e-01\n",
      "   -6.39971673e-01  6.04712069e-01 -8.59635890e-01  6.99656904e-01\n",
      "   -3.45304072e-01  1.41395795e+00 -1.04601347e+00 -1.75303125e+00]\n",
      "  [ 1.19105601e+00  6.01129889e-01  3.93038094e-02  4.87445354e-01\n",
      "   -6.61754668e-01  8.12084973e-01  1.74837068e-01  3.17051888e-01\n",
      "   -1.31270194e+00  1.46537018e+00 -1.42688251e+00 -1.68694019e+00]\n",
      "  [ 8.62344146e-01  2.49831170e-01  9.24054027e-01  5.88451862e-01\n",
      "   -8.11926842e-01  3.88371974e-01  6.01497246e-03  1.65743917e-01\n",
      "   -8.38069797e-01  1.54521656e+00 -7.26764679e-01 -2.35326767e+00]\n",
      "  [-1.23476624e+00  6.44548893e-01  1.37193394e+00  3.45725179e-01\n",
      "   -1.04841232e+00  1.03857934e+00 -4.68277752e-01  3.51037592e-01\n",
      "    2.77033776e-01  1.38381481e+00 -1.28086472e+00 -1.38035274e+00]\n",
      "  [ 1.99904191e+00  5.36261857e-01  1.54226452e-01  2.97004580e-01\n",
      "   -6.50856555e-01 -5.59069216e-01 -4.86216284e-02  4.18274313e-01\n",
      "   -7.84521043e-01  1.42748904e+00 -1.12298191e+00 -1.66624773e+00]\n",
      "  [ 1.18418622e+00 -6.03542864e-01  1.22787833e+00  5.24501801e-01\n",
      "    6.06352128e-02  5.37778676e-01  1.00417161e+00 -1.22837532e+00\n",
      "   -1.22734547e+00  9.75185931e-01 -9.36617911e-01 -1.51845670e+00]\n",
      "  [ 7.41433978e-01 -2.59351671e-01  1.46950316e+00  2.60113418e-01\n",
      "   -2.40967393e-01  7.32469618e-01  1.07961929e+00 -5.23183525e-01\n",
      "   -1.35627723e+00  8.84174824e-01 -1.94850457e+00 -8.39030564e-01]\n",
      "  [ 1.13554311e+00 -4.01984006e-01  4.66275483e-01  2.69014448e-01\n",
      "    2.22416773e-01  1.50326979e+00  1.28159940e-01 -9.29250062e-01\n",
      "    3.82067353e-01  7.64415920e-01 -1.96230197e+00 -1.57762730e+00]\n",
      "  [ 1.43952036e+00 -7.87486196e-01  4.04563904e-01  5.40251434e-02\n",
      "   -7.45131671e-01  5.85072041e-01  5.42651474e-01  7.17564821e-01\n",
      "   -1.06675076e+00  1.52177119e+00 -8.70829642e-01 -1.79497039e+00]]\n",
      "\n",
      " [[ 1.37682784e+00  5.13871193e-01  2.50644863e-01 -1.14199959e-01\n",
      "   -6.46550536e-01 -3.04126114e-01 -2.14828873e+00  9.54245090e-01\n",
      "   -5.46448767e-01  1.67472184e+00 -7.44114220e-01 -2.66582340e-01]\n",
      "  [ 4.43413407e-01 -5.66404343e-01  2.41939068e+00 -8.38486373e-01\n",
      "   -1.55894518e+00 -7.56959841e-02  4.57675219e-01 -1.45464502e-02\n",
      "    3.07912111e-01  8.07220876e-01 -1.52501732e-01 -1.22903228e+00]\n",
      "  [ 1.51656497e+00 -1.24259204e-01 -3.96894425e-01 -7.99292885e-03\n",
      "   -1.24678202e-01 -1.04597576e-01 -1.52160919e+00 -7.54768252e-01\n",
      "   -7.23124146e-01  2.08901167e+00  1.00271416e+00 -8.50366652e-01]\n",
      "  [-3.26918364e-01  1.11219573e+00  1.80259183e-01 -1.40222239e+00\n",
      "   -2.14260265e-01  4.17336196e-01 -5.16615152e-01  7.75115788e-01\n",
      "   -1.37321436e+00  1.32380262e-01  2.17753148e+00 -9.61588085e-01]\n",
      "  [ 1.27001739e+00  8.47986579e-01 -7.74844885e-02  7.50942707e-01\n",
      "   -8.63345087e-01 -3.95706594e-01 -1.39296079e+00  3.61739248e-01\n",
      "   -1.47029310e-01  1.48771083e+00  1.26363754e-01 -1.96823418e+00]\n",
      "  [ 2.39460230e+00  9.38077211e-01 -2.17053175e-01 -2.73465902e-01\n",
      "   -7.02928066e-01  7.72061348e-02 -1.99681652e+00 -5.22771299e-01\n",
      "   -2.11475834e-01  6.65584862e-01 -3.00096236e-02 -1.20950192e-01]\n",
      "  [ 4.29917753e-01 -1.25251567e+00  1.28737164e+00  1.15622973e+00\n",
      "   -6.47044003e-01 -1.03011167e+00  3.64905775e-01 -1.08503520e+00\n",
      "   -4.29902583e-01  1.69405425e+00  5.58615446e-01 -1.04648542e+00]\n",
      "  [ 1.20671058e+00  8.64599347e-01 -3.47624660e-01 -9.34445783e-02\n",
      "   -1.93668410e-01 -4.73291874e-01 -1.85866785e+00 -1.16135526e+00\n",
      "    1.88727236e+00  8.33430469e-01 -8.21202472e-02 -5.81840336e-01]\n",
      "  [ 5.39917827e-01  1.09298170e+00  3.69586289e-01  7.49796748e-01\n",
      "   -7.49477804e-01  3.21743935e-01 -1.74360478e+00  9.05122310e-02\n",
      "   -1.13673484e+00  1.60577559e+00  3.18668365e-01 -1.45916522e+00]\n",
      "  [ 1.92467320e+00  3.25027913e-01  4.16922867e-01  3.47092420e-01\n",
      "   -1.29243386e+00  5.20911574e-01 -1.39116549e+00  1.40791833e-01\n",
      "   -7.24692464e-01  1.40210724e+00 -5.95408976e-01 -1.07382631e+00]]\n",
      "\n",
      " [[ 1.24925983e+00 -2.92739630e-01  9.81318235e-01  2.33471051e-01\n",
      "   -7.21351326e-01  7.54244864e-01  8.28805007e-03  8.30815613e-01\n",
      "   -1.17101634e+00  1.00002158e+00 -2.16419554e+00 -7.08116829e-01]\n",
      "  [ 3.19401920e-01  2.09589791e+00  7.87125468e-01 -3.81095946e-01\n",
      "   -1.42559528e+00  2.29003981e-01  1.23277247e-01  6.96851373e-01\n",
      "   -9.84544009e-02  4.49316740e-01 -1.26543927e+00 -1.53028941e+00]\n",
      "  [ 1.07772946e+00  7.49490380e-01 -1.89253747e-01  5.24076641e-01\n",
      "    3.27900082e-01  8.67047668e-01 -5.77877581e-01  3.02404553e-01\n",
      "   -1.16656661e+00  1.36052442e+00 -1.86709821e+00 -1.40837610e+00]\n",
      "  [ 3.96578640e-01  1.30357683e+00 -1.78717181e-01 -4.69675586e-02\n",
      "    2.14336533e-02  5.93189061e-01 -1.02084112e+00  4.93720591e-01\n",
      "   -1.38891900e+00  1.53979981e+00  3.06314051e-01 -2.01916766e+00]\n",
      "  [ 1.03678930e+00 -3.90168428e-01 -1.18718490e-01  1.72556138e+00\n",
      "    8.53356440e-03  7.90401816e-01  5.58290362e-01  6.78012669e-02\n",
      "   -1.72293413e+00  6.53436124e-01 -1.44773090e+00 -1.16126227e+00]\n",
      "  [ 8.56559277e-01 -1.83414117e-01  1.14385879e+00  8.77217829e-01\n",
      "   -8.86036456e-02  5.94663024e-01  2.01336160e-01  5.74654996e-01\n",
      "   -5.52573442e-01  3.59915614e-01 -2.53586888e+00 -1.24774635e+00]\n",
      "  [ 1.40454113e+00 -1.10381234e+00  7.22850561e-01  4.70605820e-01\n",
      "    8.29048529e-02  1.02953678e-02 -4.85338122e-01  9.83098745e-01\n",
      "   -8.06716800e-01  1.23744690e+00 -3.89359772e-01 -2.12651658e+00]\n",
      "  [-4.67467070e-01  1.68246543e+00 -4.78907526e-01  4.06644464e-01\n",
      "   -1.39903709e-01  1.46987721e-01  2.92954594e-01  2.08255455e-01\n",
      "    4.72467504e-02  1.65811396e+00 -1.70382595e+00 -1.65256417e+00]\n",
      "  [-5.42176425e-01  6.02978528e-01  5.65636575e-01  1.76249969e+00\n",
      "   -2.23812647e-03  7.16669858e-01  2.94716895e-01 -7.16442645e-01\n",
      "   -9.39824164e-01  1.19824719e+00 -1.75383353e+00 -1.18623412e+00]\n",
      "  [ 1.21977293e+00  8.14243257e-01 -2.52244622e-01  7.48212576e-01\n",
      "   -4.51871574e-01  1.02586615e+00 -5.82628287e-02 -2.76019275e-01\n",
      "   -9.81213748e-01  1.34926379e+00 -1.87168908e+00 -1.26605785e+00]]]\n"
     ]
    }
   ],
   "source": [
    "encoder_output, attn_weights = encoder.apply(params_enc2, next(rng), \n",
    "                                             padded_input_seqs, training=True, mask=enc_mask)\n",
    "print(f\"Encoder output {encoder_output.shape}:\")\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the **Decoder Block**. Everything we did to create the **encoder** block applies here. The major differences are that the **Decoder Block** has:\n",
    "1. a **Multi-Head Cross-Attention** layer which uses the encoder's outputs as the keys and values.\n",
    "\n",
    "2. an extra skip/residual connection along with an extra layer normalization step.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WVT4SX49bnta4uscOTF4xrsxFI4PbPER\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(hk.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = feed_forward_network(d_model, hidden_dim)\n",
    "\n",
    "        self.dropout1 = partial(hk.dropout, rng=next(rng), rate=dropout_rate)\n",
    "        self.dropout2 = partial(hk.dropout, rng=next(rng), rate=dropout_rate)\n",
    "        self.dropout3 = partial(hk.dropout, rng=next(rng), rate=dropout_rate)\n",
    "\n",
    "        self.layernorm1 = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "        self.layernorm2 = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "        self.layernorm3 = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "    \n",
    "    # Note the decoder block takes two masks. One for the first MHSA, another\n",
    "    # for the second MHSA.\n",
    "    def __call__(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
    "        mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
    "        if training:\n",
    "            mhsa_output1 = self.dropout1(x=mhsa_output1)\n",
    "        mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
    "\n",
    "        mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output, \n",
    "                                                encoder_output, \n",
    "                                                memory_mask)\n",
    "        if training:\n",
    "            mhsa_output2 = self.dropout2(x=mhsa_output2)\n",
    "        mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)\n",
    "\n",
    "        ffn_output = self.ffn(mhsa_output2)\n",
    "        if training:\n",
    "            ffn_output = self.dropout3(x=ffn_output)\n",
    "        output = self.layernorm3(ffn_output + mhsa_output2)\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is almost the same as the encoder except it takes the encoder's output as part of its input, and it takes two masks: the decoder mask and memory mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
    "                max_seq_len, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_embed = hk.Embed(target_vocab_size, self.d_model)\n",
    "        self.pos_embed = hk.Embed(max_seq_len, self.d_model)\n",
    "\n",
    "        self.dropout = partial(hk.dropout, rng=next(rng), rate=dropout_rate)\n",
    "\n",
    "        self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
    "\n",
    "    def __call__(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
    "        token_embeds = self.token_embed(target)\n",
    "\n",
    "        # Generate position indices.\n",
    "        num_pos = target.shape[0] * self.max_seq_len\n",
    "        pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
    "        pos_idx = np.reshape(pos_idx, target.shape)\n",
    "\n",
    "        pos_embeds = self.pos_embed(pos_idx)\n",
    "\n",
    "        if training:\n",
    "            x = self.dropout(x=token_embeds + pos_embeds)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
    "\n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try the decoder, let's cover the masks involved. The decoder takes two masks:\n",
    "\n",
    "The *decoder mask* which is a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask. This mask is used in the decoder's **first** multi-head self-attention layer.\n",
    "\n",
    "The *memory mask* which is used in the decoder's **second** multi-head self-attention. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose this is our batch of vectorized target *input* sequences for the decoder. These values are just made up.<br>\n",
    "\n",
    "**Note**: If you need a refresher on how to prepare target input and output sequences for the decoder, refer to the [seq2seq notebook](https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_seq2seq_and_attention.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made up values.\n",
    "target_input_seqs = [\n",
    "    [1, 652, 723, 123, 62],\n",
    "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
    "    [1, 2369, 1259, 125, 486],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the encoder input sequences, we need to pad out this batch so that all sequences within it are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded target inputs to the decoder:\n",
      "(3, 8)\n",
      "[[   1  652  723  123   62    0    0    0]\n",
      " [   1   25   98  129  248  215  359  249]\n",
      " [   1 2369 1259  125  486    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
    "print(\"Padded target inputs to the decoder:\")\n",
    "print(padded_target_input_seqs.shape)\n",
    "print(padded_target_input_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the padding mask the same way we did for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "dec_padding_mask = np.where(padded_target_input_seqs, 1, 0).astype(jnp.float32)\n",
    "dec_padding_mask = dec_padding_mask[:, None, None, :]\n",
    "print(dec_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we covered in the slides, the look-ahead mask is a diagonal where the lower half are 1s and the upper half are zeros. This is easy to create using the *band_part* method:<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/linalg/band_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
    "look_ahead_mask = jnp.tril(np.ones((target_input_seq_len, \n",
    "                                    target_input_seq_len)))\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the decoder mask, we just need to combine the padding and look-ahead masks. Note how the columns of the resulting decoder mask are all zero for padding positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decoder mask:\n",
      "[[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "dec_mask = jnp.minimum(dec_padding_mask, look_ahead_mask)\n",
    "print(\"The decoder mask:\")\n",
    "print(dec_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now declare a decoder and pass it everything it needs. In our case, the *memory* mask is the same as the *encoder* mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output (3, 8, 12):\n",
      "[[[-1.6127183e+00 -1.8213560e-01  9.1229409e-01  3.8112000e-01\n",
      "    7.0431960e-01  2.9676458e-01 -1.6670176e-01  9.1213053e-01\n",
      "    1.8696641e+00 -7.3093414e-01 -1.3832730e+00 -1.0005306e+00]\n",
      "  [-6.9025362e-01  1.3053727e+00  1.8864352e+00 -2.3502527e-01\n",
      "    5.5938315e-01 -4.6635270e-01 -8.7049794e-01  7.3003548e-01\n",
      "    8.5302389e-01 -1.1479421e+00 -1.4559454e+00 -4.6823293e-01]\n",
      "  [-1.2876213e+00 -4.4866640e-02  1.6226444e+00  1.1253241e+00\n",
      "    4.6261898e-01  1.1346519e+00 -8.4329134e-01  6.9205856e-01\n",
      "    3.4550142e-01 -1.4324353e+00 -9.2756349e-01 -8.4702140e-01]\n",
      "  [ 9.5239811e-02  7.6274380e-02  1.6914363e+00  9.3968374e-01\n",
      "    7.9112315e-01  5.6380188e-01 -7.2576153e-01 -2.5724638e-01\n",
      "    6.4836198e-01 -1.1518744e+00 -2.1691794e+00 -5.0185937e-01]\n",
      "  [-6.6998619e-01 -1.9678682e-01  1.2874272e+00  1.3559465e+00\n",
      "    2.8068951e-01 -4.4451751e-02 -7.4557924e-01  5.5065371e-02\n",
      "    1.6681513e+00 -5.4560304e-01 -2.0309603e+00 -4.1391230e-01]\n",
      "  [-9.8530191e-01 -8.0644721e-01  1.6441455e+00  1.2989568e+00\n",
      "    3.5766050e-01  1.1291432e+00 -3.8329729e-01  9.1773474e-01\n",
      "    1.1200096e-02 -9.4969380e-01 -1.4321160e+00 -8.0198449e-01]\n",
      "  [-9.6138340e-01 -6.5659916e-01  1.5868551e+00  6.2267768e-01\n",
      "    7.4717468e-01  3.2336536e-01 -4.6678171e-01  1.5118130e+00\n",
      "    4.6625635e-01 -9.7563636e-01 -1.8017428e+00 -3.9599851e-01]\n",
      "  [-7.6151854e-01 -5.3193718e-01  1.4670361e+00  9.2366719e-01\n",
      "    5.9787905e-01  4.9408120e-01 -4.4885257e-01  9.1885501e-01\n",
      "    1.1051788e+00 -1.1476820e+00 -1.8193069e+00 -7.9740083e-01]]\n",
      "\n",
      " [[-1.0186899e+00 -3.3065614e-01 -9.6140367e-01  4.3170813e-02\n",
      "    2.3262693e-01  2.3157918e+00  1.5646578e-01  2.3402020e-01\n",
      "    8.9104337e-01 -4.5886138e-01  6.3505769e-01 -1.7385656e+00]\n",
      "  [-9.8357761e-01 -8.8648975e-01  7.0458919e-02  3.2500380e-01\n",
      "   -7.6285625e-01  1.6014149e+00 -5.4406786e-01 -1.1093134e+00\n",
      "    1.8799855e+00  1.8937458e-01  1.0766114e+00 -8.5654473e-01]\n",
      "  [-9.9327099e-01 -8.7652028e-01 -6.3639319e-01  5.9475631e-01\n",
      "   -1.6485314e-01  1.6153479e+00 -2.3422715e-01 -1.9576971e-01\n",
      "    4.5523578e-01  1.4471275e+00  8.9850038e-01 -1.9099334e+00]\n",
      "  [-9.2067164e-01 -7.7179617e-01 -8.5906309e-01  2.1554217e-01\n",
      "   -2.8940691e-03  1.9566612e+00  1.7256117e-01 -4.4037226e-01\n",
      "    4.6502537e-01  1.3624530e+00  6.2663317e-01 -1.8040795e+00]\n",
      "  [-6.9900930e-01 -4.2554969e-01 -1.7618103e+00  5.5329186e-01\n",
      "   -1.6423137e-01  1.4476959e+00 -3.0008140e-01 -6.6114491e-01\n",
      "    1.2630733e+00  1.4338851e+00  4.8861131e-01 -1.1747310e+00]\n",
      "  [-1.5134935e+00 -1.3908870e+00 -1.7813323e-03  3.3944294e-02\n",
      "   -3.8614058e-01  2.1143208e+00  1.5787974e-01 -6.5660879e-02\n",
      "    6.4449644e-01  4.2278436e-01  1.1171169e+00 -1.1325794e+00]\n",
      "  [-5.8316672e-01 -9.8009229e-01 -1.0950973e+00  4.9354777e-01\n",
      "    8.5813604e-02  1.9217579e+00 -3.7236395e-01  1.8691401e-01\n",
      "    1.3079177e+00  4.5489430e-01  3.9816189e-01 -1.8182868e+00]\n",
      "  [-8.8148266e-01 -8.8130307e-01 -2.5396520e-01  2.1397853e-01\n",
      "   -3.4897181e-01  1.4477062e+00 -4.4909182e-01 -3.7460265e-01\n",
      "    1.0202018e+00  1.7974086e+00  5.1110297e-01 -1.8009812e+00]]\n",
      "\n",
      " [[-8.6847019e-01 -7.3515236e-01  5.7664531e-01  1.8520094e+00\n",
      "   -2.3025465e-01  4.4222811e-01 -7.4983060e-01  1.4729122e+00\n",
      "    1.0173811e+00 -7.3535776e-01 -1.4093328e+00 -6.3277745e-01]\n",
      "  [-4.6988189e-01 -6.7563367e-01  8.2006820e-02  1.8447253e+00\n",
      "   -2.1987185e-01  5.4742533e-01 -1.1353949e+00  1.1926858e+00\n",
      "    1.4348625e+00 -4.1288099e-01 -1.4332705e+00 -7.5477219e-01]\n",
      "  [-9.1393942e-01 -2.7197760e-01  7.7539736e-01  1.0027881e+00\n",
      "   -1.7397417e-01 -4.4441089e-01 -4.5097971e-01  1.6836487e+00\n",
      "    1.6582359e+00 -1.0012931e+00 -1.5356413e+00 -3.2785395e-01]\n",
      "  [-1.9463779e+00  2.2089878e-02  5.3625709e-01  1.6251382e+00\n",
      "   -5.2030778e-01 -2.5089771e-01  1.0115993e-01  1.1277897e+00\n",
      "    1.4051962e+00 -3.2566115e-01 -9.6369791e-01 -8.1068885e-01]\n",
      "  [-2.1795040e-01 -6.1764890e-01  2.0276760e-01  1.1789851e+00\n",
      "    2.0593652e-01 -2.7441999e-01 -8.5533500e-01  1.4789931e+00\n",
      "    1.9112898e+00 -1.4857032e+00 -9.5963472e-01 -5.6727982e-01]\n",
      "  [-1.0105145e+00 -1.0386373e+00  9.9222660e-01  1.1096495e+00\n",
      "   -5.1760662e-01  3.3822420e-01 -8.2192719e-01  1.5150721e+00\n",
      "    1.5576130e+00 -1.1088662e+00 -7.9023820e-01 -2.2499578e-01]\n",
      "  [-1.3299749e+00 -7.4685234e-01  8.2885975e-01  1.6686374e+00\n",
      "   -8.8311440e-01  1.2514403e-01 -8.0754474e-02  1.6416751e+00\n",
      "    7.4466866e-01 -1.0337632e-01 -1.3799932e+00 -4.8491970e-01]\n",
      "  [-1.3005912e+00 -8.6881131e-01  1.1123693e+00  3.6504000e-01\n",
      "    1.4364821e-01 -6.5720417e-02 -4.8929909e-01  1.1883377e+00\n",
      "    2.0838542e+00 -1.1585916e+00 -8.9965725e-01 -1.1057827e-01]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_decoder(encoder_output, target, training, decoder_mask, memory_mask):\n",
    "    return Decoder(6, 12, 3, 48, 10000, 8)(\n",
    "        encoder_output, target, training, decoder_mask, memory_mask)\n",
    "    \n",
    "decoder = hk.transform(build_decoder)\n",
    "p_dec = decoder.init(next(rng), encoder_output, padded_target_input_seqs, \n",
    "                            True, dec_mask, enc_mask)\n",
    "    \n",
    "decoder_output, _ = decoder.apply(p_dec, next(rng), encoder_output, padded_target_input_seqs, \n",
    "                            True, dec_mask, enc_mask)\n",
    "print(f\"Decoder output {decoder_output.shape}:\")\n",
    "print(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "We now have all the pieces to build the **Transformer** itself, and it's pretty simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
    "                target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size, \n",
    "                            max_input_len, dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
    "                            max_target_len, dropout_rate)\n",
    "        \n",
    "        # The final dense layer to generate logits from the decoder output.\n",
    "        self.output_layer = Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
    "            decoder_mask, memory_mask):\n",
    "        encoder_output, encoder_attn_weights = self.encoder(input_seqs, \n",
    "                                                            training, encoder_mask)\n",
    "\n",
    "        decoder_output, decoder_attn_weights = self.decoder(encoder_output, \n",
    "                                                            target_input_seqs, training,\n",
    "                                                            decoder_mask, memory_mask)\n",
    "\n",
    "        return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer output (3, 8, 7000):\n",
      "[[[0.         0.02891155 1.180111   ... 0.70502913 0.2725506  0.16392171]\n",
      "  [0.         0.         0.         ... 0.71455824 0.13397743 0.82286805]\n",
      "  [0.2733927  0.776259   0.32521093 ... 0.         0.         0.58686864]\n",
      "  ...\n",
      "  [0.13541673 0.73615843 0.6665622  ... 0.37348574 0.         1.038203  ]\n",
      "  [0.24842629 0.7897228  0.6983148  ... 0.26553378 0.         0.7705957 ]\n",
      "  [0.1324051  0.85790825 0.50942504 ... 0.37143385 0.         1.234074  ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.50757706 0.82317847 0.6037749 ]\n",
      "  [0.         0.         0.         ... 0.         1.2882888  0.5017553 ]\n",
      "  [0.         0.         0.02330121 ... 0.         1.5237322  0.10844213]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.39341128 0.30955768 0.02940781]\n",
      "  [0.         0.         0.         ... 0.45490438 0.         0.05493074]\n",
      "  [0.         0.         0.         ... 0.05803161 0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.5005577  ... 0.         0.         0.        ]\n",
      "  [0.08249459 0.         0.         ... 0.         0.         0.13180482]\n",
      "  [0.         0.         0.         ... 0.         0.03838204 0.05386013]\n",
      "  ...\n",
      "  [0.         0.         0.6800021  ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "def build_transformer(input_seqs, target_input_seqs, \n",
    "                      training, encoder_mask, decoder_mask, memory_mask):\n",
    "    return Transformer(\n",
    "        num_blocks = 6,\n",
    "        d_model = 12,\n",
    "        num_heads = 3,\n",
    "        hidden_dim = 48,\n",
    "        source_vocab_size = bpemb_vocab_size,\n",
    "        target_vocab_size = 7000, # made-up target vocab size.\n",
    "        max_input_len = padded_input_seqs.shape[1],\n",
    "        max_target_len = padded_target_input_seqs.shape[1])(input_seqs, target_input_seqs, \n",
    "                      training, encoder_mask, decoder_mask, memory_mask)\n",
    "\n",
    "transformer = hk.transform(build_transformer)\n",
    "params = transformer.init(next(rng), padded_input_seqs, \n",
    "                          padded_target_input_seqs, True, \n",
    "                          enc_mask, dec_mask, memory_mask=enc_mask)\n",
    "\n",
    "transformer_output, _, _ = transformer.apply(params, next(rng), padded_input_seqs, \n",
    "                                       padded_target_input_seqs, True, \n",
    "                                       enc_mask, dec_mask, memory_mask=enc_mask)\n",
    "print(f\"Transformer output {transformer_output.shape}:\")\n",
    "print(transformer_output) # If training, we would use this output to calculate losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the whole original transformer from scratch. From here, if you want to train this transformer, you can use the same approach we used when we built the translation model with attention in the [seq2seq notebook](https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_seq2seq_and_attention.ipynb#scrollTo=x8Ef_eWXjWMn&line=3&uniqifier=1). Remember to use a learning rate warmup (Refer to the paper for more information on this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to know how these models work under the hood, but to train our own transformer to get impressive results is expensive. Both in terms of compute and data.<br>\n",
    "\n",
    "Fortunately, there's a zoo of **pretrained** transformer models we can use. We'll explore that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training and Transfer Learning with Hugging Face and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**<br>\n",
    "Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n",
    "<br><br>\n",
    "Also, if you're running this in the cloud rather than a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore pre-training and transfer learning using the **Transformers** library from [Hugging Face](https://huggingface.co/). **Transformers** is an API and toolkit to download pre-trained models and further train them as needed. <br>\n",
    "\n",
    "We'll start with the **pipelines** module which abstracts a lot of operations such as tokenization, vectorization, inference, etc.<br>\n",
    "\n",
    "With **Transformers pipelines**, we can just feed text input and get text output. And there are **pipelines** for common tasks including classification, NER, summarization, etc.<br>\n",
    "https://huggingface.co/docs/transformers/index<br>\n",
    "https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines\n",
    "\n",
    "To get started, we'll need to install **Transformers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m224.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m226.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Installing collected packages: tokenizers, safetensors, fsspec, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-12.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m189.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m214.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m372.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m452.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m410.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m192.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/wadh6511/Kode/env_evo/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 dill-0.3.6 frozenlist-1.4.0 multidict-6.0.4 multiprocess-0.70.14 pyarrow-12.0.1 xxhash-3.2.0 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import TFAutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
