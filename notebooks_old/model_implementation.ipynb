{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union, Optional, Dict, List, Callable, Tuple\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     ESMLearnedPositionalEmbeddings,\n\u001b[1;32m      8\u001b[0m     RobertaLMHead,\n\u001b[1;32m      9\u001b[0m     SelfAttentionBlock,\n\u001b[1;32m     10\u001b[0m     TokensDropout,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnucleotide_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m NucleotideTransformerConfig\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     AttentionMask,\n\u001b[1;32m     15\u001b[0m     Embedding,\n\u001b[1;32m     16\u001b[0m     Tokens,\n\u001b[1;32m     17\u001b[0m     TransformerOutput,\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import os\n",
    "import sys\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Union, Optional, Dict, List, Callable, Tuple\n",
    "\n",
    "\n",
    "if __package__ is None:\n",
    "\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "    __package__ = os.path.basename(module_path)\n",
    "\n",
    "root_dir = '..'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.layers import (\n",
    "    ESMLearnedPositionalEmbeddings,\n",
    "    RobertaLMHead,\n",
    "    SelfAttentionBlock,\n",
    "    TokensDropout,\n",
    ")\n",
    "from src.models.nucleotide_transformer import NucleotideTransformerConfig\n",
    "from src.models.types import (\n",
    "    AttentionMask,\n",
    "    Embedding,\n",
    "    Tokens,\n",
    "    TransformerOutput,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy_loss(x):\n",
    "    return 1\n",
    "\n",
    "def loss_func(x, y):\n",
    "    return cross_entropy_loss(x)\n",
    "\n",
    "def compute_grads(params, x, y, model):\n",
    "    logits = model.apply(params, None, x)\n",
    "    return loss_func(logits, y)\n",
    "\n",
    "def update(params, grads, lr = 0.1):\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - g * lr, params, grads)\n",
    "\n",
    "def train_step(params, x, y, model):\n",
    "    params_grads = jax.grad(compute_grads)(params, x, y, model)\n",
    "    return update(params, params_grads)\n",
    "\n",
    "def train(iterations, params, xdata, ydata, model):\n",
    "    for i in iterations:\n",
    "        x = next(xdata)\n",
    "        y = next(ydata)\n",
    "        params = train_step(params, x, y, model)\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OKConfig:\n",
    "\n",
    "    ok: int\n",
    "\n",
    "class OK(hk.module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: OKConfig,\n",
    "        name: Optional[str] = None,\n",
    "    ) -> None: \n",
    "        \n",
    "        self._config = config\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        # Layer declarations incl norms\n",
    "        \n",
    "        # Prcoess config if needed\n",
    "        \n",
    "        # Check config compatibility\n",
    "    \n",
    "    def _attention_block(self, layer_idx) -> SelfAttentionBlock:\n",
    "\n",
    "        return SelfAttentionBlock(  # type: ignore\n",
    "            num_heads=self._config.attention_heads,\n",
    "            embed_dim=self._config.embed_dim,\n",
    "            key_size=self._config.key_size,\n",
    "            ffn_embed_dim=self._config.ffn_embed_dim,\n",
    "            name=f\"attention_layer_{layer_idx}\",\n",
    "        )\n",
    "\n",
    "    @hk.transparent\n",
    "    def apply_attention_blocks(\n",
    "        self,\n",
    "        x: Embedding,\n",
    "        outs: Dict[str, Embedding],\n",
    "        attention_mask: Optional[AttentionMask] = None,\n",
    "    ) -> Tuple[Embedding, Dict[str, Embedding]]:\n",
    "        \"\"\"\n",
    "        Create the blocks of attention layers and applies them.\n",
    "\n",
    "        Args:\n",
    "            x: The sequence embedding.\n",
    "            outs: A dictionary to carry through the attention layers which stores the\n",
    "                intermediate sequence embedding and attention maps.\n",
    "            attention_mask: Attention mask of shape (batch_size, 1, seq_len, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            The output sequence embedding.\n",
    "            The optional intermediate results (embeddings of the layer and attention\n",
    "                weights).\n",
    "        \"\"\"\n",
    "\n",
    "        layers: List[Callable] = [\n",
    "            self._attention_block(layer_idx)\n",
    "            for layer_idx in range(self._config.num_layers)\n",
    "        ]\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            output = layer(\n",
    "                x=x,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            x = output[\"embeddings\"]\n",
    "            # Save intermediate embeddings if needed\n",
    "            if (layer_idx + 1) in self._config.embeddings_layers_to_save:\n",
    "                outs[f\"embeddings_{(layer_idx+1)}\"] = output[\"embeddings\"]\n",
    "            # Save intermediate attention maps if needed\n",
    "            if (layer_idx + 1) in self._attention_layers_to_save:\n",
    "                for map_number in self._attention_maps_per_layer_to_save[layer_idx + 1]:\n",
    "                    dkey = f\"attention_map_layer_{layer_idx + 1}_number_{map_number}\"\n",
    "                    outs[dkey] = output[\"attention_weights\"][:, map_number + 1]\n",
    "\n",
    "        return x, outs\n",
    "        \n",
    "    def __call__(self, inputs: jnp.array):\n",
    "        \n",
    "        # Compute embeddings\n",
    "        # Prepare outputs dict\n",
    "        outs: Dict[str, jnp.ndarray] = {}\n",
    "\n",
    "        # Compute embeddings\n",
    "        x = self._embed_layer(inputs)\n",
    "        \n",
    "        # If masked, scale\n",
    "        x, outs = self.apply_attention_blocks(\n",
    "            x=x,\n",
    "            outs=outs,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # If requirements on inputs, check\n",
    "        \n",
    "        # Positional embeddings\n",
    "        \n",
    "        # Construct layers, eg attention / Head\n",
    "        \n",
    "        \n",
    "        return outs\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OkTransformer(hk.Module):\n",
    "    \"\"\"\n",
    "    Jax implementation of Nucleotide Transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: NucleotideTransformerConfig,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a Nucleotide Transformer model.\n",
    "\n",
    "        Args:\n",
    "            config: Dataclass containing model hyperparameters.\n",
    "            name: Name for module (custom will break weight loading).\n",
    "        \"\"\"\n",
    "\n",
    "        self._config = config\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self._embed_layer = hk.Embed(self._config.alphabet_size, self._config.embed_dim)\n",
    "\n",
    "        # self._pos_embed_layer = ESMLearnedPositionalEmbeddings(\n",
    "        #     config.max_positions, config.embed_dim, config.pad_token_id\n",
    "        # )\n",
    "\n",
    "        self._lm_head = RobertaLMHead(\n",
    "            embed_dim=self._config.embed_dim,\n",
    "            alphabet_size=self._config.alphabet_size,\n",
    "            name=\"roberta_lm_head\",\n",
    "        )\n",
    "\n",
    "        if self._config.emb_layer_norm_before:\n",
    "            self.emb_ln_before = hk.LayerNorm(\n",
    "                axis=-1,\n",
    "                create_scale=True,\n",
    "                create_offset=True,\n",
    "                name=\"emb_layer_norm_before\",\n",
    "            )\n",
    "\n",
    "        # Process attention maps to save requirement into more suitable format\n",
    "        attention_maps_to_save = config.attention_maps_to_save\n",
    "        self._attention_layers_to_save = list({t[0] for t in attention_maps_to_save})\n",
    "        self._attention_maps_per_layer_to_save = {t[0]: list(t[1:]) for t in attention_maps_to_save}\n",
    "\n",
    "        # Checking user request can be executed, raise error otherwise\n",
    "        max_layer = max(self._attention_layers_to_save + [0])\n",
    "        if max_layer > config.num_layers:\n",
    "            raise ValueError(\n",
    "                f\"You are requiring attention maps for layer {max_layer}, \"\n",
    "                f\"while the model has {config.num_layers} layers only.\"\n",
    "            )\n",
    "\n",
    "        for layer, maps in self._attention_maps_per_layer_to_save.items():\n",
    "            max_map = max(maps)\n",
    "            if max_map > config.attention_heads:\n",
    "                raise ValueError(\n",
    "                    f\"You are requiring attention maps number {max_map} \"\n",
    "                    f\"at layer {layer}, while the model has {config.attention_heads} \"\n",
    "                    f\"only.\"\n",
    "                )\n",
    "\n",
    "    @hk.transparent\n",
    "    def apply_attention_blocks(\n",
    "        self,\n",
    "        x: Embedding,\n",
    "        outs: Dict[str, Embedding],\n",
    "        attention_mask: Optional[AttentionMask] = None,\n",
    "    ) -> Tuple[Embedding, Dict[str, Embedding]]:\n",
    "        \"\"\"\n",
    "        Create the blocks of attention layers and applies them.\n",
    "\n",
    "        Args:\n",
    "            x: The sequence embedding.\n",
    "            outs: A dictionary to carry through the attention layers which stores the\n",
    "                intermediate sequence embedding and attention maps.\n",
    "            attention_mask: Attention mask of shape (batch_size, 1, seq_len, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            The output sequence embedding.\n",
    "            The optional intermediate results (embeddings of the layer and attention\n",
    "                weights).\n",
    "        \"\"\"\n",
    "\n",
    "        layers: List[Callable] = [\n",
    "            self._attention_block(layer_idx)\n",
    "            for layer_idx in range(self._config.num_layers)\n",
    "        ]\n",
    "\n",
    "        if self._config.use_gradient_checkpointing:\n",
    "            # the remat-ed function cannot take control flow arguments\n",
    "            layers = [hk.remat(layer) for layer in layers]\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            output = layer(\n",
    "                x=x,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            x = output[\"embeddings\"]\n",
    "            # Save intermediate embeddings if needed\n",
    "            if (layer_idx + 1) in self._config.embeddings_layers_to_save:\n",
    "                outs[f\"embeddings_{(layer_idx+1)}\"] = output[\"embeddings\"]\n",
    "            # Save intermediate attention maps if needed\n",
    "            if (layer_idx + 1) in self._attention_layers_to_save:\n",
    "                for map_number in self._attention_maps_per_layer_to_save[layer_idx + 1]:\n",
    "                    dkey = f\"attention_map_layer_{layer_idx + 1}_number_{map_number}\"\n",
    "                    outs[dkey] = output[\"attention_weights\"][:, map_number + 1]\n",
    "\n",
    "        return x, outs\n",
    "\n",
    "    @hk.transparent\n",
    "    def _attention_block(self, layer_idx: int) -> SelfAttentionBlock:\n",
    "\n",
    "        return SelfAttentionBlock(  # type: ignore\n",
    "            num_heads=self._config.attention_heads,\n",
    "            embed_dim=self._config.embed_dim,\n",
    "            key_size=self._config.key_size,\n",
    "            ffn_embed_dim=self._config.ffn_embed_dim,\n",
    "            name=f\"attention_layer_{layer_idx}\",\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        tokens: Tokens,\n",
    "        attention_mask: Optional[AttentionMask] = None,\n",
    "    ) -> TransformerOutput:\n",
    "        \"\"\"\n",
    "        Computes the embeddings based on the input tokens.\n",
    "\n",
    "        Args:\n",
    "            tokens: Input tokens out of the tokenizer of shape (batch_size, seq_len).\n",
    "            attention_mask: Attention mask of shape (batch_size, 1, seq_len, seq_len).\n",
    "                If no mask is provided, a mask by default which equals 1 over all non\n",
    "                pad tokens and 0 over pad tokens is computed.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the final embeddings and logits.\n",
    "        \"\"\"\n",
    "        # Prepare outputs dict\n",
    "        outs: Dict[str, jnp.ndarray] = {}\n",
    "\n",
    "        # Compute embeddings\n",
    "        x = self._embed_layer(tokens)\n",
    "        # Tokens dropout if needed\n",
    "        if self._config.token_dropout:\n",
    "            x = TokensDropout(\n",
    "                embed_dim=self._config.embed_dim,\n",
    "                mask_token_id=self._config.mask_token_id,\n",
    "                pad_token_id=self._config.pad_token_id,\n",
    "                masking_ratio=self._config.masking_ratio,\n",
    "                masking_prob=self._config.masking_prob,\n",
    "            )(x, tokens)\n",
    "\n",
    "        # RoBERTa's mask scaling factor\n",
    "        x = self._config.embed_scale * x\n",
    "\n",
    "        # Add check that the sequence fed into the transformer is not longer\n",
    "        # than the max positions used to instantiate the learned positional\n",
    "        # embeddings layer\n",
    "        assert tokens.shape[1] <= self._config.max_positions, (\n",
    "            \"Inputs to the learned positional embeddings layer have a length \"\n",
    "            f\"{x.shape[1]} greater than the max positions used to instantiate \"\n",
    "            f\"it: {self._config.max_positions}\"\n",
    "        )\n",
    "\n",
    "        # !! Removed pos emb\n",
    "        # Positional Embedding\n",
    "        # x = x + self._pos_embed_layer(tokens)\n",
    "\n",
    "        if self._config.emb_layer_norm_before:\n",
    "            x = self.emb_ln_before(x)\n",
    "\n",
    "        # Attention mask\n",
    "        if attention_mask is None:\n",
    "            attention_mask = build_padding_attention_mask(\n",
    "                tokens=tokens, pad_token_id=self._config.pad_token_id\n",
    "            )\n",
    "\n",
    "        # construct a tower of attention layers\n",
    "        x, outs = self.apply_attention_blocks(\n",
    "            x=x,\n",
    "            outs=outs,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        # Language Model Head\n",
    "        lm_head_outs = self._lm_head(x)\n",
    "        outs[\"logits\"] = lm_head_outs[\"logits\"]\n",
    "\n",
    "        embeddings = lm_head_outs[\"embeddings\"]\n",
    "        # Save final embeddings if needed\n",
    "        if self._config.num_layers in self._config.embeddings_layers_to_save:\n",
    "            outs[f\"embeddings_{self._config.num_layers}\"] = embeddings\n",
    "\n",
    "        return outs  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
